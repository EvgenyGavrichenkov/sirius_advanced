[
["index.html", "Анализ данных в социальных науках с помощью языка R. Вторая неделя 1 Начало работы", " Анализ данных в социальных науках с помощью языка R. Вторая неделя Иван Поздняков 2020-01-23 1 Начало работы Здесь содержатся конспекты для курса “Анализ данных в социальных науках с помощью языка R” в Сириусе. Полезные ссылки: Ссылка на страницу курса на сайте образовательного центра “Сириус” Как это было Github страница с материалами курса - там же можно найти исходные .Rmd файлы со всеми конспектами. "],
["d1.html", "2 Неделя 2, День 1 2.1 Warmup exercise 2.2 Простая линейная регрессия 2.3 Множественная линейная регрессия", " 2 Неделя 2, День 1 2.1 Warmup exercise Итак, давайте загрузим датасет про студентов и вес их рюкзаков и сконвертируем его в data.table: install.packages(&quot;Stat2Data&quot;) library(Stat2Data) library(data.table) data(&quot;Backpack&quot;) back &lt;- as.data.table(Backpack) Самостоятельное задание: Исследуйте колонки BackpackWeight и BodyWeight. В чем измеряются эти переменные? Переведите их в килограммы, создав колонки BackpackWeightKG, BodyWeightKG. back[, summary(BackpackWeight)] ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 8.00 11.00 11.66 14.25 35.00 back[, summary(BodyWeight)] ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 105.0 130.0 147.5 153.1 170.0 270.0 back[,BackpackWeightKG:= 0.45359237*BackpackWeight] back[,BodyWeightKG:= 0.45359237*BodyWeight] Присвойте id каждому испытуемому. back[,id:=1:.N] Как различается вес рюкзака в зависимости от пола? Кто весит больше? Если допустить, что выборка репрезентативна, то можно ли сделать вывод о различии по среднему весу в генеральной совокупности? back[,mean(BackpackWeightKG), by = Sex] ## Sex V1 ## 1: Female 5.006010 ## 2: Male 5.634625 back[,t.test(BackpackWeightKG ~ Sex)] ## ## Welch Two Sample t-test ## ## data: BackpackWeightKG by Sex ## t = -1.1782, df = 86.25, p-value = 0.242 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.6892365 0.4320067 ## sample estimates: ## mean in group Female mean in group Male ## 5.006010 5.634625 Повторите пункт 3 для веса самих студентов. back[,mean(BodyWeightKG), by = Sex] ## Sex V1 ## 1: Female 62.28236 ## 2: Male 78.14893 back[,t.test(BodyWeightKG ~ Sex)] ## ## Welch Two Sample t-test ## ## data: BodyWeightKG by Sex ## t = -7.0863, df = 77.002, p-value = 5.704e-10 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -20.32511 -11.40803 ## sample estimates: ## mean in group Female mean in group Male ## 62.28236 78.14893 Визуализируйте распределение этих двух переменных в зависимости от пола (используя ggplot2) library(ggplot2) ggplot(back)+ geom_histogram(aes(x = BodyWeightKG, fill = Sex), bins = 15, position = &quot;identity&quot;, alpha = 0.7) ggplot(back)+ geom_histogram(aes(x = BackpackWeightKG, fill = Sex), bins = 12, position = &quot;identity&quot;, alpha = 0.7) Теперь исследуем взаимосвязь переменных. Посчитайте коэффициент корреляции Пирсона и Спирмена. back[, cor.test(BodyWeightKG, BackpackWeightKG)] ## ## Pearson&#39;s product-moment correlation ## ## data: BodyWeightKG and BackpackWeightKG ## t = 1.9088, df = 98, p-value = 0.05921 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.007360697 0.371918344 ## sample estimates: ## cor ## 0.1893312 back[, cor.test(BodyWeightKG, BackpackWeightKG, method = &quot;spearman&quot;)] ## Warning in cor.test.default(BodyWeightKG, BackpackWeightKG, method = ## &quot;spearman&quot;): Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: BodyWeightKG and BackpackWeightKG ## S = 131520, p-value = 0.03527 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.2108001 Постройте диаграмму рассеяния с помощью ggplot2. Цветом закодируйте пол респондента. ggplot(back, aes(x = BodyWeightKG, y = BackpackWeightKG))+ geom_point(aes(colour = Sex), alpha = 0.5, size = 2) 2.2 Простая линейная регрессия Вы уже умеете считать коэффициент корреляции Пирсона: back[,cor.test(BackpackWeightKG, BodyWeightKG)] ## ## Pearson&#39;s product-moment correlation ## ## data: BackpackWeightKG and BodyWeightKG ## t = 1.9088, df = 98, p-value = 0.05921 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.007360697 0.371918344 ## sample estimates: ## cor ## 0.1893312 Простая линейная регрессия - это примерно то же самое. В синтаксисе линейной регрессии уже не обойтись без формул, это такой специальный тип данных в R: class(y ~ x) ## [1] &quot;formula&quot; Если видите эту волнистую линию - тильду, это, что значит перед Вами формула. Давайте исследуем зависимость размера рюкзака от массы тела. В простой лингейной регрессии, в отличие от корреляции, есть направленность: одна переменная является как бы независимой переменной (предиктором), другая - как бы объясняемой переменной (outcome). В формуле предикторы находятся справа от тильды, а объясняемая переменная - слева. Терминология линейной регрессии может немного запутать: если одна переменная предиктор, а другая объясняется этим предиктором, то кажется, что они должны быть обязательно связаны причинно-следственной связью. Это не так: обозначения условны, более того, Вы можете поменять переменные местами и ничего не изменится! Короче говоря, “линейная регрессия” не дает никакой магической каузальной силы переменным. 2.2.1 Функция lm() Давайте посчитаем линейную регрессию функцией lm(). model &lt;- lm(BackpackWeightKG ~ BodyWeightKG, data = back) model ## ## Call: ## lm(formula = BackpackWeightKG ~ BodyWeightKG, data = back) ## ## Coefficients: ## (Intercept) BodyWeightKG ## 2.71125 0.03713 summary(model) ## ## Call: ## lm(formula = BackpackWeightKG ~ BodyWeightKG, data = back) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4853 -1.7629 -0.4681 1.2893 9.8803 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.71125 1.37483 1.972 0.0514 . ## BodyWeightKG 0.03713 0.01945 1.909 0.0592 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.581 on 98 degrees of freedom ## Multiple R-squared: 0.03585, Adjusted R-squared: 0.02601 ## F-statistic: 3.644 on 1 and 98 DF, p-value: 0.05921 print(model) или просто model выводит коэфициенты линейной регрессии - это коэффициенты прямой, которая лучше всего подогнанна к данным. Как измеряется качество этой подгонки? В расстоянии точек исходных точек до прямой. По идее, расстояние до прямой нужно было бы считать просто по модулю. И так делают, хоть и очень редко. Обычно в линейной регрессии используются квадратичные расстояния точек до прямой для оценки расстояния (метод наименьших квадратов - ordinary least squares). Это дает кучу клевых математических свойств, например, возможность легко аналитически найти коэффициенты прямой линейной регрессии. back[, body := BodyWeightKG] back[, bp := BackpackWeightKG] library(ggplot2) ggplot(data = back,aes(x = body, y = bp))+ geom_point(alpha = 0.3)+ geom_abline(slope = 0.03713, intercept = 2.71125)+ coord_cartesian(xlim = c(-1, 140)) Функция predict() позволяет скормить модели новые данные и получить предсказания для новых значений предикторов. Попробуем поиграть с этим немного. Допустим, предскажем вес рюкзака для студента весом в 100 кг: predict(model, newdata = data.frame(BodyWeightKG = 100)) ## 1 ## 6.424229 Мы можем даже попробовать какие-нибудь экстремальные значения для предикторов. Например, сколько будет весить рюкзак студента весом 1000 кг? predict(model, newdata = data.frame(BodyWeightKG = 1000)) ## 1 ## 39.841 Очевидно, что в этом не очень много смысла: студент весом 1000 кг не сможет ходить на занятия, поэтому и про вес рюкзака как-то не имеет смысл спрашивать. Это проблема экстрополяции: линейная регрессия позволяет более-менее достоверно предсказывать значения внутри диапазона значений, на которых была построена модель. Еще один “странный” пример - студент весом 0 кг. predict(model, newdata = data.frame(BodyWeightKG = 0)) ## 1 ## 2.711255 Здесь бессмысленность происходящего еще очевиднее. Конечно, вес студента не может быть равен нулю, иначе это не студент вовсе. Однако это позволяет понять, что такое intercept модели - это значение зависимой переменой в случае, если предиктор равен нулю. А коэффициент предиктора означает, насколько килограммов увеличивается вес рюкзака при увеличении веса студента на 1 кг: на 0.0371297. Не очень много! 2.2.2 Интерпретация вывода линейной регрессии Давайте еще раз посмотрим на summary(model): summary(model) ## ## Call: ## lm(formula = BackpackWeightKG ~ BodyWeightKG, data = back) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4853 -1.7629 -0.4681 1.2893 9.8803 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.71125 1.37483 1.972 0.0514 . ## BodyWeightKG 0.03713 0.01945 1.909 0.0592 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.581 on 98 degrees of freedom ## Multiple R-squared: 0.03585, Adjusted R-squared: 0.02601 ## F-statistic: 3.644 on 1 and 98 DF, p-value: 0.05921 Теперь мы понимаем, что это за коэффициенты. Однако это всего лишь их оценка. Это значит, что мы допускаем, что в реальности есть некие настоящие коэффициенты линейной регрессии, а каждый раз собирая новые данные, они будут посчитаны как немного разные. Короче говоря, эти коэффициенты - те же статистики, со своим выборочным распределением и стандартными ошибками. На основе чего и высчитывается p-value для каждого коэффициента - вероятность получить такой и более отклоняющийся от нуля коэффициент при верности нулевой гипотезы - независимости зависимой переменной от предиктора. Кроме p-value, у линейной регрессии есть R2 - доля объясненной дисперсии. Как ее посчитать? Для начала давайте сохраним как отдельные колонки ошибки (необъясненную часть модели) и предсказанные значения (они означают объясненную часть модели). Можно убедиться, что сумма предсказанных значений и ошибок будет равна зависимой переменной. model$residuals ## 1 2 3 4 5 6 ## -0.73414413 -2.36666022 -0.19634292 -2.60017426 -2.11403371 -4.48531686 ## 7 8 9 10 11 12 ## -1.94561603 -4.01261202 -2.63272245 -3.82508188 -1.35615417 4.11950245 ## 13 14 15 16 17 18 ## -0.58483892 -0.58483892 0.62663298 -0.66904776 0.74339000 -1.75808589 ## 19 20 21 22 23 24 ## -0.28055176 -3.22218430 3.49749241 1.66855186 -0.16379474 0.64006870 ## 25 26 27 28 29 30 ## -2.72260803 2.75872534 -1.60878068 -2.80114012 5.42861891 -2.76859193 ## 31 32 33 34 35 36 ## -0.61738711 2.07161893 -0.90256180 -1.97816422 0.86014703 2.55775948 ## 37 38 39 40 41 42 ## 1.28119121 -0.41642125 -3.50735900 -1.49088831 -1.52457185 0.91180768 ## 43 44 45 46 47 48 ## -0.36476060 1.31373940 -0.53317827 -2.38009594 -2.14544654 -2.96955779 ## 49 50 51 52 53 54 ## -0.51974255 -0.70159594 3.04390004 -1.69298952 4.86054022 1.34628758 ## 55 56 57 58 59 60 ## 0.74339000 2.57460125 -0.36476060 -3.55901965 -0.98677064 1.08022535 ## 61 62 63 64 65 66 ## -1.12264013 -0.70159594 -0.75325660 -1.44036301 -0.97333492 9.88033377 ## 67 68 69 70 71 72 ## 0.59294945 1.11277354 -3.90929072 -2.54851361 -3.02121845 3.32907473 ## 73 74 75 76 77 78 ## 1.11277354 1.53381772 -1.77719836 6.20334021 -3.57245537 -1.18773650 ## 79 80 81 82 83 84 ## 1.90320125 -0.19634292 5.68124542 5.73290607 -2.38009594 -0.02792525 ## 85 86 87 88 89 90 ## 3.71757073 0.91180768 -0.78466943 1.36540005 3.49749241 1.59891409 ## 91 92 93 94 95 96 ## 2.92714302 2.75872534 -0.55115539 0.57497233 0.55585986 0.86014703 ## 97 98 99 100 ## -3.27384496 0.08883177 -0.98677064 1.22953056 back$residuals = residuals(model) back$fitted = fitted(model) back[, bp - (fitted + residuals)] ## [1] 0.000000e+00 -4.440892e-16 0.000000e+00 -4.440892e-16 4.440892e-16 ## [6] -4.440892e-16 -4.440892e-16 2.220446e-16 0.000000e+00 1.110223e-16 ## [11] -4.440892e-16 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ## [16] 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ## [21] 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ## [26] 0.000000e+00 4.440892e-16 4.440892e-16 0.000000e+00 0.000000e+00 ## [31] 0.000000e+00 0.000000e+00 0.000000e+00 -4.440892e-16 -8.881784e-16 ## [36] 0.000000e+00 0.000000e+00 0.000000e+00 -2.220446e-16 -4.440892e-16 ## [41] -4.440892e-16 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ## [46] 0.000000e+00 4.440892e-16 0.000000e+00 0.000000e+00 0.000000e+00 ## [51] 0.000000e+00 -4.440892e-16 0.000000e+00 0.000000e+00 0.000000e+00 ## [56] 0.000000e+00 0.000000e+00 -4.440892e-16 0.000000e+00 0.000000e+00 ## [61] -8.881784e-16 0.000000e+00 0.000000e+00 -4.440892e-16 0.000000e+00 ## [66] 0.000000e+00 0.000000e+00 0.000000e+00 -3.330669e-16 0.000000e+00 ## [71] 4.440892e-16 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ## [76] 0.000000e+00 -3.330669e-16 4.440892e-16 0.000000e+00 0.000000e+00 ## [81] 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ## [86] 0.000000e+00 0.000000e+00 -8.881784e-16 0.000000e+00 -8.881784e-16 ## [91] 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ## [96] -8.881784e-16 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 Соответственно, вся сумма объясненной дисперсии разделяется на объясненую и необъясненную. Полная дисперсия (total sum of squares = TSS) может быть посчитана как сумма квадратов разниц со средним. Необъясненная дисперсия - это сумма квадратов ошибок - residual sum of squares (RSS). rss &lt;- back[, sum(residuals^2)] rss ## [1] 652.7272 tss &lt;- back[, sum((bp - mean(bp))^2)] tss ## [1] 676.995 1- rss/tss ## [1] 0.03584628 Это очень мало, мы объяснили всего 3.5846285% дисперсии. Собственно, и p-value больше, чем 0,05. При этом этот p-value тот же, что и при коэффициента корреляции Пирсона. А R2 - это квадрат коэффициента корреляции Пирсона, если речь идет только об одном предикторе. summary(model) ## ## Call: ## lm(formula = BackpackWeightKG ~ BodyWeightKG, data = back) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4853 -1.7629 -0.4681 1.2893 9.8803 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.71125 1.37483 1.972 0.0514 . ## BodyWeightKG 0.03713 0.01945 1.909 0.0592 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.581 on 98 degrees of freedom ## Multiple R-squared: 0.03585, Adjusted R-squared: 0.02601 ## F-statistic: 3.644 on 1 and 98 DF, p-value: 0.05921 cor.test(back$bp, back$body)$estimate^2 ## cor ## 0.03584628 2.2.3 Допущения линейной регрессии Как и в случае с другими параметрическими методами, линейная регрессия имеет определенные допущения относительно используемых данных. Если они не соблюдаются, то все наши расчеты уровня значимости могут некорректными. Очень важно ставить вопрос о том, насколько результаты будут некорректными. Как сильно нарушения допущений будет влиять на модель? Ответ на этот вопрос может быть контринтуитивен. Например, достаточно большие отклонения от нормальности нам обычно не стражны при условии того, что выборка достаточно большая. Допущения линейной регрессии связаны с ошибками: они должны быть нормально распределены, а разброс ошибок должен не уменьшаться и не увеличиваться в зависимости от предсказанных значений. Это то, что называется гомоскедастичностью или гомогенностью (когда все хорошо) и гетероскедастичностью или гетерогенностью (когда все плохо). Если мы применим функцию plot(), то получим 4 скаттерплота: Зависимость ошибок от предсказанных значений. На что здесь смотреть? На симметричность относительно нижней и верхней части графика, на то, что разброс примерно одинаковый слева и справа. Q-Q plot. Здесь все довольно просто: если ошибки являются выборкой из нормального распределения, то они выстраиваются в прямую линию. Если это мало похоже на прямую линию, то имеет место отклонение от нормальности. Scale-Location plot. Этот график очень похож на график 1, только по оси у используются квадратные корни модуля ошибки. Еще один способ исследовать гетеро(гомо)скедастичность и находить выбросы. Residuals-Leverage plot. Здесь по оси х - расстояние Кука, а по оси у - стандартизированный размер выбросов. Расстояние Кука показывает high-leverage points - точки, которые имеют экстремальные предсказанные значения, то есть очень большие или очень маленькие значения по предикторам. Для линейной регрессии такие значения имеют большее значение, чем экстремальные точки по предсказываемой переменной. Особенно сильное влияние имеют точки, которые имеют экстремальные значения и по предикторам, и по предсказываемой переменной. Одна такая точка может поменять направление регрессионной прямой! Расстояние Кука отражает уровень leverage, а стандартизированные ошибки отражают экстремальные значения по у (вернее, экстремальные отклонения от предсказанных значений). В этом графике нужно смотреть на точки с правой стороны графика, особенно если они находятся высоко или низко по оси у. plot(model) Давайте теперь нарисуем регрессионную прямую на скаттерплоте: ggplot(data = back,aes(x = body, y = bp))+ geom_point(alpha = 0.3)+ geom_abline(slope = model$coefficients[2], intercept = model$coefficients[1]) Самостоятельное задание: На практике порогом для экстремальных точек часто выбирают среднее ±2 или ±3 стандартных отклонения. Напишите функцию is_outlier(), которая возвращает TRUE, если значение выходит за 2 стандартных отклонения от среднего. Затем измените функцию is_outlier() так, чтобы можно было бы самостоятельно вводить количество стандартных отклонений с помощью параметра n =. Пусть по умолчанию это будет 3. Но проверьте, что все работает и на 2! Теперь измените функцию is_outlier() так, чтобы можно было выбирать функцию для меры центральности (centr =, по умолчанию - среднее) и функцию для меры разброса (vary =, по умолчанию - стандартное отклонение). Проверьте, что это работает на медиане и 3 median absolute deviation. Как Вы думаете, какой из вариантов подходит больше? Почему? О, да, в R функция тоже может быть использована в качестве аргумента функции. Впрочем, это не первый случай, когда мы с этим сталкиваемся. Другой пример - функции семейства *apply(). Заметьте - там тоже в качестве аргумента выступала функция (как объект), а не просто название функции как строковая переменная. В этом задании нужно сделать так же. 2.2.4 Влияние выбросов на линейную модель Давайте теперь попробуем посмотреть, как изменится модель, если выкинуть high leverage points (экстремальные значения по предиктору - body) и что будет, если выкинуть экстремальные значения по у. Обычная линия - регрессионная прямая для модели со всеми точками, штрихованная линия - регрессионная прямая для модели без экстремальных значений по предиктору, пунктирная линия - регрессионная прямая для модели без экстремальных значений по предсказываемой переменной. back[, body_outlier := is_outlier(body)] back[, bp_outlier := is_outlier(bp)] model1 &lt;- lm(BackpackWeightKG ~ BodyWeightKG, data = back[!is_outlier(body),]) summary(model1) ## ## Call: ## lm(formula = BackpackWeightKG ~ BodyWeightKG, data = back[!is_outlier(body), ## ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.6526 -1.7471 -0.3773 1.1699 9.0854 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.48577 1.65749 0.293 0.77011 ## BodyWeightKG 0.07128 0.02413 2.953 0.00397 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.548 on 94 degrees of freedom ## Multiple R-squared: 0.08491, Adjusted R-squared: 0.07517 ## F-statistic: 8.722 on 1 and 94 DF, p-value: 0.003971 model2 &lt;- lm(BackpackWeightKG ~ BodyWeightKG, data = back[!is_outlier(bp),]) summary(model2) ## ## Call: ## lm(formula = BackpackWeightKG ~ BodyWeightKG, data = back[!is_outlier(bp), ## ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7843 -1.4343 -0.1363 1.4296 4.9122 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.60560 1.13144 3.187 0.00196 ** ## BodyWeightKG 0.01915 0.01610 1.190 0.23716 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.087 on 93 degrees of freedom ## Multiple R-squared: 0.01499, Adjusted R-squared: 0.004402 ## F-statistic: 1.416 on 1 and 93 DF, p-value: 0.2372 ggplot(data = back, aes(x = body, y = bp, colour =bp_outlier, shape = body_outlier))+ geom_point(alpha = 0.3)+ geom_abline(intercept = model$coefficients[1], slope = model$coefficients[2])+ geom_abline(intercept= model1$coefficients[1], slope = model1$coefficients[2], linetype = &quot;dashed&quot;)+ geom_abline(intercept= model2$coefficients[1], slope = model2$coefficients[2], linetype = &quot;dotted&quot;)+ theme_minimal() 2.3 Множественная линейная регрессия В множественной линейной регрессионной регрессии у нас появляется несколько предикторов. Какая модель лучше: где есть много предикторов или где мало предикторов? С одной стороны, чем больше предикторов, тем лучше: каждый новый предиктор может объяснить чуть больше необъясненной дисперсиии. С другой стороны, если эта прибавка маленькая (а она всегда будет не меньше нуля), то, возможно, новый предиктор просто объясняет “случайный шум”. В действительности, если у нас будет достаточно много предикторов, то мы сможем объяснить любые данные! Парадоксальным образом такая модель будет давать очень хорошие результаты на той выборке, по которой мы считаем коэффициенты, но делать очень плохие предсказания на новой выборке - это то, что в машинном обучении называют переобучением (overfitting). Идеальная модель будет включать минимум предикторов, которые лучше всего объясненяют исследуемую переменную. Это что-то вроде бритвы Оккама в статистике. Поэтому часто используются показатели качества модели, которые “наказывают” модель за большое количество предикторов. Например, adjusted R2: \\[R_{adj} = 1 - (1 - R^2) \\frac{n -1}{n - p - 1}\\] Здесь n - это количество наблюдений, p - количество параметров. Итак, добавим новый предиктор - Units. Это количество кредитов, которые студенты взяли в четверти1. Можно предположить, что чем больше у студента набрано кредитов, тем более тяжелый у нее/него рюкзак. Давайте добавим это как второй предиктор. Для этого нужно просто записать второй предиктор в формуле через плюс. model3 &lt;- lm(bp ~ body + Units, data = back) summary(model3) ## ## Call: ## lm(formula = bp ~ body + Units, data = back) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.6221 -1.8347 -0.5023 1.2519 10.0623 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.28481 2.16170 0.132 0.8955 ## body 0.04391 0.01990 2.207 0.0297 * ## Units 0.13703 0.09456 1.449 0.1505 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.566 on 97 degrees of freedom ## Multiple R-squared: 0.05628, Adjusted R-squared: 0.03682 ## F-statistic: 2.892 on 2 and 97 DF, p-value: 0.06025 Множественная линейная регрессия имеет еще одно допущение: отстутсвие мультиколлинеарности. Это значит, что предикторы не должны коррелировать друг с другом. Для измерения мультколлинеарности существует variance inflation factor (VIF-фактор). Считается он просто: для предиктора \\(i\\) считается линейная регрессия, где все остальные предикторы предсказывают предиктор \\(i\\). Сам VIF-фактор считается на основе полученного R2 регрессии: \\[VIF_i = \\frac{1}{1 - R_i^2}\\] Если Ri2 большой, то и VIFi выходит большим. Это означает, что предиктор сам по себе хорошо объясняется другими предикторами. Какой VIF считать большим? Здесь нет единого мнения, но если он выше 3 и особенно если он выше 10, то с этим нужно что-то делать. car::vif(model3) ## body Units ## 1.05858 1.05858 В нашем случае это не так. Но если бы VIF был большим для какого-либо предиктора, то можно было бы либо попробовать его выкинуть или же использовать анализ главных компонент (см. 5.3), о котором пойдет речь в один из следующих дней. Потом, правда, оказалось, что в данном случае факторный анализ ничего не доказывает, зато метод оказался очень полезным и получил большое распространение (особенно в психологии).↩ "],
["d2.html", "3 Неделя 2, День 2 3.1 Дисперсионный анализ (ANOVA) 3.2 RMarkdown", " 3 Неделя 2, День 2 3.1 Дисперсионный анализ (ANOVA) Дисперсионный анализ или ANOVA2 - один из самых распространенных методов статистического анализа в психологии и многих других дисциплинах. Дисперсионный анализ очень хорошо подходит для анализа данных, полученных в эксперименте - методе организации исследования, при котором исследователь напрямую управляет уровнями независимой переменной. Терминологическая связь между дисперсионным анализом и планированием эксперимента настолько тесная, что многие термины пересекаются, поэтому нужно быть осторожными. Как и в случае с линейной регрессией, если мы что-то называем “независимой переменной” (или “фактором”), это не порождает никакой каузальной связи. Еще одна важная вещь, которую нужно понимать про дисперсионный анализ, это то, что у этого метода очень запутывающее название: из названия кажется, что этот статистический метод для сравнения дисперсий. Нет, это не так (хотя такие статистические тесты тоже есть, и они нам сегодня пригодятся - см. 3.1.6). Нет, это просто сравнение средних в случае, если есть больше, чем 2 группы для сравнения. У дисперсионного анализа очень много разновидностей, для которых придумали множество названий. “Обычная” ANOVA называется One-Way ANOVA, она же межгрупповая ANOVA, это аналог независимого т-теста для нескольких групп. Давайте начнем сразу с проведения теста. Мы будем использовать данные с курса по статистике Университета Шеффилда про эффективность диет. library(data.table) diet &lt;- fread(&quot;data/stcp-Rdataset-Diet.csv&quot;) Сделаем небольшой препроцессинг данных. Создадим дополнительные “факторные” переменные, создадим переменную, в которой будет разница массы “до” и “после”, удалим NA. diet[, weight.loss := weight6weeks - pre.weight] diet[, Dietf := factor(Diet, labels = LETTERS[1:3])] diet[, Person := factor(Person)] diet &lt;- diet[complete.cases(diet),] 3.1.1 Функция aov() Попробуем сразу провести дисперсионных анализ с помощью функции aov(): aov_model &lt;- aov(weight.loss ~ Dietf, diet) aov_model ## Call: ## aov(formula = weight.loss ~ Dietf, data = diet) ## ## Terms: ## Dietf Residuals ## Sum of Squares 60.5270 410.4018 ## Deg. of Freedom 2 73 ## ## Residual standard error: 2.371064 ## Estimated effects may be unbalanced summary(aov_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Dietf 2 60.5 30.264 5.383 0.0066 ** ## Residuals 73 410.4 5.622 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Мы получили что-то похожее на результат применения функции lm(). Правда, лаконичнее, но с новыми столбцами Sum Sq, Mean Sq и новой статистикой F вместо t. Что будет, если с теми же данными с той же формулой запустить lm() вместо aov()? summary(lm(weight.loss ~ Dietf, diet)) ## ## Call: ## lm(formula = weight.loss ~ Dietf, data = diet) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7000 -1.6519 -0.1759 1.4420 5.3680 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.3000 0.4840 -6.818 2.26e-09 *** ## DietfB 0.0320 0.6776 0.047 0.96246 ## DietfC -1.8481 0.6652 -2.778 0.00694 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.371 on 73 degrees of freedom ## Multiple R-squared: 0.1285, Adjusted R-squared: 0.1047 ## F-statistic: 5.383 on 2 and 73 DF, p-value: 0.006596 lm() превратил Dietf в две переменные, но F и p-value у двух моделей одинаковые! Кроме того, функция aov() является, по сути, просто “оберткой” над lm(): This provides a wrapper to lm for fitting linear models to balanced or unbalanced experimental designs. 3.1.2 Тестирование значимости нулевой гипотезы в ANOVA. Как и в случае с другими статистическими тестами, мы можем выделить 4 этапа в тестировании значимости нулевой гипотезы в ANOVA: Формулирование нулевой и альтернативной гипотезы. Нулевая гипотеза говорит, что между средними в генеральной совокупности нет различий: \\[H_0:\\mu_1 = \\mu_2 = ... = \\mu_n\\] Можно было бы предположить, что ненулевая гипотеза звучит как “все средние не равны”, но вообще-то это не так. Альтернативная гипотеза в дисперсионном анализе звучит так: \\[H_1: \\text{Не все средние равны}\\] Подсчет статистики. Как мы уже видели раньше, в дисперсионном анализе используется новая для нас статистика F. Впрочем, мы ее видели, когда смотрели на аутпут функции lm(), когда делали линейную регрессию. Чтобы считать F (если вдруг мы хотим сделать это вручную), нужно построить талбицу ANOVA (ANOVA table). Таблица ANOVA Степени свободы Суммы квадратов Средние квадраты F-статистика Межгрупповые \\(df_{b}\\) \\(SS_{b}\\) \\(MS_{b} =\\frac{SS_{b}}{df_{b}}\\) \\(F=\\frac{MS_{b}}{MS_{w}}\\) Внутригрупповые \\(df_{w}\\) \\(SS_{w}\\) \\(MS_{w} =\\frac{SS_{w}}{df_{w}}\\) Общие \\(df_{t}\\) \\(SS_{t}= SS_{b} + SS_{w}\\) Именно эту таблицу мы видели, когда использовали функцию aov(): summary(aov_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Dietf 2 60.5 30.264 5.383 0.0066 ** ## Residuals 73 410.4 5.622 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Вот как это все считается: Таблица ANOVA Степени свободы Суммы квадратов Средние квадраты F-статистика Между \\(df_{b}=J-1\\) \\(SS_{b}= \\sum\\limits_{j=1}^J \\sum\\limits_{i=1}^{n_j} (\\overline{x_j}-\\overline{x})^2\\) \\(MS_{b} =\\frac{SS_{b}}{df_{b}}\\) \\(F=\\frac{MS_{b}}{MS_{w}}\\) Внутри \\(df_{w}=N-J\\) \\(SS_{w}= \\sum\\limits_{j=1}^J \\sum\\limits_{i=1}^{n_j} (x_{ij}-\\overline{x_j})^2\\) \\(MS_{w} =\\frac{SS_{w}}{df_{w}}\\) Общие \\(df_{t}=N-1\\) \\(SS_{t}= \\sum\\limits_{j=1}^J \\sum\\limits_{i=1}^{n_j} (x_{ij}-\\overline{x})^2\\) \\(J\\) означает количество групп, \\(N\\) - общее количество наблюдений во всех группах, \\(n_j\\) означает количество наблюдений в группе j, а \\(x_{ij}\\) - наблюдение под номером \\(i\\) в группе \\(j\\). Вариабельность обозначается \\(SS\\) и означает “сумму квадратов” (sum of squares) - это то же, что и дисперсия, только мы не делим вме в конце на количество наблюдений (или количество наблюдений минус один): \\[SS = \\sum\\limits_{i=1}^{n_j} (x_{i}-\\overline{x})^2\\] Здесь много формул, но суть довольно простая: мы разделяем вариабельность зависимой переменной на внутригрупповую и межгрупповую, считаем их соотношение, которое и будет F. В среднем, F будет равен 1 при верности нулевой гипотезы. Это означает, что и межгрупповая вариабельность, и внутригрупповая вариабельность - это просто шум. Но если же межгрупповая вариабельность - это не просто шум, то это соотношение будет сильно больше единицы. Подсчет p-value. В т-тесте мы смотрели, как статистика распределена при условии верности нулевой гипотезы. То есть что будет, если нулевая гипотеза верна, мы будем повторять эксперимент с точно таким же дизайном (и размером выборок) бесконечное количество раз и считать F. betweendf &lt;- 2 withindf &lt;- 73 f &lt;- summary(aov_model)[[1]]$F[1] v &lt;- seq(0.1,10, 0.01) fdist &lt;- data.frame(fvalues = v, pdf = df(v, betweendf, withindf)) library(ggplot2) label &lt;- paste0(&quot;F(&quot;, betweendf, &quot;, &quot;, withindf, &quot;) = &quot;, round(f, 3)) ggplot(fdist, aes(x = fvalues, y = pdf))+ geom_line()+ geom_vline(xintercept = f)+ annotate(&quot;text&quot;, x = f+1, y = 0.2, label = label)+ scale_y_continuous(expand=c(0,0)) + theme_minimal()+ theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_blank(), axis.title.y = element_blank()) Рисунок 3.1: F-распределение при верности нулевой гипотезы (см. детали в тексте) Заметьте, распределение F несимметричное3. Это значит, что мы всегда считаем считаем площадь от F до плюс бесконечности (без умножения на 2, как мы это делали в т-тесте): 1 - pf(f, betweendf, withindf) ## [1] 0.006595853 Это и есть наш p-value! Сравнение p-value с уровнем \\(\\alpha\\). Самый простой этап: если наш p-value меньше, чем \\(\\alpha\\) (который обычно равен .05), то мы отвергаем нулевую гипотезу. Если нет - не отвергаем. В нашем случае это 0.0065959, что, очевидно, меньше, чем .05. Отвергаем нулевую гипотезу (о том, что нет различий), принимаем ненулевую (о том, что различия есть). Все! 3.1.3 Post-hoc тесты Тем не менее, дисперсионного анализа недостаточно, чтобы решить, какие именно группы между собой различаются. Для этого нужно проводить post-hoc тесты (апостериорные тесты). Post-hoc переводится с латыни как “после этого”. Post-hoc тесты проводятся, если в результате ANOVA Вы отвергли нулевую гипотезу. Собственно, пост-хоки никак не связаны с дисперсионным анализом на уровне расчетов - это абсолютно независимые тесты, но исторически так сложилось, что они известны именно как дополнительный этап ANOVA. Самый простой вариант пост-хок теста - это попарные т-тесты с поправками на множественные сравнения: pairwise.t.test(diet$weight.loss, diet$Dietf) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: diet$weight.loss and diet$Dietf ## ## A B ## B 0.962 - ## C 0.017 0.017 ## ## P value adjustment method: holm Второй подход связан с использованием специализированных тестов, таких как тест Тьюки (Tukey Honest Significant Differences = Tukey HSD). Для этого в R есть функция TukeyHSD(), которую нужно применять на объект aov: TukeyHSD(aov_model) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = weight.loss ~ Dietf, data = diet) ## ## $Dietf ## diff lwr upr p adj ## B-A 0.032000 -1.589085 1.6530850 0.9987711 ## C-A -1.848148 -3.439554 -0.2567422 0.0188047 ## C-B -1.880148 -3.454614 -0.3056826 0.0152020 3.1.4 ANOVA и т-тест как частные случаи линейной регрессии Как мы уже видели, если применить lm() или aov() на одних и тех же данных с одной и той же формулой, то результат будет очень похожим. Но есть одно но: lm() создает из одного фактора две переменных-предиктора: summary(lm(weight.loss ~ Dietf, diet)) ## ## Call: ## lm(formula = weight.loss ~ Dietf, data = diet) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7000 -1.6519 -0.1759 1.4420 5.3680 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.3000 0.4840 -6.818 2.26e-09 *** ## DietfB 0.0320 0.6776 0.047 0.96246 ## DietfC -1.8481 0.6652 -2.778 0.00694 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.371 on 73 degrees of freedom ## Multiple R-squared: 0.1285, Adjusted R-squared: 0.1047 ## F-statistic: 5.383 on 2 and 73 DF, p-value: 0.006596 Дело в том, что мы не можем просто так загнать номинативную переменную в качестве предиктора в линейную регрессию. Мы можем это легко сделать, если у нас всего два уровня в номинативном предикторе. Тогда один из уровней можно обозначить за 0, другой - за 1. Такие переменные иногда называются “бинарными”. Тогда это легко использовать в линейной регрессии: summary(lm(weight.loss ~ gender, diet)) ## ## Call: ## lm(formula = weight.loss ~ gender, data = diet) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.1848 -1.7264 0.2041 1.6846 5.9930 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.8930 0.3846 -10.123 1.3e-15 *** ## gender -0.1221 0.5836 -0.209 0.835 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.522 on 74 degrees of freedom ## Multiple R-squared: 0.0005914, Adjusted R-squared: -0.01291 ## F-statistic: 0.04379 on 1 and 74 DF, p-value: 0.8348 Можно ли так делать? Вполне! Допущения линейной регрессии касаются остатков, а не переменных самих по себе. Разве что это немного избыточно: линейная регрессия с бинарным предиктором - это фактически независимый т-тест: t.test(weight.loss ~ gender, diet, var.equal = TRUE) ## ## Two Sample t-test ## ## data: weight.loss by gender ## t = 0.20925, df = 74, p-value = 0.8348 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.040810 1.285067 ## sample estimates: ## mean in group 0 mean in group 1 ## -3.893023 -4.015152 Как видите, p-value совпадают! А t статистика в квадрате - это F (при двух группах): t.test(weight.loss ~ gender, diet, var.equal = TRUE)$statistic^2 ## t ## 0.04378592 Более того, те же самые результаты можно получить и с помощью коэффициента корреляции Пирсона: cor.test(diet$gender, diet$weight.loss) ## ## Pearson&#39;s product-moment correlation ## ## data: diet$gender and diet$weight.loss ## t = -0.20925, df = 74, p-value = 0.8348 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.2484113 0.2022466 ## sample estimates: ## cor ## -0.02431772 Теперь должно быть понятно, почему все эти функции делают вроде бы разные статистические тесты, но выдают такой похожий результат - это фактически один и тот же метод! Все эти методы (и некоторые из тех, что будем рассматривать далее) можно рассматривать как разновидности множественной линейной регрессии.4 3.1.5 Dummy coding Тем не менее, вопрос остается открытым: как превратить номинативную переменную в количественную и загнать ее в регрессию? Для этого можно использовать “фиктивное кодирование” (dummy coding): diet[, isA := as.numeric(Dietf == &quot;A&quot;)] diet[, isB := as.numeric(Dietf == &quot;B&quot;)] diet[, isC := as.numeric(Dietf == &quot;C&quot;)] diet[c(1:2,15:16,35:36),c(&quot;Dietf&quot;, &quot;isA&quot;, &quot;isB&quot;, &quot;isC&quot;)] ## Dietf isA isB isC ## 1: A 1 0 0 ## 2: A 1 0 0 ## 3: B 0 1 0 ## 4: B 0 1 0 ## 5: C 0 0 1 ## 6: C 0 0 1 Заметьте, что такое кодирование избыточно. Если мы знаем, что диет 3, а данная диета - это не диета В и не диета С, то это диета А. Значит, одна из созданных нами колонок - “лишняя”: diet[, isA := NULL] Используем новую колонки для линейной регрессии и сравним результаты: summary(lm(weight.loss ~ isB + isC, diet)) ## ## Call: ## lm(formula = weight.loss ~ isB + isC, data = diet) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7000 -1.6519 -0.1759 1.4420 5.3680 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.3000 0.4840 -6.818 2.26e-09 *** ## isB 0.0320 0.6776 0.047 0.96246 ## isC -1.8481 0.6652 -2.778 0.00694 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.371 on 73 degrees of freedom ## Multiple R-squared: 0.1285, Adjusted R-squared: 0.1047 ## F-statistic: 5.383 on 2 and 73 DF, p-value: 0.006596 summary(lm(weight.loss ~ Dietf, diet)) ## ## Call: ## lm(formula = weight.loss ~ Dietf, data = diet) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7000 -1.6519 -0.1759 1.4420 5.3680 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.3000 0.4840 -6.818 2.26e-09 *** ## DietfB 0.0320 0.6776 0.047 0.96246 ## DietfC -1.8481 0.6652 -2.778 0.00694 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.371 on 73 degrees of freedom ## Multiple R-squared: 0.1285, Adjusted R-squared: 0.1047 ## F-statistic: 5.383 on 2 and 73 DF, p-value: 0.006596 То же самое! 3.1.6 Допущения ANOVA Нормальность распределения ошибок: hist(residuals(aov_model)) Как мы видим, распределение не сильно далеко от нормального - этого вполне достаточно. ANOVA - это метод достаточно устойчивый к отклонениям от нормальности. Гомогенность дисперсий. То есть их равенство. Можно посмотреть на распределение остатков: diet$residuals &lt;- residuals(aov_model) ggplot(diet, aes(x = Dietf, y = residuals))+ geom_jitter(width = 0.1, alpha = 0.5) Все выглядит неплохо: нет какой-то одной группы, у которой разброс сильно больше или меньше. Есть и более формальные способы проверить равенство дисперсий. Например, с помощью теста Ливиня (Levene’s test). Для того, чтобы его провести, мы воспользуемся новым пакетом ez (читать как “easy”). Этот пакет сильно упрощает проведение дисперсионного анализа, особенно для более сложных дизайнов. install.packages(&quot;ez&quot;) Синтаксис довольно простой: нужно указать, данные, зависимую переменную, переменную с ID, факторы. Необходимо прописать фактор в between = или within =. В данном случае - в between =. library(ez) ez_model &lt;- ezANOVA(data = diet, dv= weight.loss, wid = Person, between = Dietf, detailed = T, return_aov = T) ## Warning: You have removed one or more Ss from the analysis. Refactoring ## &quot;Person&quot; for ANOVA. ## Warning: Data is unbalanced (unequal N per group). Make sure you specified ## a well-considered value for the type argument to ezANOVA(). ## Coefficient covariances computed by hccm() ez_model ## $ANOVA ## Effect DFn DFd SSn SSd F p p&lt;.05 ges ## 1 Dietf 2 73 60.52701 410.4018 5.383104 0.006595853 * 0.1285269 ## ## $`Levene&#39;s Test for Homogeneity of Variance` ## DFn DFd SSn SSd F p p&lt;.05 ## 1 2 73 2.040419 160.8859 0.4629076 0.6312856 ## ## $aov ## Call: ## aov(formula = formula(aov_formula), data = data) ## ## Terms: ## Dietf Residuals ## Sum of Squares 60.5270 410.4018 ## Deg. of Freedom 2 73 ## ## Residual standard error: 2.371064 ## Estimated effects may be unbalanced Если при проведении теста Ливиня мы получаем p &lt; .05, то мы отбрасываем нулевую гипотезу о равенстве дисперсий. В данном случае мы не можем ее отбросить и поэтому принимаем5 Полученный объект (если поставить return_aov = T) содержит еще и объект aov() - на случай, если у Вас есть функции, которые работают с этим классом: TukeyHSD(ez_model$aov) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = formula(aov_formula), data = data) ## ## $Dietf ## diff lwr upr p adj ## B-A 0.032000 -1.589085 1.6530850 0.9987711 ## C-A -1.848148 -3.439554 -0.2567422 0.0188047 ## C-B -1.880148 -3.454614 -0.3056826 0.0152020 Примерно одинаковое количество испытуемых в разных группах. Здесь у нас все в порядке: diet[,.N, by = Dietf] ## Dietf N ## 1: A 24 ## 2: B 25 ## 3: C 27 Небольшие различия в размерах групп - это ОК, тем более, что на практике такое очень часто случается: кого-то пришлось выкинуть из анализа, для какой-то строчки были потеряны данные и т.д. Однако больших различий в размерах групп стоит избегать. Самое плохое, когда группы различаются значительно по размеру (более чем в 2 раза) и вариабельность внутри групп отличается значительно (более чем в 2 раза). 3.1.7 Многофакторный дисперсионный анализ (Factorial ANOVA) На практике можно встретить One-Way ANOVA (однофакторную ANOVA) довольно редко. Обычно в исследованиях встречается многофакторный дисперсионный анализ, в котором проверяется влияние сразу нескольких факторов. В научных статьях это обозначается примерно так: “3х2 ANOVA”. Это означает, что был проведен двухфакторный дисперсионный анализ, причем в одном факторе было три уровня, во втором - два. В нашем случае это будут факторы “Диета” и “Пол”. Это означает, что у нас две гипотезы: о влиянии диеты на потерю веса и о влиянии пола на потерю веса. Кроме того, появляется гипотеза о взаимодействии факторов - то есть о том, что разные диеты по разному влияют на потерю веса для разных полов. Взаимодействие двух факторов хорошо видно на графике с линиями: если две линии параллельны, то взаимодействия нет. Если они не параллельны (пересекаются, сходятся, расходятся), то взаимодействие есть. diet[, genderf:=factor(gender, labels = c(&quot;ж&quot;, &quot;м&quot;))] sem &lt;- function(x) sd(x)/sqrt(length(x)) pivot &lt;- diet[,.(meanloss = mean(weight.loss), se = sem(weight.loss)), by = .(Dietf, genderf)] library(ggplot2) pd = position_dodge(0.05) ggplot(pivot, aes(x = Dietf, y = meanloss, colour = genderf))+ geom_line(aes(group = genderf), position = pd)+ geom_pointrange(aes(ymin = meanloss - se, ymax = meanloss +se), position = pd) Как видно по картинке, разница в эффективности диеты С по сравнению с другими видна только для женщин. ezANOVA(data = diet, dv= weight.loss, wid = Person, between = .(Dietf, gender), detailed = T, return_aov = T) ## Warning: You have removed one or more Ss from the analysis. Refactoring ## &quot;Person&quot; for ANOVA. ## Warning: &quot;gender&quot; will be treated as numeric. ## Warning: Data is unbalanced (unequal N per group). Make sure you specified ## a well-considered value for the type argument to ezANOVA(). ## Coefficient covariances computed by hccm() ## Warning: At least one numeric between-Ss variable detected, therefore no ## assumption test will be returned. ## $ANOVA ## Effect DFn DFd SSn SSd F p p&lt;.05 ## 1 Dietf 2 70 60.4172197 376.329 5.61902602 0.00545568 * ## 2 gender 1 70 0.1686958 376.329 0.03137868 0.85990976 ## 3 Dietf:gender 2 70 33.9040683 376.329 3.15320438 0.04884228 * ## ges ## 1 0.138334829 ## 2 0.000448066 ## 3 0.082645860 ## ## $aov ## Call: ## aov(formula = formula(aov_formula), data = data) ## ## Terms: ## Dietf gender Dietf:gender Residuals ## Sum of Squares 60.5270 0.1687 33.9041 376.3290 ## Deg. of Freedom 2 1 2 70 ## ## Residual standard error: 2.318648 ## Estimated effects may be unbalanced Итак, теперь мы проверяем три гипотезы вместо одной. Действительно, взаимодействие диеты и пола оказалось значимым, как и ожидалось. 3.1.8 Дисперсионный анализ с повторными измерениями (Repeated-measures ANOVA) Если обычный дисперсионный анализ - это аналог независимого т-теста для нескольких групп, то дисперсионный анализ с повторными измерениями - это аналог зависимого т-теста. В функции ezANOVA() для проведения дисперсионного анализа с повторными измерениями нужно просто поставить нужным параметром внутригрупповую переменную. Это означает, что в данном случае мы должны иметь данные в длинном формате, для чего мы воспользуемся функцией melt(): dietlong &lt;- melt(diet, measure = c(&quot;pre.weight&quot;, &quot;weight6weeks&quot;), variable = &quot;time&quot;, value = &quot;weight&quot;) ## Warning in melt.data.table(diet, measure = c(&quot;pre.weight&quot;, ## &quot;weight6weeks&quot;), : &#39;measure.vars&#39; [pre.weight, weight6weeks] are not all ## of the same type. By order of hierarchy, the molten data value column will ## be of type &#39;double&#39;. All measure variables not of type &#39;double&#39; will be ## coerced too. Check DETAILS in ?melt.data.table for more on coercion. dietlongC &lt;- droplevels(dietlong[Dietf == &quot;C&quot;,]) ezANOVA(dietlongC, dv = weight, wid = Person, within = time) ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 time 1 26 124.6949 2.030459e-11 * 0.0986036 3.1.9 Смешанный дисперсионный анализ (Mixed between-within subjects ANOVA) Нам никто не мешает совмещать и внутригруппоые, и межгрупповые факторы вместе. ezANOVA(dietlong, dv = weight, wid = Person, within = time, between = Dietf) ## Warning: You have removed one or more Ss from the analysis. Refactoring ## &quot;Person&quot; for ANOVA. ## Warning: Data is unbalanced (unequal N per group). Make sure you specified ## a well-considered value for the type argument to ezANOVA(). ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 Dietf 2 73 0.8280758 4.409507e-01 0.021710057 ## 3 time 1 73 210.5004045 3.346036e-23 * 0.059209996 ## 4 Dietf:time 2 73 5.3831045 6.595853e-03 * 0.003208607 Здесь нас интересует взаимодействие между факторами. Результаты, полученные для этой гипотезы, идентичны результатам по обычному дисперсионному анализу на разницу до и после - по сути это одно и то же. 3.1.10 Заключение Мы разобрали много разных вариантов дисперсионного анализа. И это неудивительно - дисперсионный анализ является одним из самых распространенных статистических методов, в особенности в экспериментальных науках. При этом дисперсионный анализ можно представить как частный случай множественной линейной регрессии! 3.2 RMarkdown RMarkdown - это мощный инструмент создания отчетов с использованием R (и других языков программирования). С помощью RMarkdown можно превратить скрипт с анализом в красивый отчет - Word-документ, PDF-документ, веб-страницу, презентацию (с интерактивными элементами!). Кстати, этот сайт тоже сделан с помощью RMarkdown. Вот как это работает: Как работает RMarkdown В основе всего лежит pandoc - программа, которая преобразует разные форматы друг в друга. Поскольку разные форматы и даже виды документов имеют много схожих элементов, это вохможно сделать: например, и Word-файлы, и LaTeX, и HTML-документы имеют заголовки разных уровней. Markdown - это еще одна разметка. Ее преимущество - в удобстве использовании и простоте синтаксиса. Например, именно эта разметка используется на GitHub для ReadMe файлов. Markdown-документы - это просто текстовые документы с .Md разрешением. knitr - R пакет для работы со специальными файлами .Rmd для превращения их в обычные .Md. Отличие .Rmd от .Md в наличии специально оформленных кусков кода (чанков). Эти куски кода выполняются и в .Md файле кроме самого кода (или даже без него) вставляется результат выполнения этого кода. Например, результат выполнения статистических тестов или график. RMarkdown - просто удобная оболочка над knitr с нужными настройками. Пример Markdown кода: ![Как работает RMarkdown](https://d33wubrfki0l68.cloudfront.net/61d189fd9cdf955058415d3e1b28dd60e1bd7c9b/b739c/lesson-images/rmarkdownflow.png) Как работает RMarkdown Вот так делать заголовки: # R Markdown ## Что такое RMarkdown ## Чанки с кодом Это чанк с кодом. Он отделяется ``` с обоих сторон и {r}. Это означает, что внутри находится код на R, который должен быть выполнен: ``` {r} 2+2 ``` 2+2 ## [1] 4 3.2.1 Настройки чанка У чанка с кодом есть набор настроек. Самый важные из них такие: echo: будет ли показан сам код message и warning: будут ли показаны сообщения и предупреждения, всплывающие во время исполнения кода eval: будет ли испольняться код внутри чанка 3.2.2 Настройка нескольких чанков Все эти настройки можно настроить как для отдельных чанков, так и для все чанков сразу: knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE) 3.2.3 Чанки с Python! Вместо {r} нужно написать {python} x = &#39;hello, python !&#39; print (x.split(&quot; &quot;)) ## [&#39;hello,&#39;, &#39;python&#39;, &#39;!&#39;] 3.2.4 Код вне чанков (inline code) Число пи равно ` r pi `: Число пи равно 3.1415927 3.2.5 Синтаксис Markdown (без R) 3.2.5.1 Выделение текста *Курсив* _Тоже курсив_ **Полужирный** __Тоже полужирный__ Курсив Тоже курсив Полужирный Тоже полужирный 3.2.5.2 Заголовки разных уровней ## Заголовки разных уровней ### Мне заголовок #### И моему сыну тоже ##### И моему! ###### OK, boomer 3.2.5.3 Списки Первый вариант списка выглядит так: Можно и с подсписком Почему бы и нет? Кому нужен порядок Тот списки номерует 3.2.5.4 Цитаты Цитата: Я устал Который год во мне живет нарвал 3.2.6 Таблицы library(data.table) go &lt;- fread(&quot;data/iGLAS for R course.csv&quot;) go[1:4,1:4] ## StartDate EndDate Status IPAddress ## 1: 1/15/2017 1/16/2017 0 144.139.7.52 ## 2: 1/27/2017 1/27/2017 0 31.54.151.215 ## 3: 1/24/2017 1/24/2017 0 176.62.130.7 ## 4: 01/10/2017 1/23/2017 0 86.161.181.218 library(knitr) kable(go[1:5,1:4]) StartDate EndDate Status IPAddress 1/15/2017 1/16/2017 0 144.139.7.52 1/27/2017 1/27/2017 0 31.54.151.215 1/24/2017 1/24/2017 0 176.62.130.7 01/10/2017 1/23/2017 0 86.161.181.218 12/11/2016 12/11/2016 0 31.211.8.249 3.2.6.1 Динамические таблицы library(DT) datatable(go[1:5, 1:5]) 3.2.7 Визуализации library(ggplot2) library(Stat2Data) library(data.table) data(&quot;Backpack&quot;) back &lt;- as.data.table(Backpack) ggplot_scatter &lt;- ggplot(back, aes(x = BodyWeight, y = BackpackWeight))+ geom_point(aes(colour = Sex), alpha = 0.5, size = 2) ggplot_scatter 3.2.8 Динамические визуализации в plotly library(plotly) ggplotly(ggplot_scatter) 3.2.9 Вставлять HTML Можно собирать RMarkdown документ в разные форматы: .pdf (с помощью LaTeX), Word-файл, HTML-страницу и некоторые другие форматы. Если во собираете документ соответствующего формата, то вам доступны и соответствующие плюшки этого формата. Например, этот сайт - это .Rmd, собранный в HTML. Я могу вставлять сюда любой HTML код. Например, видео с YouTube! &lt;iframe width=&quot;966&quot; height=&quot;543&quot; src=&quot;https://www.youtube.com/embed/hHW1oY26kxQ&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt; Потом, правда, оказалось, что в данном случае факторный анализ ничего не доказывает, зато метод оказался очень полезным и получил большое распространение (особенно в психологии).↩ Форма F-распределения будет сильно зависеть от числа степеней свободы. Но оно всегда определено от 0 до плюс бесконечности: в числителе и знаменателе всегда неотрицательные числа.↩ Обобщением множественной линейной регрессии (вернее, одним из) можно считать общую линейную модель (general linear model). Общая линейная модель может предсказывать не одну, а сразу несколько объясняемых переменных в отличие от множественной линейной регрессии. Следующим этапом обобщения служит обобщенная линейная модель (generalized linear model). Фишка последней в том, что можно использовать не только модели с нормально распределенными остатками, но и, например, логистическую и пуассоновскую регрессию.↩ Вообще-то эта логика не совсем корректна. Тест Ливиня - это такой же статистический тест, как и остальные. Поэтому считать, что допущения соблюдаются на основании того, что p-value больше допустимого уровня \\(\\alpha\\), - это неправильно. Но для проверки допущений такая не очень корректная практика считается допустимой.↩ "],
["d3.html", "4 Неделя 2, День 3 4.1 Введение в работу с текстом 4.2 Базовые операции с текстом 4.3 Поиск паттернов и регулярные выражения 4.4 Анализ текста - что дальше? 4.5 Fuzzy matching", " 4 Неделя 2, День 3 library(data.table) 4.1 Введение в работу с текстом Работа с текстом - это отдельная и сложная задача. И у R все необходимые инструменты для этого! Всю работу со строками (и текстом в целом) можно условно поделить на три уровня: Базовые операции (до регулярных выражений) Регулярные выражения Natural language processing Мы начнем с базовых операций. В R есть много функций для работы со строками. В принципе, их достаточно для того, чтобы делать весьма сложные вещи, но как это часто бывает с R, есть дополнительные пакеты, которые не только и не столько расширяют функционал, сколько делают нашу жизнь удобнее. Основных таких пакета два: stringi и stringr. Давайте сразу их установим: install.packages(&quot;stringi&quot;) install.packages(&quot;stringr&quot;) library(stringi) library(stringr) stringi - это базовый пакет, который имеет очень широкий функционал. Функции из этого пакета начинаются на stri_. stringr - это пакет, который является “оберткой” пакета stringi. Функции пакета stringr начинаются на str_. stringr - более “минималистичный”: в нем меньше функций, чем в stringi. А еще stringr - это часть tidyverse, но этот пакет вполне можно использовать и без tidyverse, например, работая в data.table. Тем не менее, для более-менее продвинутой работы с текстом придется выучить специальный язык - “регулярные выражения” (regular expressions или просто regex). Регулярные выражения реализованы на многих языках, в том числе в R. Но мы пока обойдемся наиболее простыми функциями, которые покроют большую часть того, что нам нужно уметь делать при работе с текстом. Ну а после освоения базовых возможностей и регулярных выражений можно кидаться в естественную обработку языка и делать всякие топик моделлинги и прочие сентимент анализы. Впрочем, можно туда запрыгивать сразу, но обладая базовым инструментарием работы со строковыми данными, все эти штуки делать будет гораздо проще и эффективнее. Итак, поехали. 4.2 Базовые операции с текстом Строковые данные (character) - один из базовых типов данных в R. Для того, чтобы создать строковую переменную нужно использовать кавычки. Можно одинарные: &#39;Всем привет!&#39; ## [1] &quot;Всем привет!&quot; Можно двойные: &quot;Всем привет!&quot; ## [1] &quot;Всем привет!&quot; Разницы никакой! Главное использовать один вид кавычек для одного значения вектора. И еще: если использовать один вид кавычек для задания значения переменной, то другой вид кавычек можно использовать “внутри”: &quot;Всем &#39;привет&#39;!&quot; ## [1] &quot;Всем &#39;привет&#39;!&quot; &#39;Всем &quot;привет&quot;!&#39; ## [1] &quot;Всем \\&quot;привет\\&quot;!&quot; Пустую строку можно сделать с помощью функции character(): character(1) ## [1] &quot;&quot; А вот это уже похоже на пассивную агрессию: &quot;С тобой все в порядке?&quot; ## [1] &quot;С тобой все в порядке?&quot; character(5) ## [1] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; Конечно, можно превращать другие типы данных в строки: as.character(1:10) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; as.character(TRUE) ## [1] &quot;TRUE&quot; 4.2.1 Объединение и разъединение строк Как объединить строки? Очевидным способом будет попробовать функцию c(): c(&quot;Всем&quot;, &quot;Привет&quot;) ## [1] &quot;Всем&quot; &quot;Привет&quot; И вроде бы это то, что нужно, но если посмотрите внимательно, то увидите, что оба слова выделены в кавычки. Короче говоря, мы просто соединили два значения в вектор (ну или два вектора длиной один в вектор длиной два). Для того, чтобы соединить строковые значения, есть функция paste(): paste(&quot;Всем&quot;, &quot;Привет&quot;) ## [1] &quot;Всем Привет&quot; Вот это то, что нужно! У функции paste() есть параметр sep = - разделитель, который по умолчанию является пробелом - sep = &quot; &quot;, но его можно поменять на любой другой, например: paste(&quot;Всем&quot;, &quot;Привет&quot;, sep = &quot;хэй!&quot;) ## [1] &quot;Всемхэй!Привет&quot; Обычно, правда, нам не нужно придумывать такие ухищренные разделители, а нужно, чтобы его вообще не было: paste(&quot;Всем&quot;, &quot;Привет&quot;, sep = &quot;&quot;) ## [1] &quot;ВсемПривет&quot; Есть функция paste0(), которая является оберткой над обычным paste(), но уже с sep = &quot;&quot; по умолчанию: paste0(&quot;Всем&quot;, &quot;Привет&quot;) ## [1] &quot;ВсемПривет&quot; Теперь попробуем создать простой data.table с буквами. Буквы латиницы зашиты в R как константы: let &lt;- data.table(small = letters[1:10], big = LETTERS[1:10]) let ## small big ## 1: a A ## 2: b B ## 3: c C ## 4: d D ## 5: e E ## 6: f F ## 7: g G ## 8: h H ## 9: i I ## 10: j J Давайте теперь попробуем создать колонку both, где объединим обе колонки с помощью какого-нибудь разделителя: let[, both := paste(small, big, sep = &quot;_&quot;)] let ## small big both ## 1: a A a_A ## 2: b B b_B ## 3: c C c_C ## 4: d D d_D ## 5: e E e_E ## 6: f F f_F ## 7: g G g_G ## 8: h H h_H ## 9: i I i_I ## 10: j J j_J Ага! Если немного присмотреться, то можно заметить, что функция paste() “склеивает” несколько векторов и выдает на выходе вектор такой же длины. А вот чтобы соединить значения одного вектора в одно значение, нужно воспользоваться параметром collapse =: let[, paste(small, big, sep = &quot;+&quot;, collapse = &quot; -&gt; &quot;)] ## [1] &quot;a+A -&gt; b+B -&gt; c+C -&gt; d+D -&gt; e+E -&gt; f+F -&gt; g+G -&gt; h+H -&gt; i+I -&gt; j+J&quot; Есть и более продвинутый способ соединения строк, который происходит еще из C и очень популярен в Python. Для этого способа есть функция sprintf(). Вот пример того, как она работает: sprintf(&quot;Добро пожаловать в %s, на дворе %i год&quot;, &quot;СИРИУС&quot;, year(Sys.Date())) ## [1] &quot;Добро пожаловать в СИРИУС, на дворе 2020 год&quot; Это может быть удобно, если Вы хотите в большую строчку вставить какую-то информацию из переменных. Ну и наконец, самый продвинутый способ соединять строчки - это пакет glue. О да, для того, чтобы делать простые вещи очень просто нужно осваивать целый пакет! Но не беспокойтесь, у этого пакета очень интуитивно понятный синтаксис, который удобнее (на мой взгляд), чем sprintf(): install.packages(&quot;glue&quot;) library(glue) let[, glue(&quot;{1:.N} буква латинского алфавита: {let$big}(заглавная) - {let$small}(строчная)&quot;)] ## 1 буква латинского алфавита: A(заглавная) - a(строчная) ## 2 буква латинского алфавита: B(заглавная) - b(строчная) ## 3 буква латинского алфавита: C(заглавная) - c(строчная) ## 4 буква латинского алфавита: D(заглавная) - d(строчная) ## 5 буква латинского алфавита: E(заглавная) - e(строчная) ## 6 буква латинского алфавита: F(заглавная) - f(строчная) ## 7 буква латинского алфавита: G(заглавная) - g(строчная) ## 8 буква латинского алфавита: H(заглавная) - h(строчная) ## 9 буква латинского алфавита: I(заглавная) - i(строчная) ## 10 буква латинского алфавита: J(заглавная) - j(строчная) Теперь осталось научиться разъединять строковые значения. Для этого в пакете data.table есть функция tstrsplit() let[, tstrsplit(both, &quot;_&quot;)] ## V1 V2 ## 1: a A ## 2: b B ## 3: c C ## 4: d D ## 5: e E ## 6: f F ## 7: g G ## 8: h H ## 9: i I ## 10: j J 4.2.2 Подсчет длины строк Чтобы посчитать количество знаков, можно воспользоваться функцией nchar(): nchar(c(&quot;Всем&quot;, &quot;привет&quot;)) ## [1] 4 6 В качестве самостоятельного задания загрузите датасет со всеми текстами песен Дэвида Боуи, взятых с сайта Genius.com: b &lt;- fread(&quot;data/bowie2.csv&quot;) b ## song_id song_name ## 1: 4193889 1917 ## 2: 338950 1984 ## 3: 1181295 1984/Dodo ## 4: 4131335 1984 (Live 1974) ## 5: 4178509 1984 (Live &#39;74) ## --- ## 657: 4178274 Ziggy Stardust (Live 1972) ## 658: 4133052 Ziggy Stardust (Live 1973) ## 659: 4131908 Ziggy Stardust (Live 1978) ## 660: 2002305 Ziggy Stardust (Live &#39;73) [Stereo] ## 661: 4128663 Ziggy Stardust (Live, Glastonbury, 2000) ## song_lyrics_url ## 1: https://genius.com/David-bowie-1917-lyrics ## 2: https://genius.com/David-bowie-1984-lyrics ## 3: https://genius.com/David-bowie-1984-dodo-lyrics ## 4: https://genius.com/David-bowie-1984-live-1974-lyrics ## 5: https://genius.com/David-bowie-1984-live-74-lyrics ## --- ## 657: https://genius.com/David-bowie-ziggy-stardust-live-1972-lyrics ## 658: https://genius.com/David-bowie-ziggy-stardust-live-1973-lyrics ## 659: https://genius.com/David-bowie-ziggy-stardust-live-1978-lyrics ## 660: https://genius.com/David-bowie-ziggy-stardust-live-73-stereo-lyrics ## 661: https://genius.com/David-bowie-ziggy-stardust-live-glastonbury-2000-lyrics ## annotation_count artist_id artist_name ## 1: 1 9534 David Bowie ## 2: 11 9534 David Bowie ## 3: 6 9534 David Bowie ## 4: 0 9534 David Bowie ## 5: 0 9534 David Bowie ## --- ## 657: 0 9534 David Bowie ## 658: 0 9534 David Bowie ## 659: 0 9534 David Bowie ## 660: 1 9534 David Bowie ## 661: 0 9534 David Bowie ## artist_url ## 1: https://genius.com/artists/David-bowie ## 2: https://genius.com/artists/David-bowie ## 3: https://genius.com/artists/David-bowie ## 4: https://genius.com/artists/David-bowie ## 5: https://genius.com/artists/David-bowie ## --- ## 657: https://genius.com/artists/David-bowie ## 658: https://genius.com/artists/David-bowie ## 659: https://genius.com/artists/David-bowie ## 660: https://genius.com/artists/David-bowie ## 661: https://genius.com/artists/David-bowie ## lyrics ## 1: ## 2: Someday they won&#39;t let you, now you must agree The times they are a-telling and the changing isn&#39;t free You&#39;ve read it in the tea leaves, and the tracks are on TV Beware the savage jaw of 1984 They&#39;ll split your pretty cranium, and fill it full of air And tell that you&#39;re eighty, but brother, you won&#39;t care You&#39;ll be shooting up on anything, tomorrow&#39;s neverthere Beware the savage jaw of 1984 Come see, come see, remember me? We played out an all night movie role You said it would last, but I guess we enrolled In 1984 (who could ask for more) 1984 (who could ask for mor-or-or-or-ore) (Mor-or-or-or-ore) I&#39;m looking for a vehicle, I&#39;m looking for a ride I&#39;m looking for a party, I&#39;m looking for a side I&#39;m looking for the treason that I knew in &#39;65 Beware the savage jaw of 1984 Come see, come see, remember me? We played out an all night movie role You said it would last, but I guess we enrolled In 1984 (who could ask for more) 1984 (who could ask for mor-or-or-or-ore) (Mor-or-or-or-ore) 1984 ## 3: Someday they won&#39;t let you, now you must agree The times they are a-telling, and the changing isn&#39;t free You&#39;ve read it in the tea leaves, the tracks are on tv Beware the savage jaw Of 1984 They&#39;ll break your pretty cranium, and fill it full of air And tell you that you&#39;re eighty, but lover you won&#39;t care You&#39;ll be shooting up this huge world, like tomorrow&#39;s wasn&#39;t there Beware the savage jaw Of 1984 (come see, come see, remember me? ) We played out an all night movie role You said it would last, but I guess we&#39;ve grown In 1984 (who could ask for more) 1984 (who could ask for more) Now we can talk in confidence Did you guess that we&#39;ve been done wrong Lies jumped the queue to be first in line Such a shameless design He thinks he&#39;s well screened from the man at the top It&#39;s a shame that his children disagree They cooly decide to sell him down the line Daddy&#39;s brainwashing time He&#39;s a do do, no no didn&#39;t hear it from me He&#39;s a do do, no no didn&#39;t hear it from me She doesn&#39;t recall her blessed childhood out of yore When a unit was a figure not a she When lovers chose each other seems the perk&#39;s are due Another memo to screw She&#39;s a do do, no no didn&#39;t hear it from me She&#39;s a do do, no no didn&#39;t hear it from me Can you wipe your nose my child without them slotting in your file a photograph Will you sleep in fear tonight wake to find the scorching light of neighbour jim Come to, turn you in But the do do, no no, didn&#39;t hear it from me Another do do, no no, didn&#39;t hear it from me Another do do, no no, didn&#39;t hear it from me Another do do, no no, didn&#39;t hear from me Come see, come see, remember me? We played out an all night movie role You said it would last, but I guess we enrolled In 1984 (who could ask for more) 1984 (who could ask for mor-or-or-or-ore) (mor-or-or-or-ore) 198 4 1984 1984 (mor-or-or-or-ore) 1984 1984 (mor-or-or-or-ore) 1984 1984 (mor-or-or-or-ore) ## 4: Someday they won&#39;t let you So now you must agree The times they are a-telling And the changing isn&#39;t free You&#39;ve read it in the tea leaves, and the tracks are on TV Beware the savage jaw of 1984 They&#39;ll split your pretty cranium, and fill it full of air And tell that you&#39;re eighty, but brother, you won&#39;t care You&#39;ll be shooting up on anything, tomorrow&#39;s never there Beware the savage jaw Of 1984 (Come see, come see, remember me?) We played out an all night movie role You said it would last, but I guess we enrolled In 1984 (Who could ask for more?) 1984 (Who could ask for mor-or-or-or-ore?) (Mor-or-or-or-ore) I&#39;m looking for a vehicle, I&#39;m looking for a ride I&#39;m looking for a party, I&#39;m looking for a side I&#39;m looking for the reason that I knew in &#39;65 Beware the savage jaw of 1984 (Come see, come see, remember me?) We played out an all night movie role You said it would last, but I guess we enrolled In 1984 (Who could ask for more?) 1984 (Who could ask for mor-or-or-or-ore?) (Mor-or-or-or-ore) 1984 (Who could ask for more?) 1984 (Who could ask for more?) 1984 (Who could ask for more?) ## 5: Someday they won&#39;t let you So now you must agree The times they are a-telling And the changing isn&#39;t free You&#39;ve read it in the tea leaves, and the tracks are on TV Beware the savage jaw of 1984 They&#39;ll split your pretty cranium, and fill it full of air And tell that you&#39;re eighty, but brother, you won&#39;t care You&#39;ll be shooting up on anything, tomorrow&#39;s neverthere Beware the savage jaw of 1984 Come see, come see, remember me? We played out an all night movie role You said it would last, but I guess we enrolled In 1984 (Who could ask for more?) 1984 (Who could ask for...) (Mor-or-or-or-ore) I&#39;m looking for a vehicle, I&#39;m looking for a ride I&#39;m looking for a party, I&#39;m looking for a side I&#39;m looking for the reason that I knew in &#39;65 Beware the savage jaw of 1984 Come see, come see, remember me? We played out an all night movie role You said it would last, but I guess we enrolled In 1984 (Who could ask for more?) 1984 (Who could ask for mor-or-or-or-ore?) (Mor-or-or-or-ore) 1984 (Who could ask for mor-or-or-or-ore?) 1984 (Who could ask for mor-or-or-or-ore?) 1984 (Who could ask for mor-or-or-or-ore?) 1984 (Who could ask for more?) ## --- ## 657: Ooh yeah Uh! Now Ziggy played guitar Jamming good with Weird and Gilly And The Spiders from Mars And he played it left hand But he made it too far Became the special man Then we were Ziggy&#39;s Band Now Ziggy really sang Screwed up eyes and screwed down hairdo Like some cat from Japan Oh he could kill ’em by smiling He could leave &#39;em to hang He came on so loaded man Well hung, snow white tan So where were the spiders While the fly tried to break our balls? Just the beer light to guide us So we bitched about his fans And should we crush his sweet hands? Ziggy played for time Jiving us that we were Voodoo The kids was just crass He was the nazz With God-given ass He took it all too far But boy could he play guitar Making love with his ego Ziggy sucked up into his mind, ah Like a leper messiah When the kids had killed a man I had to break up the band Oh yeah Now Ziggy played guitar ## 658: Oh yeah Ah Now Ziggy played guitar Jamming good with Weird and Gilly And The Spiders from Mars Well he played it left hand But made it too far Became the special man Then we were Ziggy&#39;s Band Ziggy really sang Screwed up eyes and screwed down hairdo Like some cat from Japan Oh he could lick &#39;em by smiling He could leave &#39;em to hang Well he came on so loaded man Well hung, snow white tan So where were the spiders While the fly tried to break our balls? Just the beer light to guide us So we bitched about his fans And should we crush his sweet hands? Oh yeah! Ziggy played for time Jiving us that we were Voodoo The kids was just crass He was the nazz With God-given ass But he took it all too far But boy could he play guitar Making love with his ego (Oh, yeah) Ziggy sucked up into his mind (Ah) Like a leper messiah When the kids had killed a man I had to break up the band Oh yeah Oooh Ohh-hoo-hoo Now Ziggy played guitar ## 659: Oh yeah Ziggy played guitar Jamming good with Weird and Gilly And The Spiders from Mars He played it left hand But made it too far Became the special man Then we were Ziggy&#39;s Band Ziggy really sang Screwed up eyes and screwed down hairdo Like some cat from Japan Oh he could lick &#39;em by smiling He could leave &#39;em to hang He came on so loaded man Well hung, snow white tan So where were the spiders While the fly tried to break our balls? Just the beer light to guide us So we bitched about his fans And should we crush his sweet hands? Oh yeah Ziggy played for time Jiving us that we were Voodoo The kids was just crass He was the naz With God-given ass Well he took it all too far But boy could he play guitar Making love with his ego Ziggy sucked up into his mind Like some leper messiah When the kids had killed a man I had to break up the band Oh yeah Oh-ooh-hoo And Ziggy played guitar ## 660: Oh Oh, yeah Ziggy played guitar Jamming good with Weird and Gilly And the Spiders from Mars He played it left hand But made it too far Became the special man Then we were Ziggy&#39;s band Ziggy really sang Screwed-up eyes and screwed-down hairdo Like some cat from Japan He could lick &#39;em by smiling He could leave &#39;em to hang They came on so loaded, man Well-hung and snow-white tan So where were the spiders While the fly tried to break our balls? Just the beer light to guide us So we bitched about his fans And should we crush his sweet hands? Oh Ziggy played for time Jiving us that we were voodoo The kids were just crass He was the nazz With God-given ass He took it all too far But, boy, could he play guitar Making love with his ego Ziggy sucked up into his mind, ah Like a leper messiah When the kids had killed the man I had to break up the band Oh, yeah Ooh Ziggy played guitar ## 661: Oh yeah Now Ziggy played guitar Jamming good with Weird and Gilly And the Spiders from Mars He played it left hand But he made it too far Became the special man Then we were Ziggy&#39;s Band Ziggy really sang Screwed up eyes and screwed down hairdo Like some cat from Japan He could kill &#39;em by smiling He could leave &#39;em to hang Came on so loaded man Well hung, snow white tan So where were the spiders While the fly tried to break our balls? Just the beer light to guide us So we bitched about his fans And should we crush his sweet hands? Oh yeah Oh Ziggy played for time Jiving us that we were Voodoo The kids was just crass He was the naz With God-given ass He took it all too far But boy could he play guitar Making love with his ego Ziggy sucked up into his mind Like a leper messiah When the kids had killed a man I had to break up the band Oh yeah Oooh Oh hohoo Ziggy played guitar Текст песен находится в колонке lyrics. b[, n_letters := nchar(lyrics)] b[which.max(n_letters),] ## song_id ## 1: 3407882 ## song_name ## 1: The diary of Nathan Adler or the art-ritual murder of Baby Grace Blue: A non-linear Gothic Drama Hyper-cycle ## song_lyrics_url ## 1: https://genius.com/David-bowie-the-diary-of-nathan-adler-or-the-art-ritual-murder-of-baby-grace-blue-a-non-linear-gothic-drama-hyper-cycle-annotated ## annotation_count artist_id artist_name ## 1: 0 9534 David Bowie ## artist_url ## 1: https://genius.com/artists/David-bowie ## lyrics ## 1: It was precisely 5.47am on the morning of Friday 31 of December 1999 that a dark spirited pluralist began the dissection of 14-year-old &quot;&quot;Baby Grace Blue&quot;&quot;. The arms of the victim were pin-cushioned with 16 hypodermic needles, pumping in four major preservatives, colouring agents, memory information transport fluids and some kind of green stuff. From the last and 17th, all blood and liquid was extracted. The stomach areas was carefully flapped open and the intestines removed, disentangled and re-knitted as it were, into a small net or web and hung between the pillars of the murder-location, the grand damp doorway of Oxford Town Museum of Modern Parts, New Jersey. The Limbs of Baby were then served from the torso. Each limb was implanted with a small, highly sophisticated, binary code translator which in turn was connected to small speakers attached to far ends of each limb. The self-contained mini amplifiers were then activated, amplifying the decoded memory info-transport substances, revealing themselves as little clue haiku&#39;s, small verses detailing memories of other brutal acts, well documented by the ROMbloids. The limbs and their components were then hung upon the splayed web, slug-like prey of some unimaginable creature. The torso, by means of its bottom-most orifice, had been placed on a small support fastened to a marble base. It was shown to warring degrees of success depending upon where one stood from behind the web but in front of the Museum door itself, acting as both signifier and guardian to the act. It was definitely murder - but was it art? All this was to be the lead-up to the most provocative event in the whole sequence of serial-events that had started around November of that same year, plunging me into the most portentous chaos-abyss that a quiet lone hacker like myself could comprehend. My name is Nathan Adler, or Detective Professor Adler in my circuit. I&#39;m attached to the division of Art-Crime Inc., the recently instigated corporation funded by an endowment from the Arts Protectorate of London, it being felt that the investigation of art-crimes was in itself inseparable from other forms of expression and therefore worthy of support from this significant body. Nicolas Serota himself had deemed us, the small-fry of the division, worthy of an exhibit at last year&#39;s Biennale in Venice, three rooms of evidence and comparative study work which conclusively proved that the cow in Mark Tansey&#39;s &quot;&quot;The Innocent Bye Test&quot;&quot; could not differentiate between Paulus Potter&#39;s &quot;&quot;The Young Bull&quot;&quot; of 1647 (exactly 300 years before I was born, incidentally) and one of Monet&#39;s grain stack paintings of the 1890&#39;s. The traditional art press deemed this extrapolation &quot;&quot;bullshit&quot;&quot; and removed itself to study the more formal ideas contained in Damien Hirst&#39;s &quot;&quot;Sheep In A Box&quot;&quot;. Art&#39;s a farmyard. It&#39;s my job to pick thru&#39; the manure heap looking for peppercorns. friday, december 31, 1999, 10.15 am As in any crime, my first position is to pursue the motive-gag. The recent spate, thru&#39; &#39;98-&#39;99, of concept-muggings pretty much had me pulling breath for an art-murder. It was a crime whose time was now. The precedents were all there. It had probably its beginnings in the &#39;70s with the Viennese castrationists and the blood-rituals of Nitsch. Public revulsion put the lid on that episode, but you can&#39;t keep a good down. Spurred on by Chris Burden&#39;s having himself shot by his collaborator in a gallery, tied up in a bag, thrown on a highway and then crucified upon the top of a Volkswagen, stories circulated thru&#39; the nasty-neon of N.Y. night that a young Korean artist was the self-declared patient of wee-hours surgery in cut and run operations at not-so-secret locations in the city. If you found out about it, you could go and watch this guy having bits and pieces removed under anaesthetic. A finger-joint one night, a limb another. By the dawning of the &#39;80s, rumour had it that he was down to a torso and one arm. He&#39;d asked to be left in a cave in the Catskills, fed every so often by his acolytes. He didn&#39;t do much after that. I guess he read a lot. Maybe wrote a whole bunch. I suppose you can never tell what an artist will do once he&#39;s peaked. Round this same time, Bowie the singer remarked on a copula goons who frequented the Berlin bars wearing full surgery regalia; caps, aprons, rubber gloves and masks. The cutting edge. Then came Damien Hirst with the Shark-Cow-Sheep thing. No humans, palatable ritual for the world-wide public. The acceptable face of gore. Meanwhile in the US, 1994, I was in town on the night of the Athey sacrifactions. thursday, october 27, 1994 122 east village, manhattan Ron Athey, performance artist not for the squeamish - former heroin addict-HIV positive, pushes what looks like a knitting needle repeatedly into his forehead, a crown of blood, must hurt like hell. Stream red dribble-dribble. No screams. Face moves in pain. Carried upstage and scrubbed down in his own blood. The water. Now dresses in nice suit and tie. Now in black T-shirt and jean, carving, with a disposable scalpel, patterns, into the back of Darryl Carlton, a black man. Bloody blotted paper towels then hung on a washing line suspended over the heads of the audience Blood-prints from life. An extremely limited edition. When it was first performed back in March. &quot;&quot;Four Scenes In A Harsh Life&quot;&quot; exploited controversy shrapnel throughout the National Endowment For the Arts. &quot;&quot;We have taken every precaution with our disposal systems,&quot;&quot; an Athey spokesperson said. &quot;&quot;The towels containing the blood are immediately deposited in hazardous-waste bags. Each evening, the material will be driven to a hospital for final disposal&quot;&quot;. Athey says he is dealing with issues of self-loathing, suffering, healing and redemption. friday, december 31, 1999, 10.30 am museum of modern parts I&#39;m drinking up the Oxford Town, New Jersey fume. Salty and acid. Maybe I can get a handle on this thing back in Soho at the bureau. It used to be Rothko&#39;s studio, now the playground for all us Art-Crime folk, AC&#39; or &quot;&quot;the daubers&quot;&quot; as we&#39;re dubbed. Rothko himself, in a dark-deep-drunk one night, carefully removed his clothes, folded them up neatly, placing them upon a chair, lay upon the floor in a crucified position and after several attempts, found the soft blue pump of wrists and checked out. He&#39;d held the razor blades between wads of tissue paper so that he wouldn&#39;t cut his fingers. Deep thinker. Always was. 11.00 am &quot;&quot;dauber&quot;&quot; hq, soho The only names the Data bank can associate with Baby Grace are Leon Blank, Ramona A. Stone and Algeria Touchshriek. The rundowns are brief but not to the point: Ramona A. Stone: Female. Caucasian. Mid-40s. Assertive maintenance interest-drug dealer and Tyrannical Futurist. No convictions. Contacts: Leon Blank, Baby Grace Blue, Algeria Touchshriek. Leon Blank: Male. Mixed race. 22 years. Outsider Three convictions for petty theft, appropriation and plagiaries without license. Contacts: Baby Grace Blue, Algeria Touchshriek. Algeria Touchshriek: Male. Caucasian. 78 years. Owner of small establishment on Rail Yard, Oxford town, N.J. Deals in art-drugs and DNA prints. Fence for all apparitions of any medium. Harmless, lonely. Small cog, no wheels. Not much to go on but R.A.Stone weighs heavy on my memory. No problem, it&#39;ll come back. Best thing to do now is feed all relevant pieces into the Mack-Verbasiser, the Metarandom programme the re-strings real life facts as improbable virtual-fact. I may get a lead or two from that. 11.15 am Jesus Who. I hate typing. Anyhow, we&#39;ve got some real interesting solvents from Mack-random. How about this! Verbasiser download, first block: No convictions of assertive saints believed Caucasian way-out tyrannical evoked no images described Christian saints questions no female described christian tyrannical questions R.A.Stone christian machine believed no work is caucasian assertive saints assertive believed female convictions martyrs and tyrannical are evoked Female described the fabric machine Slashing way out saints and martyrs and thrown downstairs Now the swirl begins. Now the image stack backs up and takes centre stage. Ramona A. Stone. I remember this thickness, this treacly liquid thought. But wait, I&#39;m ahead of myself. june 15, 1977 kreutzburg, berlin It&#39;s two in the morning. I can&#39;t sleep for the screaming of some poor ostracized Turkish immigrant screaming his guts out from over the street. His hawking shriek sounds semi-stifled like he&#39;s got a pillow over his mouth. But the desperation comes through the spongy rubber like a knife. It cuts the breeze and bangs my eardrums. I take a walk past the fabric machine, turn left onto a street with no name, The caucasian suicide centre, naked and grimy, silhouetted by fungus yellow street lamps female slashing way-out saints for a dollar a time thrown downstairs if you can&#39;t take any more. Pure joy of retreat into death, led by the shepherdess. Anti mixed-race posters pasted upon their alter of pop-death icons party people. A zero with no name looks dull-eyed to Ms. Stone, the drone that says &quot;&quot;in the future, everything was up to itself&quot;&quot;. Yes. I remember Ramona. She set herself up as the no-future priestess of the Caucasian Suicide Temple, vomiting out her doctrine of death-as-eternal- party into the empty vessels of Berlin youth. The top floor rooms were the gateways to giving up to the holy ghost. She must have overseen more than 30 or 40 check-outs before the local squad twigged what was going down. october 28, 1994 New Yorker Magazine, advanced copy, celebrating fashion. It&#39;s a first of its kind since Tina Brown took over as editor. One look is all it took. It took the look and wrote a new book on what sophistaplites would take and bake. Guy Bourdin featured heavily in this new eDISHion. Since the advent of AIDS and the new morality, and, of course, his death, his dark sexy fatal style had fallen out of Vogue. An uncompromising photographer, he had found a twisty avenue through desire and death. A white female leg sticking gloomily out of a bath of black liquid enamel. Two glued up babes covered in tiny pearls. The glue prevented their skins from breathing and they pass out. &quot;&quot;Oh it would be beautiful.&quot;&quot; he is to have said. &quot;&quot;to photograph them dead in bed.&quot;&quot; He was a French Guy. Ha had known Man Ray. Loved Lewis Carroll. His first gig was doing hats for Vogue. He&#39;d place dead flies or bees on the faces of the models, or, female head wears hat crushed between three skinned calves heads, tongues lolling. Well, it was the &#39;50s, that was it was. The tight-collor &#39;50s seen through unspeakable hostility. He wanted but he couldn&#39;t paint. So he threw globs of revengeful hatred at his mobile subjects. He would systematically pull the phone cord out of the wall. He was never to be disturbed. Disturbed. Never. Everything and everyone died around him. One shoot focusing upon a women lying in bed was said to be a reconstruction of his estranged wife&#39;s death. Another picture has a women in a phone booth making some frantic call. Her hand is pressed whitely against the glass. Behind her and outside are two female bodies partially covered by the autumn leaves. His dream, so he told his friends, was to do shoots in the morgue, with the stiffs as mannequins, I don&#39;t know. I just read this stuff. Now his spirit was being resurrected. We&#39;re mystified by blood. It&#39;s our enemy now. We don&#39;t understand it. Can&#39;t live with it. Can&#39;t , well... y&#39;know? friday, december 31, 1999, 11.30 am After Surgery and investment in a bullet-proof mask, Ramona turned up in London, Canada as owner of a string of body-parts jewellery stores. Lamb penis necklaces, goat-scrotum purses, nipple earrings, that sort of thing. The word on the street, however, suggested that it was not in the best of interests to become one of her clients as occasionally, a customer would step into her shop and not come out again. The whistle blew after a much-loved and highly respected celebrity, known for being known, failed to show for a gallery-hanging of her mirrors. Other celebrities, equally known for being known, some only to each other, thought it was the most profound exhibit in years and couldn&#39;t take their eyes off the works. All the pieces sold within an hour, many for record prices. When the critic for Date magazine asked for an interview with the celebrity-artist, the gallery owner recalled that he hadn&#39;t seen her since earlier that day. She&#39;d mentioned that she would be going shopping for a diamond-encrusted umbilical cord as a celebratory thing to announce her pregnancy. She would be back in an hour. Just a quick stop at the &quot;&quot;Gallstone&quot;&quot;. 1986. That pregnancy would have produced a being that would be around 14 years of age. If it was still alive. To be continued...? ## n_letters ## 1: 12837 Это какой-то речетатив Боуи, который еще и очень непросто найти. Какова средняя длина песен Боуи? b[, mean(n_letters)] ## [1] 1115.231 4.2.3 Выделение подстрок и обрезание строк Еще одна полезная функция - substr(), она позволяет “вырезать” из character кусок от “сих” (start =) и до “сих” (stop =) Выглядит это так: substr(&quot;Не режь меня!&quot;, 4, 7) #вырезаем от 4 знака до 7 ## [1] &quot;режь&quot; Более продвинутый способ “обрезать” значения есть в пакете stringr: b[, song_name_trunc := str_trunc(song_name, width = 15)] head(b[, song_name_trunc], 10) ## [1] &quot;1917&quot; &quot;1984&quot; &quot;1984/Dodo&quot; ## [4] &quot;1984 (Live 1...&quot; &quot;1984 (Live &#39;74)&quot; &quot;5.15 The Ang...&quot; ## [7] &quot;&#39;87 and Cry&quot; &quot;&#39;87 and Cry,...&quot; &quot;Abdulmajid&quot; ## [10] &quot;A Better Future&quot; В аргументе width = вы ставите максимально допустимую длину значения в векторе. Если значение длиннее, то конец отрезается, а вместо него присобачивается то, что задано в параметре ellipsis = (по умолчанию там стоит многоточие). Отрезается ровно столько, сколько нужно, чтобы начало и ellipsis = вместе были не больше чем width =. Эта функция очень удобна при создании графиков: очень неприятно, когда все не помещается из-за одного очень длинного названия. Функция str_pad() позволяет добавить нужное количество пробелов (или других знаков): b[, song_name_pad := str_pad(song_name, 20)] head(b[, song_name_pad], 10) ## [1] &quot; 1917&quot; &quot; 1984&quot; ## [3] &quot; 1984/Dodo&quot; &quot; 1984 (Live 1974)&quot; ## [5] &quot; 1984 (Live &#39;74)&quot; &quot;5.15 The Angels Have Gone&quot; ## [7] &quot; &#39;87 and Cry&quot; &quot; &#39;87 and Cry, 2018&quot; ## [9] &quot; Abdulmajid&quot; &quot; A Better Future&quot; Функция str_trim() выполняет обратную задачу - удаляет лишние пробелы слева и/или справа: b[, song_name_trimmed := str_trim(song_name_pad)] Функция str_squish() делает еще круче: она еще и удаляет повторяющиеся пробелы внутри. str_squish(&quot; Привет, всем&quot;) ## [1] &quot;Привет, всем&quot; 4.2.4 Изменение регистра Чтобы перевести маленькие буквы в большие, нужно воспользоваться функцией toupper(): toupper(&quot;В чащах юга жил бы цитрус? Да, но фальшивый экземпляръ!&quot;) ## [1] &quot;В ЧАЩАХ ЮГА ЖИЛ БЫ ЦИТРУС? ДА, НО ФАЛЬШИВЫЙ ЭКЗЕМПЛЯРЪ!&quot; tolower(&quot;Съешь ещё этих мягких французских булок, да выпей же чаю&quot;) ## [1] &quot;съешь ещё этих мягких французских булок, да выпей же чаю&quot; Ну а чтобы сделать первую букву каждого слова заглавной - str_to_title(): str_to_title(&quot;Съешь ещё этих мягких французских булок, да выпей же чаю&quot;) ## [1] &quot;Съешь Ещё Этих Мягких Французских Булок, Да Выпей Же Чаю&quot; 4.2.5 Случайные последовательности Можно сделать случайные последовательности символов с помощью stri_rand_strings() set.seed(42) stri_rand_strings(n = 5, length = 5:9) ## [1] &quot;uwHpd&quot; &quot;Wj8ehS&quot; &quot;ivFSwy7&quot; &quot;TYu8zw5V&quot; &quot;OuRpjoOg0&quot; Можно задавать символы, допустимые для генерации таких “псевдослов” с помощью параметра pattern =. Например, если хотим использовать только строчные буквы латиницы, нужно использовать паттерн &quot;[a-z]&quot;. Так паттерны задаются в регулярных выражениях, к которым мы вернемся позже. stri_rand_strings(n = 5, length = 5:9, pattern = &quot;[a-z]&quot;) ## [1] &quot;vafxp&quot; &quot;jlazly&quot; &quot;xqzqijk&quot; &quot;ubtregnr&quot; &quot;ztowehvsg&quot; Строго говоря, это не то, что принято называть “псевдословами”. Под псевдословами подразумеваются такие последовательности, которые звучат как настоящие слова, но не имеют смысла. Например, “ошмаска” или “утурник” Ну а функция stri_rand_shuffle() принимает на вход строчку и возвращает ее же, но уже с перемешанными знаками. stri_rand_shuffle(&quot;съешь ещё этих мягких французских булок, да выпей же чаю&quot;) ## [1] &quot;еёхз денх лку пкщх квтсчреу йыьэоиаюжия, а шъгифаб смце&quot; 4.2.6 Сортировка Чтобы сортировать слова в алфавитном порядке, можно воспользоваться generic функцией sort(): sort(unlist(tstrsplit(&quot;съешь ещё этих мягких французских булок, да выпей же чаю&quot;, split = &quot; &quot;))) ## [1] &quot;булок,&quot; &quot;выпей&quot; &quot;да&quot; &quot;ещё&quot; &quot;же&quot; ## [6] &quot;мягких&quot; &quot;съешь&quot; &quot;французских&quot; &quot;чаю&quot; &quot;этих&quot; generic функция в R - это функция, которая по-разному работает для разных объектов (использует соответствующий данному классу метод). Когда функция получает объект, она первым делом смотрит, что это за класс, а потом действует исходя из класса объекта. Например, функции print(), summary() и plot() - они работают почти на любых объектах, по-разному реагируя на объекты разных классов. Поэтому столкнувшись с объектом неизвестного класса в R, попробуйте применить эти функции. Самостоятельное задание: Создайте data.table mon следующего вида: ## n Month ## 1: 1 January ## 2: 2 February ## 3: 3 March ## 4: 4 April ## 5: 5 May ## 6: 6 June ## 7: 7 July ## 8: 8 August ## 9: 9 September ## 10: 10 October ## 11: 11 November ## 12: 12 December Названия месяцов - это еще одна константа, зашитая в R! Она называется month.name. Создайте колонку info следующего вида: “January is the 1 month” … “December is the 12 month” ## [1] &quot;January is the 1 month&quot; &quot;February is the 2 month&quot; ## [3] &quot;March is the 3 month&quot; &quot;April is the 4 month&quot; ## [5] &quot;May is the 5 month&quot; &quot;June is the 6 month&quot; ## [7] &quot;July is the 7 month&quot; &quot;August is the 8 month&quot; ## [9] &quot;September is the 9 month&quot; &quot;October is the 10 month&quot; ## [11] &quot;November is the 11 month&quot; &quot;December is the 12 month&quot; Сократите длину каждого месяца так, чтобы она была не больше 6 символов: ## [1] &quot;Jan...&quot; &quot;Feb...&quot; &quot;March&quot; &quot;April&quot; &quot;May&quot; &quot;June&quot; &quot;July&quot; ## [8] &quot;August&quot; &quot;Sep...&quot; &quot;Oct...&quot; &quot;Nov...&quot; &quot;Dec...&quot; Создайте в mon колонку month, где каждый месяц будет записан с маленькой буквы. ## n Month month ## 1: 1 January january ## 2: 2 February february ## 3: 3 March march ## 4: 4 April april ## 5: 5 May may ## 6: 6 June june ## 7: 7 July july ## 8: 8 August august ## 9: 9 September september ## 10: 10 October october ## 11: 11 November november ## 12: 12 December december Создайте колонку month_anagram, в котором будут анаграммы названия каждого месяца (т.е. в данном случае - перемешанные) из колонки month. set.seed(42) #запустите, чтобы у нас одинаковые рандомизации получились ## n Month month month_anagram ## 1: 1 January january yjuarna ## 2: 2 February february aefrbyur ## 3: 3 March march arhmc ## 4: 4 April april arilp ## 5: 5 May may mya ## 6: 6 June june eujn ## 7: 7 July july uylj ## 8: 8 August august tsaugu ## 9: 9 September september seperembt ## 10: 10 October october orbcote ## 11: 11 November november rbmvonee ## 12: 12 December december bdemerec (*) Ну а теперь сложное задание - создайте функцию is_anagram(), которая проверяет, что два слова являются анаграммами. Подсказка: если в tstrsplit() использовать split = &quot;&quot;, то это разделит строку пол отдельным знакам. is_anagram(&quot;спаниель&quot;, &quot;апельсин&quot;) ## [1] TRUE is_anagram(&quot;скол&quot;, &quot;клок&quot;) ## [1] FALSE (*) Затем эту функцию нужно применить на колонки month и month_anagram. Скорее всего, придется либо векторизовать функцию, либо применить mapply(). mon[, mapply(is_anagram,month, month_anagram)] ## january february march april may june july ## TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## august september october november december ## TRUE TRUE TRUE TRUE TRUE 4.3 Поиск паттернов и регулярные выражения Очень большая часть работы с текстом - это поиск паттернов. В самом простом случае - простых последовательностей. 4.3.1 grep(), gsub() Для этого в R есть очень много функций. Пожалуй, самая распространенная из них - функция grep(). Она выдает индексы значений вектора, в которых находится подходящая подстрока: mon[, grep(&quot;ber&quot;, month)] ## [1] 9 10 11 12 mon[ grep(&quot;ber&quot;, month),] #Следите за запятой! ## n Month month month_anagram ## 1: 9 September september seperembt ## 2: 10 October october orbcote ## 3: 11 November november rbmvonee ## 4: 12 December december bdemerec Можно возвращать и сами значения с помощью параметра value = TRUE: mon[, grep(&quot;ber&quot;, month, value = TRUE)] ## [1] &quot;september&quot; &quot;october&quot; &quot;november&quot; &quot;december&quot; mon[, grep(&quot;ber&quot;, month, value = TRUE, invert = TRUE)] #все остальные ## [1] &quot;january&quot; &quot;february&quot; &quot;march&quot; &quot;april&quot; &quot;may&quot; &quot;june&quot; ## [7] &quot;july&quot; &quot;august&quot; Ну а функция gsub() заменяет найденный паттерн на новый. Давайте сделаем год более веселым: mon[, month_fun := gsub(&quot;ber&quot;, &quot;berfest&quot;, month)] Заметьте, первым параметром функции grep() идет pattern =, а не данные (вектор). Это довольно нетипичное поведение для R. Это “заимствованное” слово для R - изначально grep() появилась в UNIX очень давно и означала «search globally for lines matching the regular expression, and print them». Вот и настала пора поразбираться с регулярными выражениями. Функции grep() и gsub() могут использовать “избегая” регулярных выражений, для этого нужно задать параметр fixed = TRUE. Я очень рекомендую это делать, если Вы еще не освоились с регулярными выражениями, иначе результат этих функций будет казаться непредсказуемым. 4.3.2 Регулярные выражения Регулярные выражения - это целый язык, который позволяет найти любой сложный паттерн в тексте. В основе всей сложной работы с текстом обычно лежат “регулярки”. Поэтому стоит ознакомиться хотя бы с их основами. Регулярные выражения реализованы в очень многих языках программирования, не только в R, так что это довольно универсальный навык. Учить их лучше всего интерактивно: очень удобно смотреть, что в тексте находится по введенному паттерну. В качестве примера я могу посоветовать ресурс regexone. Потренироваться можно на кроссворде. Пакет stringr тоже предоставляет удобный инструмент: функции str_view() (показывает первый найденный паттерн) и str_view_all() (показывает все найденные паттерны). Первый аргумент в них - вектор с данными, второй - паттерн регулярных выражений. Если Все введено верно, то во вкладке Viewer окна RStudio появится данный вектор с выделенными паттернами. Итак, перейдем к синтаксису “регулярок”. Если нужно найти простую последовательность, то здесь все так же, как и обычно. Очень похоже на то, что Вы делаете, если нажимаете Ctrl + F и пытаетесь найти в тексте какое-то ключевое слово: names &lt;- c(&quot;Саня&quot;, &quot;Ваня&quot;, &quot;Даня&quot;, &quot;Женя&quot;, &quot;Аня&quot;, &quot;Андрей&quot;, &quot;Леша&quot;, &quot;Лера&quot;, &quot;Витя&quot;, &quot;Валера&quot;) str_view_all(names, &quot;аня&quot;) Заметьте, “Аня” осталась за бортом, потому что регулярные выражения case-sensitive. Скажем, мы хотим найти паттерны “аня”, написанные как с большой, так и с маленькой буквы. Здесь придут на помощь наборы - возможные варианты букв, которые мы ожидаем увидеть на нужном месте: str_view_all(names, &quot;[аА]ня&quot;) Ура, мы поймали Аню! Наборы можно задавать в целом диапазоне. Например, чтобы задать все буквы кириллицы, нужно задать такой диапазон: [а-яА-ЯёЁ] (ё и Ë находятся за пределами диапазонов а-я и А-Я). str_view_all(names, &quot;[а-яА-ЯёЁ]ня&quot;) Теперь мы поймали еще и Женю. Символ ^ (внутри набора) означает, что мы ожидаем увидеть любые символы, кроме тех, что в наборе. Попробуйте догадаться, какие имена мы поймаем следующими паттернами: &quot;[^а-яА-ЯёЁ]ня&quot;, &quot;[^а]ня&quot;. str_view_all(names, &quot;[^а-яА-ЯёЁ]ня&quot;) str_view_all(names, &quot;[^а]ня&quot;) Иногда нужно найти один из двух паттернов - то есть реализовать что-то вроде логического ИЛИ. В регулярных выражениях тоже такое есть. Более того, для этого нужен тот же оператор, что и в R - |. Например, мы хотим найти все имена, заканчивающиеся на “ня” и начинающиеся на “Ле”: str_view_all(names, &quot;Ле|ня&quot;) Если бы паттерн включал бы в себя еще и варианты с маленькой буквы “л”, то мы бы поймали еще и Валеру: str_view_all(names, &quot;[лЛ]е|ня&quot;) Чтобы этого избежать, мы можем задать, что &quot;[лЛ]е&quot; должно быть в начале строки (с помощью знака &quot;^&quot;), а &quot;ня&quot; - в конце. str_view_all(names, &quot;^[лЛ]е|ня$&quot;) Что если мы хотим найти точку? Давайте попробуем использовать ее в регулярном выражении: hello &lt;- c(&quot;При 534вет.&quot;, &quot;всем&quot;) str_view_all(hello, &quot;.&quot;) Что-то не то. Дело в том, что точка (как и уже знакомые нам некоторые другие символы - []|^$) - это спецсимволы, которые имеют специальную функцию в регулярных выражениях. Конкретно точка означает “любой знак”. Но если нам нужно найти в тексте именно точку или другой спецсимвол, то его нужно экранировать с помощью специального паттерна \\\\ (вне R это \\): str_view_all(hello, &quot;\\\\.&quot;) Для наиболее распространенных последовательностей есть специальные символы. С ними все наоборот, чтобы их использовать, нужно их экранировать. Например, \\\\w выдаст все цифробуквенные (alphanumeric) знаки: str_view_all(hello, &quot;\\\\w&quot;) А \\\\W - все кроме цифробуквенных знаков str_view_all(hello, &quot;\\\\W&quot;) \\\\d - только цифры: str_view_all(hello, &quot;\\\\d&quot;) \\\\D - кроме цифр: str_view_all(hello, &quot;\\\\s&quot;) \\\\s - только пробелы: str_view_all(hello, &quot;\\\\S&quot;) \\\\S - все кроме пробелов: str_view_all(hello, &quot;\\\\D&quot;) Следующий этап наращивания нашей мощи инструментария регулярных выражений - количество паттернов. В общем виде оно задается с помощью фигурных скобочек: {n}: ровно n {n,}: n или больше {,m}: не больше m {n,m}: между n и m long &lt;- &quot;1888 - самый длинный год, записанный в римских цифрах: MDCCCLXXXVIII&quot; str_view_all(long, &quot;C{3}&quot;) str_view_all(long, &quot;X{1,}&quot;) str_view_all(long, &quot;н{1,2}&quot;) Для самых распространенных вариантов количества искомых паттернов есть сокращения: ? = 0 или 1 + = 1 или больше * = 0 или больше Например, чтобы вытащить все последовательности, начинающиеся на Л и заканчивающиеся на а (включая Ла), нужно записать так: str_view_all(names, &quot;^Л.*а$&quot;) Ну а что бы вытащить все последовательности, где стоит или не стоит какой-то символ, то записать нужно так: str_view_all(names, &quot;^Ле.?а$&quot;) Самостоятельное задание: Найдите время в формате “04:30”, “06:59” Проверьте на векторе times: times &lt;- c(&quot;04:30&quot;, &quot;06:59&quot;,&quot;fg:55&quot;,&quot;3345&quot;) Найдите время в формате “04:30”, “06:59”, игнорируя “невозможное” время, например, “19:84” Проверьте на векторе times: times &lt;- c(&quot;04:30&quot;, &quot;06:59&quot;,&quot;fg:55&quot;,&quot;3345&quot;, &quot;19:84&quot;) Найдите время, которое может быть в других форматах: “4:20”, “20-10”, но не “20г21” или “2019” Проверьте на векторе times: times &lt;- c(&quot;04:30&quot;, &quot;06:59&quot;,&quot;fg:55&quot;,&quot;3345&quot;, &quot;19:84&quot;, &quot;20-10&quot;, &quot;4:20&quot;, &quot;20r21&quot;, &quot;2019&quot;) 4.4 Анализ текста - что дальше? Регулярные выражения - это супермощный инструмент. Однако он довольно непростой, да и выглядит совершенно монструозно. Хэдли Уикхэм приводит следующий пример реально используемого кода для поиска в тексте электронных почт: &quot;(?:(?:\\r\\n)?[ \\t])*(?:(?:(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t] )+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?: \\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:( ?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\0 31]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\ ](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+ (?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?: (?:\\r\\n)?[ \\t])*))*|(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z |(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n) ?[ \\t])*)*\\&lt;(?:(?:\\r\\n)?[ \\t])*(?:@(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\ r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n) ?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t] )*))*(?:,@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])* )(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t] )+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*) *:(?:(?:\\r\\n)?[ \\t])*)?(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+ |\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r \\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?: \\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t ]))*&quot;(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031 ]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\]( ?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(? :(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(? :\\r\\n)?[ \\t])*))*\\&gt;(?:(?:\\r\\n)?[ \\t])*)|(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(? :(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)? [ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)*:(?:(?:\\r\\n)?[ \\t])*(?:(?:(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]| \\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt; @,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot; (?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t] )*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\ &quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(? :[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[ \\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*|(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000- \\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|( ?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)*\\&lt;(?:(?:\\r\\n)?[ \\t])*(?:@(?:[^()&lt;&gt;@,; :\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([ ^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot; .\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\ ]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*(?:,@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\ [\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\ r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\] |\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*)*:(?:(?:\\r\\n)?[ \\t])*)?(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\0 00-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\ .|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@, ;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(? :[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t])* (?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;. \\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[ ^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\] ]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*\\&gt;(?:(?:\\r\\n)?[ \\t])*)(?:,\\s*( ?:(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\ &quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:( ?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[ \\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t ])*))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t ])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(? :\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+| \\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*|(?: [^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\ ]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)*\\&lt;(?:(?:\\r\\n) ?[ \\t])*(?:@(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot; ()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n) ?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt; @,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*(?:,@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@, ;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t] )*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\ &quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*)*:(?:(?:\\r\\n)?[ \\t])*)? (?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;. \\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?: \\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[ &quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t]) *))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t]) +|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\ .(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z |(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*\\&gt;(?:( ?:\\r\\n)?[ \\t])*))*)?;\\s*)&quot; Обычно работа с текстовыми данными сводится к нескольким отдельным операциям. Например, нужно вытащить из HTML-файла все теги или наоборот избавиться от них. Для этого, конечно, есть специальные пакеты, например, rvest. Еще иногда нужно делать токенизацию, то есть разбивать текст на токены - значимые единицы текста (обычно слова). Для этого есть пакет tidytext. Обычно затем нужно перевести эти слова в неопределенную форму (лемматизация) или отбросить окончания (стеммизация) - см. пакет SnowballC. Есть удобные инструменты, которые умеют делать это все сразу. Например, пакет udpipe: install.packages(&quot;udpipe&quot;) library(udpipe) Для примера возьмем одну песню Дэвида Боуи и “прогоним” через udpipe::udpipe(): heroes &lt;- b[grep(&quot;Heroes&quot;,song_name)[1],lyrics] tok_heroes &lt;- udpipe(heroes, &quot;english&quot;) head(tok_heroes) ## doc_id paragraph_id sentence_id sentence start end term_id ## 1 doc1 1 1 I, I will be king 1 1 1 ## 2 doc1 1 1 I, I will be king 2 2 2 ## 3 doc1 1 1 I, I will be king 4 4 3 ## 4 doc1 1 1 I, I will be king 6 9 4 ## 5 doc1 1 1 I, I will be king 11 12 5 ## 6 doc1 1 1 I, I will be king 14 17 6 ## token_id token lemma upos xpos ## 1 1 I I PRON PRP ## 2 2 , , PUNCT , ## 3 3 I I PRON PRP ## 4 4 will will AUX MD ## 5 5 be be AUX VB ## 6 6 king king NOUN NN ## feats head_token_id dep_rel deps ## 1 Case=Nom|Number=Sing|Person=1|PronType=Prs 6 nsubj &lt;NA&gt; ## 2 &lt;NA&gt; 6 punct &lt;NA&gt; ## 3 Case=Nom|Number=Sing|Person=1|PronType=Prs 6 nsubj &lt;NA&gt; ## 4 VerbForm=Fin 6 aux &lt;NA&gt; ## 5 VerbForm=Inf 6 cop &lt;NA&gt; ## 6 Number=Sing 0 root &lt;NA&gt; ## misc ## 1 SpaceAfter=No ## 2 &lt;NA&gt; ## 3 &lt;NA&gt; ## 4 &lt;NA&gt; ## 5 &lt;NA&gt; ## 6 &lt;NA&gt; В результате мы получили очень длинный датафрейм, каждая строчка которого - это слово или знак препинания. Выглядит не очень симпатично (на первый взгляд), но с этим очень удобно работать. Например, с помощью функции data.table::merge() можно присоединить к словам сентимент-словарь и сделать сентимент анализ песен Дэвида Боуи. Можно посчитать частотность слов в его песнях. Или же исследовать их сочетание - и сделать топик моделлинг. Освоив базовый инструментарий работы с текстом, перед Вами открывается увлекательный мир анализа естественного языка! 4.5 Fuzzy matching Часто бывает, что два character значения совпадают не полностью. Например, в одном случае слово написано в единственном числе, а в другом - во множественном. Или того хуже - в одном случае слово написано с ошибкой. Такие ситуации особенно часто возникают при соединении несколько баз вместе или же когда данные вводятся вручную. Например, представим, что мы опросили людей, кто их любимый актер. Но вот незадача: некоторые написали имя любимого актера с ошибкой! actors &lt;- c(&quot;Benedict Cumberbatch&quot;, &quot;Bandersnatch Cummerbund&quot;, &quot;Bendenswitch Cumbersquash&quot;, &quot;Bennendim Cumbendatch&quot;, &quot;Bendandsnap Cucumbersnatch&quot;, &quot;Kevin Smith&quot;, &quot;Emma Stone&quot;, &quot;Martin Freeman&quot;) В этой ситуации нам поможет fuzzy matching - примерное (approximate) сопоставление с паттерном. Как же измеряется эта “примерная похожесть”? О, как и всегда в таких случаях, за этим стоит целая наука. Самая распространенный способ посчитать похожесть, точнее, непохожесть - это расстояние Левенштайна. Это количество операций, которые нужно сделать, чтобы получить из одной строки другую. Вот список допустимых операций при расчете расстояния Левенштейна: вставка ab → aNb удаление aOb → ab замена символа aOb → aNb перестановка символов ab → ba Для работы с расстояниями есть встроенная в R функция agrepl(). Однако она дает довольно ограниченные возможности, поэтому мы воспользуемся пакетом stringdist: install.packages(&quot;stringdist&quot;) library(stringdist) Можно посчитать расстояние Левенштайна с помощью функции stringdist(), можно преобразовать расстояния в меры близости с помощью функции stringsim(): stringdist(&quot;Benedict Cumberbatch&quot;, actors) ## [1] 0 13 11 6 11 14 18 16 stringsim(&quot;Benedict Cumberbatch&quot;, actors) ## [1] 1.0000000 0.4347826 0.5600000 0.7142857 0.5769231 0.3000000 0.1000000 ## [8] 0.2000000 actors[stringsim(&quot;Benedict Cumberbatch&quot;, actors)&gt;0.4] ## [1] &quot;Benedict Cumberbatch&quot; &quot;Bandersnatch Cummerbund&quot; ## [3] &quot;Bendenswitch Cumbersquash&quot; &quot;Bennendim Cumbendatch&quot; ## [5] &quot;Bendandsnap Cucumbersnatch&quot; Самостоятельное задание: Найдите в скольки процентах песен Боуи использует слово “star” (в том числе и как часть слова). ## [1] 16.94402 Ого, как много! 2. А теперь найдите все упоминания “star”, в том числе и как часть слова, в песнях Боуи. Посчитайте частоту встречаемости каждого из них. ## ## blackstar filmstar gangstar popstar pornstar star stardust ## 19 1 3 1 1 97 5 ## stare stared stares starman starring stars start ## 30 4 5 5 2 65 18 ## started starting starts starve starving superstar ## 10 1 2 3 3 4 Найдите все варианты песни &quot;Space Oddity&quot; по данной версии с помощью fuzzy matching: space &lt;- b[grep(&quot;Space Oddity&quot;, song_name)[1], lyrics] ## song_id ## 1: 112817 ## 2: 4664522 ## 3: 4216423 ## 4: 4216422 ## 5: 4178279 ## 6: 4133059 ## 7: 4178522 ## song_name ## 1: Space Oddity ## 2: Space Oddity (Clareville Grove Demo) [2019 Remaster] (Ft. John Hutchinson) ## 3: Space Oddity (demo - alternative lyrics) ## 4: Space Oddity (demo excerpt) ## 5: Space Oddity (Live 1972) ## 6: Space Oddity (Live 1973) ## 7: Space Oddity (Live &#39;74) ## song_lyrics_url ## 1: https://genius.com/David-bowie-space-oddity-lyrics ## 2: https://genius.com/David-bowie-space-oddity-clareville-grove-demo-2019-remaster-lyrics ## 3: https://genius.com/David-bowie-space-oddity-demo-alternative-lyrics-lyrics ## 4: https://genius.com/David-bowie-space-oddity-demo-excerpt-lyrics ## 5: https://genius.com/David-bowie-space-oddity-live-1972-lyrics ## 6: https://genius.com/David-bowie-space-oddity-live-1973-lyrics ## 7: https://genius.com/David-bowie-space-oddity-live-74-lyrics ## annotation_count artist_id artist_name ## 1: 25 9534 David Bowie ## 2: 0 9534 David Bowie ## 3: 0 9534 David Bowie ## 4: 0 9534 David Bowie ## 5: 0 9534 David Bowie ## 6: 0 9534 David Bowie ## 7: 0 9534 David Bowie ## artist_url ## 1: https://genius.com/artists/David-bowie ## 2: https://genius.com/artists/David-bowie ## 3: https://genius.com/artists/David-bowie ## 4: https://genius.com/artists/David-bowie ## 5: https://genius.com/artists/David-bowie ## 6: https://genius.com/artists/David-bowie ## 7: https://genius.com/artists/David-bowie ## lyrics ## 1: Ground Control to Major Tom Ground Control to Major Tom Take your protein pills and put your helmet on (Ten) Ground Control (Nine) to Major Tom (Eight, seven) (Six) Commencing (Five) countdown, engines on (Four, three, two) Check ignition (One) and may God&#39;s love (Lift off) be with you This is Ground Control to Major Tom You&#39;ve really made the grade And the papers want to know whose shirt you wear Now it&#39;s time to leave the capsule if you dare This is Major Tom to Ground Control I&#39;m stepping through the door And I&#39;m floating in a most peculiar way And the stars look very different today For here am I sitting in a tin can Far above the world Planet Earth is blue And there&#39;s nothing I can do Though I&#39;m past 100,000 miles I&#39;m feeling very still And I think my spaceship knows which way to go Tell my wife I love her very much She knows Ground Control to Major Tom Your circuit&#39;s dead, there&#39;s something wrong Can you hear me, Major Tom? Can you hear me, Major Tom? Can you hear me, Major Tom? Can you Here am I floating &#39;round my tin can Far above the moon Planet Earth is blue And there&#39;s nothing I can do ## 2: Ground Control to Major Tom Ground Control to Major Tom Take your protein pills And put your helmet on (Ten) Ground Control (Nine) to Major Tom (Eight) (Seven, six) Commencing (Five) countdown Engines on (Four, three, two) Check ignition (One) And may God&#39;s love (Blast off) be with you This is Ground Control to Major Tom You&#39;ve really made the grade And the papers want to know whose shirt you wear Now it&#39;s time to leave the capsule if you dare This is Major Tom to Ground Control I&#39;m stepping through the door And I&#39;m floating in a most peculiar way And the stars look very different today For here am I sitting in a tin can Far above the world Planet Earth is blue And there&#39;s nothing I can do Though I&#39;m past 100,000 miles I&#39;m feeling very still And I think my spaceship knows which way to go Tell my wife I love her very much She knows Ground Control to Major Tom Your circuit&#39;s dead, there&#39;s something wrong Can you hear me, Major Tom? Can you hear me, Major Tom? Can you hear me, Major Tom? Can you Here am I floating &#39;round my tin can Far above the moon Planet Earth is blue And there&#39;s nothing I can do ## 3: John Hutchinson: Nice, nice David Bowie: Didn’t sound nice to me. Let’s remember to do that. Ok, Hutch (Begins playing) DB: Ope, hun. Yeah? JH: What a single! DB: We’re recording, now. Wait, Christie, don’t talk Ground Control to Major Tom Ground Control to Major Tom Take your protein pills And put your helmet on (10) Ground Control (Nine) to Major Tom (Eight) (Seven, six) Commencing (Five) countdown Engines on (Four, three, two) Check ignition (One) And may God&#39;s love (Blast off) be with you This is Major Tom to Ground Control I&#39;m feeling very still And I think my spaceship knows which way to go Tell my wife I love her very much She knows Though I&#39;m passed 100,000 miles I&#39;m feeling very still And I think my spaceship know what I must do And I think my life on earth is never trough Ground Control to Major Tom You&#39;re off your course, direction&#39;s wrong Can you hear me, Major Tom? Can you hear me, Major Tom? Can you hear me, Major Tom? Can you heee... ...eeere am I sitting in a tin can Far above the Moon The planet Earth is blue And there&#39;s nothing I can do… ## 4: This is Major Tom to Ground Control, I’m stepping through the door And I’m floating in the most peculiar way Can I please get back inside now, if I may? For here am I floating ‘round my tin can Far above the world Planet earth is blue, and there’s nothing I can do This is Major Tom to Ground Control, I’m feeling very still And I think my spaceship knows which way to go Tell my wife I love her very much, she knows Though I’m past one hundred thousand miles, I’m feeling very still And I think my spaceship knows what i must do And I think my life on earth is nearly through Ground Control to Major Tom, you’re off your course Direction’s wrong Can you hear me, Major Tom? Can you hear me, Major Tom? Can you hear me, Major Tom? Can you- Here am I sitting in a tin can Far above the moon Planet earth is blue and there’s nothing I can do Do, do do do ## 5: Ground Control to Major Tom Ground Control to Major Tom Take your protein pills And put your helmet on (10) Ground Control (9) to Major Tom (8) (7, 6) Commencing (5) countdown Engines on (4, 3, 2) Check ignition (1) And may God&#39;s love (Liftoff) be with you Ahhh... This is Ground Control to Major Tom You’ve really made the grade And the papers want to know whose shirt you wear Now it&#39;s time to leave the capsule if you dare This is Major Tom to Ground Control I&#39;m stepping through the door And I’m floating in a most peculiar way And the stars look very different today For here am I sitting in a tin can Far above the world Planet Earth is blue And there&#39;s nothing I can do Though I&#39;m past one hundred thousand miles I&#39;m feeling very still And I think my spaceship knows which way to go Tell my wife I love her very much She knows Ground Control to Major Tom Your circuit&#39;s dead, there&#39;s something wrong Can you hear me, Major Tom? Can you hear me, Major Tom? Can you hear me, Major Tom? Can you- Here am I floating &#39;round my tin can Far above the moon Planet Earth is blue And there&#39;s nothing I can do Ohhh Duh-duh-duh-duh-duh-duh-duh-duh-duh-duh Cha-cha-cha-cha-cha-cha-cha-cha-cha-cha Cha-cha-cha-cha-cha-cha-cha-cha-cha-cha Thank You ## 6: Ground Control to Major Tom Ground Control to Major Tom Take your protein pills And put your helmet on Ground Control to Major Tom Commencing countdown Engines on Check ignition And may God&#39;s love be with you This is Ground Control to Major Tom You’ve really made the grade And the papers want to know who&#39;s shirt you wear Now it&#39;s time to leave the capsule if you dare This is Major Tom to Ground Control I’m stepping through the door And I&#39;m floating in a most peculiar way Now the stars look very different today For here am I sitting in my tin can Far above the world Planet Earth is blue And there&#39;s nothing I can do Though I&#39;m past one hundred thousand miles I&#39;m feeling very still And I think my spaceship knows which way to go Tell my wife I love her very much She knows Ground Control to Major Tom Your circuit&#39;s dead, there&#39;s something wrong Can you hear me, Major Tom? Can you hear me, Major Tom? Can you hear me, Major Tom? Can you Here am I floating &#39;round my tin can Far above the moon Planet Earth is blue And there’s nothing I can do Shhh... Shh... ## 7: Ground Control to Major Tom Ground Control to Major Tom Take your protein pills And put your helmet on Ground Control to Major Tom Commencing countdown Engines on Check ignition And may God&#39;s love be with you This is Ground Control to Major Tom You&#39;ve really made the grade And the papers want to know whose shirt you wear Now it&#39;s time to leave the capsule if you dare This is Major Tom to Ground Control I&#39;m stepping through the door And I&#39;m floating in a most peculiar way And the stars look very different today For here am I sitting in a tin can Far above the world Planet Earth is blue And there&#39;s nothing I can do Though I&#39;m past one hundred thousand miles I&#39;m feeling very still And I think my spaceship knows which way to go Tell my wife I love her very much She knows Ground Control to Major Tom Your circuit&#39;s dead, there&#39;s something wrong Can you hear me, Major Tom? Can you hear me, Major Tom? Can you hear me, Major Tom? Can you Here am I floating &#39;round my tin can Far above the moon Planet Earth is blue And there&#39;s nothing I can do ## n_letters song_name_trunc ## 1: 1113 Space Oddity ## 2: 1113 Space Oddity... ## 3: 1071 Space Oddity... ## 4: 852 Space Oddity... ## 5: 1240 Space Oddity... ## 6: 1064 Space Oddity... ## 7: 1048 Space Oddity... ## song_name_pad ## 1: Space Oddity ## 2: Space Oddity (Clareville Grove Demo) [2019 Remaster] (Ft. John Hutchinson) ## 3: Space Oddity (demo - alternative lyrics) ## 4: Space Oddity (demo excerpt) ## 5: Space Oddity (Live 1972) ## 6: Space Oddity (Live 1973) ## 7: Space Oddity (Live &#39;74) ## song_name_trimmed ## 1: Space Oddity ## 2: Space Oddity (Clareville Grove Demo) [2019 Remaster] (Ft. John Hutchinson) ## 3: Space Oddity (demo - alternative lyrics) ## 4: Space Oddity (demo excerpt) ## 5: Space Oddity (Live 1972) ## 6: Space Oddity (Live 1973) ## 7: Space Oddity (Live &#39;74) ## sim ## 1: 1.0000000 ## 2: 0.9874214 ## 3: 0.4294699 ## 4: 0.4618149 ## 5: 0.8282258 ## 6: 0.8867925 ## 7: 0.9092543 "],
["d4.html", "5 Неделя 2, День 4 5.1 Многомерные методы анализа данных 5.2 Хитмап корреляций 5.3 Анализ главных компонент (Principal component analysis) 5.4 Эксплораторный факторный анализ 5.5 Конфирматорный факторный анализ 5.6 Другие многомерные методы", " 5 Неделя 2, День 4 5.1 Многомерные методы анализа данных Сегодняшнее занятие будет посвящено многомерным методам анализа данных - методам работы с данными, в которых много колонок. Мы уже сталкивались с некоторыми многомерными методами, такими как множественная линейная регрессия. Поэтому вы знаете, что многомерность создает новые проблемы. Например, при множественных корреляциях или попарных сравнениях возникает проблема множественных сравнений, а при использовании множественной регрессии лишние предикторы могут ловить только шум и приводить к переобучению (если говорить в терминах машинного обучения). Короче говоря, больше - не значит лучше. Нужно четко понимать, зачем мы используем данные и что пытаемся измерить. Однако в некоторых случаях мы в принципе не можем ничего интересного сделать с маленьким набором переменных. Много ли мы можем измерить личностным тестом с одним единственным вопросом? Можем ли мы точно оценить уровень интеллекта по успешности выполнения одного единственного задания? Очевидно, что нет. Более того, даже концепция интеллекта в современном его представлении появилась во многом благодаря разработке многомерных методов анализа! Ну или наоборот: исследования интеллекта подстегнули развитие многомерных методов. Наши данные как раз очень многомерные. Это данные опроса с использованием очень большого количества вопросов. Мы возьмем только колонки, содержащие в названии “Op” - это ответы на всякие мировоззренческие вопросы, а это значит, что на них нет правильного ответа, а разные люди будут отвечать по-разному. Эти вопросы связаны с отношением к науке и технологиям, к “научному мировоззрению”. Например, “Scientific development is essential for improving people’s lives” и “Consuming genetically modified (GMO) food is perfectly safe”. Очевидно, что ответы на все эти вопросы будут как-то скоррелированы (положительно или отрицательно). С помощью таких методов как анализ главных компонент и факторный анализ можно уменьшить количество переменных и попытаться понять, что стоит на паттерном различий в ответах респондентов. Самостоятельное задание: Сделайте data.table opinion, в котором будут только колонки с &quot;OP&quot; или &quot;Op&quot; в названии. library(data.table) data &lt;- fread(&quot;data/iGLAS for R course.csv&quot;) Для этого можно воспользоваться регулярными выражениями! op &lt;- grep(&quot;iO[pP]&quot;, names(data), value = TRUE) opinion &lt;- data[, ..op] Удалите все колонки, где слишком много NA, потом удалите строчки с NA: Разобьем задачу на части: сначала создадим функцию, которая считает частоту NA в колонке: frac_na &lt;- function(x) mean(is.na(x)) Потом применим эту фунцкию, чтобы выделить названия колонок, в которых NA меньше половины: not_na_op &lt;- sapply(opinion, frac_na) &lt; 0.5 opinion &lt;- opinion[, ..not_na_op] Осталось удалить относительно небольшое количество строчек с NA: opinion &lt;- opinion[complete.cases(opinion),] Посчитайте матрицу корреляций opinion: Напоминаю, что это можно сделать несколькими способами. Конечно, можно просто коррелировать каждую пару переменных с каждой: opinion[, cor.test(iOp01, iOp03)] ## ## Pearson&#39;s product-moment correlation ## ## data: iOp01 and iOp03 ## t = 4.3619, df = 4981, p-value = 1.316e-05 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.03397746 0.08929939 ## sample estimates: ## cor ## 0.06168581 Из этих данных можно доставать значений корреляций - и так повторить для каждой пары переменных. Это не очень удобно. Можно сделать проще - с помощью функции cor() посчитать всю матрицу корреляций. opinion_cor &lt;- cor(opinion) opinion_cor ## iOp01 iOp03 iOp04 iOp10 iOp11 ## iOp01 1.000000000 0.06168581 -0.09790765 0.09710549 -0.005725227 ## iOp03 0.061685805 1.00000000 -0.11774907 0.38227617 -0.125308613 ## iOp04 -0.097907651 -0.11774907 1.00000000 -0.30967796 0.265454316 ## iOp10 0.097105491 0.38227617 -0.30967796 1.00000000 -0.237845009 ## iOp11 -0.005725227 -0.12530861 0.26545432 -0.23784501 1.000000000 ## iOp02.5 -0.032672387 -0.10008960 0.30204766 -0.22103331 0.211416777 ## iOp42 0.181658491 0.04159865 0.04141262 -0.03209103 0.082770905 ## iOp43 -0.003794763 -0.02254635 0.12403584 -0.05257601 0.132697538 ## iOp46 0.052547325 0.17397235 -0.14585023 0.22097407 -0.086064297 ## iOp45 0.134881909 0.14934259 -0.41026076 0.28213000 -0.192350246 ## iOP13.5 -0.040780421 -0.19078695 0.32713384 -0.29964666 0.215998965 ## iOp44 0.004436168 0.03011891 0.07615861 -0.04030566 0.113388468 ## iOp07.5 0.027425667 -0.07286297 0.18466025 -0.15426662 0.259294932 ## iOp47 -0.012785049 -0.07423265 0.33009538 -0.27045062 0.312349968 ## iOP12.5 0.168927842 0.12041632 -0.08985733 0.14446427 -0.068279608 ## iOp02.5 iOp42 iOp43 iOp46 iOp45 ## iOp01 -0.03267239 0.181658491 -0.003794763 0.05254732 0.134881909 ## iOp03 -0.10008960 0.041598655 -0.022546352 0.17397235 0.149342590 ## iOp04 0.30204766 0.041412616 0.124035835 -0.14585023 -0.410260758 ## iOp10 -0.22103331 -0.032091028 -0.052576008 0.22097407 0.282129998 ## iOp11 0.21141678 0.082770905 0.132697538 -0.08606430 -0.192350246 ## iOp02.5 1.00000000 0.136295690 0.124001544 -0.15602734 -0.209353362 ## iOp42 0.13629569 1.000000000 0.232244315 -0.03223088 -0.006915756 ## iOp43 0.12400154 0.232244315 1.000000000 -0.05692068 -0.047643316 ## iOp46 -0.15602734 -0.032230885 -0.056920680 1.00000000 0.173623846 ## iOp45 -0.20935336 -0.006915756 -0.047643316 0.17362385 1.000000000 ## iOP13.5 0.39518963 0.134509434 0.098597768 -0.15017812 -0.214800604 ## iOp44 0.07653149 0.099007929 0.108009257 -0.03229476 -0.046154797 ## iOp07.5 0.23017497 0.207500065 0.248239874 -0.10794585 -0.083293739 ## iOp47 0.25021173 0.161576936 0.113803852 -0.10644965 -0.241338330 ## iOP12.5 -0.07638594 0.094466167 -0.032880878 0.14201499 0.093801862 ## iOP13.5 iOp44 iOp07.5 iOp47 iOP12.5 ## iOp01 -0.04078042 0.004436168 0.02742567 -0.012785049 0.168927842 ## iOp03 -0.19078695 0.030118914 -0.07286297 -0.074232647 0.120416318 ## iOp04 0.32713384 0.076158609 0.18466025 0.330095376 -0.089857326 ## iOp10 -0.29964666 -0.040305663 -0.15426662 -0.270450622 0.144464274 ## iOp11 0.21599896 0.113388468 0.25929493 0.312349968 -0.068279608 ## iOp02.5 0.39518963 0.076531485 0.23017497 0.250211729 -0.076385936 ## iOp42 0.13450943 0.099007929 0.20750007 0.161576936 0.094466167 ## iOp43 0.09859777 0.108009257 0.24823987 0.113803852 -0.032880878 ## iOp46 -0.15017812 -0.032294760 -0.10794585 -0.106449653 0.142014989 ## iOp45 -0.21480060 -0.046154797 -0.08329374 -0.241338330 0.093801862 ## iOP13.5 1.00000000 0.100923055 0.21146886 0.315878637 -0.063165249 ## iOp44 0.10092306 1.000000000 0.10301330 0.103123900 -0.014437449 ## iOp07.5 0.21146886 0.103013296 1.00000000 0.296110105 -0.008059640 ## iOp47 0.31587864 0.103123900 0.29611011 1.000000000 0.005139235 ## iOP12.5 -0.06316525 -0.014437449 -0.00805964 0.005139235 1.000000000 Ну а можно воспользоваться пакетом &quot;psych&quot;, который создаст специальный объект классов &quot;psych&quot; и &quot;corr.test&quot;. Внутри этого объекта будут и матрица корреляций и матрица p-values. library(psych) opinion_corr &lt;- corr.test(opinion) class(opinion_corr) ## [1] &quot;psych&quot; &quot;corr.test&quot; class(opinion_corr$r) ## [1] &quot;matrix&quot; Если Вы решите проверить, совпадают ли результаты этих двух способов, то увидите, что, внезапно, нет: opinion_corr$r == opinion_cor ## iOp01 iOp03 iOp04 iOp10 iOp11 iOp02.5 iOp42 iOp43 iOp46 iOp45 ## iOp01 TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE ## iOp03 FALSE TRUE FALSE TRUE TRUE FALSE TRUE FALSE TRUE FALSE ## iOp04 TRUE FALSE TRUE FALSE FALSE FALSE TRUE FALSE TRUE FALSE ## iOp10 TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE ## iOp11 TRUE TRUE FALSE FALSE TRUE FALSE TRUE FALSE FALSE TRUE ## iOp02.5 TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE ## iOp42 TRUE TRUE TRUE FALSE TRUE FALSE TRUE TRUE FALSE TRUE ## iOp43 TRUE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE ## iOp46 FALSE TRUE TRUE FALSE FALSE TRUE FALSE TRUE TRUE FALSE ## iOp45 FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE FALSE TRUE ## iOP13.5 FALSE TRUE FALSE TRUE TRUE FALSE TRUE FALSE TRUE TRUE ## iOp44 TRUE FALSE TRUE FALSE FALSE FALSE TRUE FALSE TRUE FALSE ## iOp07.5 TRUE FALSE FALSE FALSE TRUE TRUE FALSE FALSE TRUE TRUE ## iOp47 FALSE TRUE TRUE FALSE TRUE FALSE TRUE FALSE TRUE TRUE ## iOP12.5 FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE FALSE ## iOP13.5 iOp44 iOp07.5 iOp47 iOP12.5 ## iOp01 FALSE TRUE TRUE FALSE FALSE ## iOp03 TRUE FALSE FALSE TRUE FALSE ## iOp04 FALSE TRUE FALSE TRUE FALSE ## iOp10 TRUE FALSE FALSE FALSE FALSE ## iOp11 TRUE FALSE TRUE TRUE FALSE ## iOp02.5 FALSE FALSE TRUE FALSE TRUE ## iOp42 TRUE TRUE FALSE TRUE FALSE ## iOp43 FALSE FALSE FALSE FALSE TRUE ## iOp46 TRUE TRUE TRUE TRUE TRUE ## iOp45 TRUE FALSE TRUE TRUE FALSE ## iOP13.5 TRUE FALSE TRUE FALSE TRUE ## iOp44 FALSE TRUE TRUE FALSE FALSE ## iOp07.5 TRUE TRUE TRUE TRUE FALSE ## iOp47 FALSE FALSE TRUE TRUE FALSE ## iOP12.5 TRUE FALSE FALSE FALSE TRUE Причина в том, что дробные числа хранятся в компьютере как степени двойки с разной степенью точности - поэтому могут возникать такие вот небольшие различия. А вот если округлить, то все значения окажутся одинаковыми: round(opinion_corr$r,2) == round(opinion_cor,2) ## iOp01 iOp03 iOp04 iOp10 iOp11 iOp02.5 iOp42 iOp43 iOp46 iOp45 ## iOp01 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## iOp03 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## iOp04 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## iOp10 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## iOp11 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## iOp02.5 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## iOp42 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## iOp43 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## iOp46 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## iOp45 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## iOP13.5 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## iOp44 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## iOp07.5 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## iOp47 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## iOP12.5 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## iOP13.5 iOp44 iOp07.5 iOp47 iOP12.5 ## iOp01 TRUE TRUE TRUE TRUE TRUE ## iOp03 TRUE TRUE TRUE TRUE TRUE ## iOp04 TRUE TRUE TRUE TRUE TRUE ## iOp10 TRUE TRUE TRUE TRUE TRUE ## iOp11 TRUE TRUE TRUE TRUE TRUE ## iOp02.5 TRUE TRUE TRUE TRUE TRUE ## iOp42 TRUE TRUE TRUE TRUE TRUE ## iOp43 TRUE TRUE TRUE TRUE TRUE ## iOp46 TRUE TRUE TRUE TRUE TRUE ## iOp45 TRUE TRUE TRUE TRUE TRUE ## iOP13.5 TRUE TRUE TRUE TRUE TRUE ## iOp44 TRUE TRUE TRUE TRUE TRUE ## iOp07.5 TRUE TRUE TRUE TRUE TRUE ## iOp47 TRUE TRUE TRUE TRUE TRUE ## iOP12.5 TRUE TRUE TRUE TRUE TRUE Давайте отдельно посмотрим на матрицу p-values (в нижнем углу - сами p-values, в верхнем - с коррекцией на множество сравнений): round(opinion_corr$p, 3) ## iOp01 iOp03 iOp04 iOp10 iOp11 iOp02.5 iOp42 iOp43 iOp46 iOp45 ## iOp01 0.000 0.000 0.000 0.000 1 0.324 0.000 1.000 0.005 0.000 ## iOp03 0.000 0.000 0.000 0.000 0 0.000 0.066 1.000 0.000 0.000 ## iOp04 0.000 0.000 0.000 0.000 0 0.000 0.066 0.000 0.000 0.000 ## iOp10 0.000 0.000 0.000 0.000 0 0.000 0.324 0.005 0.000 0.000 ## iOp11 0.686 0.000 0.000 0.000 0 0.000 0.000 0.000 0.000 0.000 ## iOp02.5 0.021 0.000 0.000 0.000 0 0.000 0.000 0.000 0.000 0.000 ## iOp42 0.000 0.003 0.003 0.023 0 0.000 0.000 0.000 0.324 1.000 ## iOp43 0.789 0.112 0.000 0.000 0 0.000 0.000 0.000 0.001 0.017 ## iOp46 0.000 0.000 0.000 0.000 0 0.000 0.023 0.000 0.000 0.000 ## iOp45 0.000 0.000 0.000 0.000 0 0.000 0.626 0.001 0.000 0.000 ## iOP13.5 0.004 0.000 0.000 0.000 0 0.000 0.000 0.000 0.000 0.000 ## iOp44 0.754 0.033 0.000 0.004 0 0.000 0.000 0.000 0.023 0.001 ## iOp07.5 0.053 0.000 0.000 0.000 0 0.000 0.000 0.000 0.000 0.000 ## iOp47 0.367 0.000 0.000 0.000 0 0.000 0.000 0.000 0.000 0.000 ## iOP12.5 0.000 0.000 0.000 0.000 0 0.000 0.000 0.020 0.000 0.000 ## iOP13.5 iOp44 iOp07.5 iOp47 iOP12.5 ## iOp01 0.072 1.000 0.529 1.000 0.000 ## iOp03 0.000 0.368 0.000 0.000 0.000 ## iOp04 0.000 0.000 0.000 0.000 0.000 ## iOp10 0.000 0.075 0.000 0.000 0.000 ## iOp11 0.000 0.000 0.000 0.000 0.000 ## iOp02.5 0.000 0.000 0.000 0.000 0.000 ## iOp42 0.000 0.000 0.000 0.000 0.000 ## iOp43 0.000 0.000 0.000 0.000 0.324 ## iOp46 0.000 0.324 0.000 0.000 0.000 ## iOp45 0.000 0.023 0.000 0.000 0.000 ## iOP13.5 0.000 0.000 0.000 0.000 0.000 ## iOp44 0.000 0.000 0.000 0.000 1.000 ## iOp07.5 0.000 0.000 0.000 0.000 1.000 ## iOp47 0.000 0.000 0.000 0.000 1.000 ## iOP12.5 0.000 0.308 0.569 0.717 0.000 5.2 Хитмап корреляций Как видите, почти все коррелирует друг с другом, даже с учетом поправок. Такие множественные корреляции лучше всего смотреть с помощью хитмап-визуализации: install.packages(&quot;corrplot&quot;) library(corrplot) corrplot(opinion_cor, method = &quot;color&quot;, order = &quot;hclust&quot;) Каждый квадратик - корреляция двух колонок. Синяя - положительная корреляция, красная - отрицательная. Чем насыщеннее цвет, тем сильнее корреляция (то есть выше значение по модулю). Колонки автоматически переставляются в нужном порядке для “группировки”. На картинке можно увидеть, что есть две основных группы вопросов, которые положительно коррелируют внутри группы друг с другом и отрицательно - с вопросами из другой группы. Различиные многомерные методы анализа позволяют нам выйти за пределы анализа отдельных пар корреляций и попытаться понять структуру, которая стоит за этой корреляционной матрицей. Поэтому во многих многомерных методах анализа данных именно матрица корреляций выступает в роли входных данных. 5.3 Анализ главных компонент (Principal component analysis) Анализ главных компонент (АГК) известен как метод “уменьшения размерности”. Представьте, что вам дан набор данных с большим количеством количеством похожих переменных… Ох, надо же, наш датасет как раз именно такой! Действительно, подобная ситуация часто возникает в опросниковых данных. Для начала представим ответы на вопросы как точки в многомерном пространстве. Ну, многомерные пространтсва представлять сложно, но вот два измерения - вполне, получится стандартная диаграмма рассеяния. Суть АГК в том, чтобы повернуть оси этого пространства так, чтобы первые оси объясняли как можно больший разброс данных, а последние - как можно меньший. Тогда мы могли бы отбросить последние оси и не очень-то многое потерять в данных. Для двух осей это выглядит вот так: Первая ось должна минимизировать красные расстояния. Вторая ось будет просто перпендикулярна первой оси. Математически, АГК - это нахождение собственных векторов и собственных значений матрицы корреляций или ковариаций. Собственные вектора - это такие особенные вектора матрицы, умножив которые на данную матрицу, можно получить тот же самый вектор (т.е. того же направления), но другой длины. А вот коэффициент множителя длины нового вектора - это собственное значение. В контексте АГК, собственные вектора - это новые оси (т.е. те самые новые компоненты), а собственные значения - это размер объясняемой дисперсии с помощью новых осей. Итак, для начала нам нужно центрировать и нормировать данные - вычесть среднее и поделить на стандартное отклонение, т.е. посчитать z-оценки. Это нужно для того, чтобы сделать все шкалы равноценными. Это особенно важно делать когда разные шкалы используют несопоставимые единицы измерения. Скажем, одна колонка - это масса человека в килограммах, а другая - рост в метрах. Если применять АГК на этих данных, то ничего хорошего не выйдет: вклад роста будет слишком маленьким. А вот если мы сделаем z-преобразование, то приведем и вес, и рост к “общему знаменателю”. В нашем случае это не так критично, поскольку во всех случаях испытуемые отвечают по одинаковой 7-балльной шкале. opinion_scaled &lt;- as.data.table(scale(opinion)) В базовом R уже есть инструменты для АГК princomp() и prcomp(), считают они немного по-разному. Возьмем более рекомендуемый вариант, prcomp(). prcomp(opinion_scaled) ## Standard deviations (1, .., p=15): ## [1] 1.7822858 1.2706276 1.0404145 1.0373031 0.9620233 0.9594423 0.9312585 ## [8] 0.9151956 0.8932286 0.8511995 0.8189853 0.8003226 0.7568713 0.7509609 ## [15] 0.7274690 ## ## Rotation (n x k) = (15 x 15): ## PC1 PC2 PC3 PC4 PC5 ## iOp01 -0.06943778 0.38155798 -0.19427723 0.4915882664 0.19803386 ## iOp03 -0.20562301 0.30500547 0.47716338 -0.2907785685 -0.20900299 ## iOp04 0.36177159 -0.08071718 0.34244974 -0.0003028303 -0.11323021 ## iOp10 -0.34078095 0.23989272 0.19732683 -0.2139680548 -0.17104896 ## iOp11 0.29931733 0.09490561 0.09826912 -0.0213681852 0.23233088 ## iOp02.5 0.32443413 0.07300092 0.07894631 0.0320344221 -0.22999432 ## iOp42 0.12845659 0.48677246 -0.23754435 0.0526953352 -0.14183215 ## iOp43 0.16507540 0.31259574 -0.28949688 -0.4294394393 -0.28394314 ## iOp46 -0.20512296 0.17552225 0.40029547 0.0300032371 0.04153754 ## iOp45 -0.29856014 0.20970290 -0.31605182 -0.0187235987 0.07757389 ## iOP13.5 0.35202797 0.02718279 0.05356841 0.1507532470 -0.03959969 ## iOp44 0.11226061 0.20192394 0.07709088 -0.4107140003 0.80179338 ## iOp07.5 0.26953643 0.30979313 -0.15452816 -0.1165836764 -0.10753154 ## iOp47 0.33900566 0.15729302 0.24253529 0.1122834985 0.03911805 ## iOP12.5 -0.10942316 0.33363638 0.26570013 0.4695379747 0.04614479 ## PC6 PC7 PC8 PC9 PC10 ## iOp01 -0.15062622 0.086873091 -0.36382134 0.434219525 -0.294884944 ## iOp03 -0.23181854 0.028323653 -0.34924941 -0.041639577 0.099043057 ## iOp04 -0.05029616 0.211504124 0.01004747 0.199516759 -0.159180605 ## iOp10 -0.13095970 -0.053855368 -0.17025640 -0.051893607 -0.186371938 ## iOp11 0.46069587 -0.051935276 -0.38659529 0.074794408 -0.174518794 ## iOp02.5 -0.36039154 -0.451433548 0.02531913 0.005203201 -0.301394416 ## iOp42 -0.19693292 0.151161377 0.15029456 0.208217043 0.586642570 ## iOp43 0.18770535 0.248241667 0.31276168 0.154365239 -0.362684643 ## iOp46 0.44736652 -0.352974058 0.39556974 0.473486940 0.079459941 ## iOp45 0.11041482 -0.512305517 -0.04298693 -0.213770985 0.023419463 ## iOP13.5 -0.26367332 -0.425241487 0.20911238 -0.010784156 -0.004230272 ## iOp44 -0.28023510 0.019551048 0.17605295 -0.045507654 -0.042046820 ## iOp07.5 0.30741820 -0.121070383 -0.17902370 -0.366582757 -0.039498614 ## iOp47 0.17602641 0.006911587 -0.16548973 -0.174333373 0.417089629 ## iOP12.5 0.04333967 0.264739933 0.39189684 -0.507508839 -0.249004492 ## PC11 PC12 PC13 PC14 PC15 ## iOp01 0.268041765 -0.10759740 -0.017454400 0.05583458 -0.07759637 ## iOp03 -0.028268175 -0.14763906 -0.207863748 -0.27700923 -0.42210555 ## iOp04 0.189110923 -0.09478334 -0.006973371 -0.52100969 0.55114105 ## iOp10 -0.047812745 0.09035923 0.551647315 0.34238261 0.44032361 ## iOp11 -0.639556895 0.07498348 0.119986008 -0.07704203 -0.04334111 ## iOp02.5 -0.162737844 0.26521465 -0.464150949 0.28093279 0.11355379 ## iOp42 -0.293136056 0.26303996 0.050047991 -0.12403748 0.14999167 ## iOp43 -0.029759426 -0.37536418 -0.058696637 0.13243197 -0.11563024 ## iOp46 0.156393341 0.14514663 -0.053539827 0.04183396 -0.04903883 ## iOp45 -0.074520395 -0.37759488 -0.197166575 -0.35794150 0.35036950 ## iOP13.5 -0.008700931 -0.29354808 0.586329244 -0.15432949 -0.31640366 ## iOp44 0.088893908 0.05750762 -0.038890456 0.02630346 0.04428991 ## iOp07.5 0.516696787 0.44286099 0.106674560 -0.14620017 -0.09660579 ## iOp47 0.174231721 -0.46253063 -0.123396692 0.48427005 0.17481876 ## iOP12.5 -0.177910829 0.01561185 -0.051961030 -0.05844298 -0.01982064 В принципе, z-преобразование было делать необязательно, prcomp() умеет делать это сам: pr &lt;- prcomp(opinion, center = TRUE, scale. = TRUE) pr ## Standard deviations (1, .., p=15): ## [1] 1.7822858 1.2706276 1.0404145 1.0373031 0.9620233 0.9594423 0.9312585 ## [8] 0.9151956 0.8932286 0.8511995 0.8189853 0.8003226 0.7568713 0.7509609 ## [15] 0.7274690 ## ## Rotation (n x k) = (15 x 15): ## PC1 PC2 PC3 PC4 PC5 ## iOp01 -0.06943778 0.38155798 -0.19427723 0.4915882664 0.19803386 ## iOp03 -0.20562301 0.30500547 0.47716338 -0.2907785685 -0.20900299 ## iOp04 0.36177159 -0.08071718 0.34244974 -0.0003028303 -0.11323021 ## iOp10 -0.34078095 0.23989272 0.19732683 -0.2139680548 -0.17104896 ## iOp11 0.29931733 0.09490561 0.09826912 -0.0213681852 0.23233088 ## iOp02.5 0.32443413 0.07300092 0.07894631 0.0320344221 -0.22999432 ## iOp42 0.12845659 0.48677246 -0.23754435 0.0526953352 -0.14183215 ## iOp43 0.16507540 0.31259574 -0.28949688 -0.4294394393 -0.28394314 ## iOp46 -0.20512296 0.17552225 0.40029547 0.0300032371 0.04153754 ## iOp45 -0.29856014 0.20970290 -0.31605182 -0.0187235987 0.07757389 ## iOP13.5 0.35202797 0.02718279 0.05356841 0.1507532470 -0.03959969 ## iOp44 0.11226061 0.20192394 0.07709088 -0.4107140003 0.80179338 ## iOp07.5 0.26953643 0.30979313 -0.15452816 -0.1165836764 -0.10753154 ## iOp47 0.33900566 0.15729302 0.24253529 0.1122834985 0.03911805 ## iOP12.5 -0.10942316 0.33363638 0.26570013 0.4695379747 0.04614479 ## PC6 PC7 PC8 PC9 PC10 ## iOp01 -0.15062622 0.086873091 -0.36382134 0.434219525 -0.294884944 ## iOp03 -0.23181854 0.028323653 -0.34924941 -0.041639577 0.099043057 ## iOp04 -0.05029616 0.211504124 0.01004747 0.199516759 -0.159180605 ## iOp10 -0.13095970 -0.053855368 -0.17025640 -0.051893607 -0.186371938 ## iOp11 0.46069587 -0.051935276 -0.38659529 0.074794408 -0.174518794 ## iOp02.5 -0.36039154 -0.451433548 0.02531913 0.005203201 -0.301394416 ## iOp42 -0.19693292 0.151161377 0.15029456 0.208217043 0.586642570 ## iOp43 0.18770535 0.248241667 0.31276168 0.154365239 -0.362684643 ## iOp46 0.44736652 -0.352974058 0.39556974 0.473486940 0.079459941 ## iOp45 0.11041482 -0.512305517 -0.04298693 -0.213770985 0.023419463 ## iOP13.5 -0.26367332 -0.425241487 0.20911238 -0.010784156 -0.004230272 ## iOp44 -0.28023510 0.019551048 0.17605295 -0.045507654 -0.042046820 ## iOp07.5 0.30741820 -0.121070383 -0.17902370 -0.366582757 -0.039498614 ## iOp47 0.17602641 0.006911587 -0.16548973 -0.174333373 0.417089629 ## iOP12.5 0.04333967 0.264739933 0.39189684 -0.507508839 -0.249004492 ## PC11 PC12 PC13 PC14 PC15 ## iOp01 0.268041765 -0.10759740 -0.017454400 0.05583458 -0.07759637 ## iOp03 -0.028268175 -0.14763906 -0.207863748 -0.27700923 -0.42210555 ## iOp04 0.189110923 -0.09478334 -0.006973371 -0.52100969 0.55114105 ## iOp10 -0.047812745 0.09035923 0.551647315 0.34238261 0.44032361 ## iOp11 -0.639556895 0.07498348 0.119986008 -0.07704203 -0.04334111 ## iOp02.5 -0.162737844 0.26521465 -0.464150949 0.28093279 0.11355379 ## iOp42 -0.293136056 0.26303996 0.050047991 -0.12403748 0.14999167 ## iOp43 -0.029759426 -0.37536418 -0.058696637 0.13243197 -0.11563024 ## iOp46 0.156393341 0.14514663 -0.053539827 0.04183396 -0.04903883 ## iOp45 -0.074520395 -0.37759488 -0.197166575 -0.35794150 0.35036950 ## iOP13.5 -0.008700931 -0.29354808 0.586329244 -0.15432949 -0.31640366 ## iOp44 0.088893908 0.05750762 -0.038890456 0.02630346 0.04428991 ## iOp07.5 0.516696787 0.44286099 0.106674560 -0.14620017 -0.09660579 ## iOp47 0.174231721 -0.46253063 -0.123396692 0.48427005 0.17481876 ## iOP12.5 -0.177910829 0.01561185 -0.051961030 -0.05844298 -0.01982064 summary() выдаст полезную информацию, в частности, долю объясненной дисперсии и кумулятивную долю объясненной дисперсии summary(pr) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 ## Standard deviation 1.7823 1.2706 1.04041 1.03730 0.9620 0.95944 ## Proportion of Variance 0.2118 0.1076 0.07216 0.07173 0.0617 0.06137 ## Cumulative Proportion 0.2118 0.3194 0.39157 0.46330 0.5250 0.58637 ## PC7 PC8 PC9 PC10 PC11 PC12 ## Standard deviation 0.93126 0.91520 0.89323 0.8512 0.81899 0.8003 ## Proportion of Variance 0.05782 0.05584 0.05319 0.0483 0.04472 0.0427 ## Cumulative Proportion 0.64418 0.70002 0.75321 0.8015 0.84623 0.8889 ## PC13 PC14 PC15 ## Standard deviation 0.75687 0.7510 0.72747 ## Proportion of Variance 0.03819 0.0376 0.03528 ## Cumulative Proportion 0.92712 0.9647 1.00000 5.3.1 Количество извлекаемых компонент Визуализация объясненных дисперсий с помощью каждого фактора может использоваться для того, чтобы решить, какие оси оставить, а какие отбросить. plot(pr) Если после какой-то шкалы график резко “падает” и становится ровным, то можно предположить, что эти последние шкалы представляют некоторый “шум” в данных - их мы и собираемся отбросить. В пакете psych есть функция fa.parallel(), которая позволяет не только визуализировать, но и дает пользователю количество рекомендуемых компонент. Важно понимать, что эти границы условны, поэтому не стоит безусловно доверять этой функции. psych::fa.parallel(opinion) ## Parallel analysis suggests that the number of factors = 6 and the number of components = 4 Можно визуализировать данные в новых осях (например, в первых двух): plot(pr$x[,1:2]) Наконец, выбрать желаемое количество компонент можно через параметр rank. = prcomp(opinion, scale. = TRUE, rank. = 4) ## Standard deviations (1, .., p=15): ## [1] 1.7822858 1.2706276 1.0404145 1.0373031 0.9620233 0.9594423 0.9312585 ## [8] 0.9151956 0.8932286 0.8511995 0.8189853 0.8003226 0.7568713 0.7509609 ## [15] 0.7274690 ## ## Rotation (n x k) = (15 x 4): ## PC1 PC2 PC3 PC4 ## iOp01 -0.06943778 0.38155798 -0.19427723 0.4915882664 ## iOp03 -0.20562301 0.30500547 0.47716338 -0.2907785685 ## iOp04 0.36177159 -0.08071718 0.34244974 -0.0003028303 ## iOp10 -0.34078095 0.23989272 0.19732683 -0.2139680548 ## iOp11 0.29931733 0.09490561 0.09826912 -0.0213681852 ## iOp02.5 0.32443413 0.07300092 0.07894631 0.0320344221 ## iOp42 0.12845659 0.48677246 -0.23754435 0.0526953352 ## iOp43 0.16507540 0.31259574 -0.28949688 -0.4294394393 ## iOp46 -0.20512296 0.17552225 0.40029547 0.0300032371 ## iOp45 -0.29856014 0.20970290 -0.31605182 -0.0187235987 ## iOP13.5 0.35202797 0.02718279 0.05356841 0.1507532470 ## iOp44 0.11226061 0.20192394 0.07709088 -0.4107140003 ## iOp07.5 0.26953643 0.30979313 -0.15452816 -0.1165836764 ## iOp47 0.33900566 0.15729302 0.24253529 0.1122834985 ## iOP12.5 -0.10942316 0.33363638 0.26570013 0.4695379747 5.4 Эксплораторный факторный анализ Чарльз Спирмен был один из пионеров использования коэффициентов корреляции в научных исследованиях. Он уже в самом начале XX века использовал корреляционные матрицы для исследования связи разных тестов, которые проходили школьники. Он обнаружил, что оценки по самым разным тестам коррелируют друг с другом. Короче говоря, если учащийся хорошо справляется с математикой, то и с музыкой у него, скорее всего, будет хорошо. Чарльз Спирмен предположил, что за этими шкалами стоит некоторый единый фактор, который он назвал фактором g (от слова general) - общий интеллект. С точки зрения Спирмена, общий интеллект - это латентный фактор, который объясняет существующие корреляции между баллами за разные тесты, а то, что общий интеллект не объясняет, - это отдельные способности. Для того, чтобы доказать свою теорию, Чарльз Спирмен придумал эксплораторный факторный анализ (ЭФА; exploratory factor analysis)6. Суть ЭФА несколько сложнее АГК. Если в АГК мы просто вертим оси исходного пространства, чтобы максимизировать дисперсию первых осей и минимизировать дисперсию последних, то в ЭФА мы строим модель с заданным количеством латентных (т.е. “скрытых”) переменных, которые должны объяснять общую дисперсию наблюдаемых переменных и быть ортогональными (перпендикулярными) друг другу. Ну а все необъясненное остается влиянием независимых индивидуальных факторов. Кроме того, полученные латентные факторы можно еще “повращать” для большей интерпретируемости латентных факторов. Причем “вращение” может быть как ортогональным (самое распространенное - варимакс) или косоугольным (например, облимин). В первом случае факторы останутся нескоррелированными, во втором случае - нет. Для ЭФА есть много пакетов, более того, уже знакомый нам psych умеет делать ЭФА. fa_none_fit &lt;- factanal(opinion_scaled, factors = 6, rotation = &quot;varimax&quot;) fa_none_fit ## ## Call: ## factanal(x = opinion_scaled, factors = 6, rotation = &quot;varimax&quot;) ## ## Uniquenesses: ## iOp01 iOp03 iOp04 iOp10 iOp11 iOp02.5 iOp42 iOp43 iOp46 ## 0.842 0.633 0.539 0.516 0.730 0.742 0.512 0.775 0.863 ## iOp45 iOP13.5 iOp44 iOp07.5 iOp47 iOP12.5 ## 0.605 0.005 0.948 0.642 0.614 0.828 ## ## Loadings: ## Factor1 Factor2 Factor3 Factor4 Factor5 Factor6 ## iOp01 -0.111 0.371 ## iOp03 0.587 0.109 ## iOp04 0.152 0.569 0.279 -0.113 -0.136 ## iOp10 -0.103 -0.270 -0.208 0.592 ## iOp11 0.188 0.447 -0.151 ## iOp02.5 0.279 0.246 0.247 -0.120 0.196 ## iOp42 0.569 0.391 ## iOp43 0.197 0.426 ## iOp46 -0.137 0.266 -0.126 0.160 ## iOp45 -0.575 -0.102 0.168 0.150 ## iOP13.5 0.939 0.173 0.198 -0.192 ## iOp44 0.145 0.154 ## iOp07.5 0.496 0.316 ## iOp47 0.148 0.313 0.482 -0.115 0.115 ## iOP12.5 0.165 0.374 ## ## Factor1 Factor2 Factor3 Factor4 Factor5 Factor6 ## SS loadings 1.044 0.990 0.977 0.934 0.724 0.538 ## Proportion Var 0.070 0.066 0.065 0.062 0.048 0.036 ## Cumulative Var 0.070 0.136 0.201 0.263 0.311 0.347 ## ## Test of the hypothesis that 6 factors are sufficient. ## The chi square statistic is 130.78 on 30 degrees of freedom. ## The p-value is 1.52e-14 Получаемый результат имеет примерно тот же вид, что и при АГК. Нужно смотреть на кумулятивную объясненную дисперсию, а также смотреть на факторные нагрузки - связь латентных факторов с наблюдаемыми шкалами. Особенно нас интересуют факторные нагрузки близкие к 1 и -1 (значения близкие к нулю скрыты). Анализируя эти шкалы, можно сделать вывод о том, что из себя представляет латентный фактор содержательно. Возьмем для примера второй фактор. В нем есть как выраженные положительные, так и отрицательные нагрузки: “Consuming genetically modified (GMO) food is perfectly safe” - положительная нагрузка .569. “When you are ill, how likely are you to turn to alternative medicine (such as homeopathy) rather than seeking treatment from conventional medicine?” - отрицательная нагрузка -.575. Можно предположить, что этот фактор означает доверие к академической науке: те, у кого высокое значение по этому фактору, не боятся ГМО, а те, у кого низкое значение, - верят в эффективность гомеопатии. Давайте визуализируем с помощью ggplot2 факторные нагрузки по первым двум факторам: load &lt;- fa_none_fit$loadings[,1:2] library(ggplot2) load_dt &lt;- as.data.table(load) load_dt$names &lt;- rownames(load) load ## Factor1 Factor2 ## iOp01 -0.01566970 -0.110771479 ## iOp03 -0.07033983 -0.025900326 ## iOp04 0.15240114 0.568881912 ## iOp10 -0.10317583 -0.270441390 ## iOp11 0.06172226 0.188263581 ## iOp02.5 0.27880820 0.246291576 ## iOp42 0.06696601 0.060755424 ## iOp43 0.01663996 0.011630244 ## iOp46 -0.05416371 -0.137096836 ## iOp45 -0.06206774 -0.574575950 ## iOP13.5 0.93850496 0.172697022 ## iOp44 0.05999292 0.049111124 ## iOp07.5 0.07140540 0.009635239 ## iOp47 0.14796036 0.312706149 ## iOP12.5 -0.01468160 -0.040392354 ggplot(load_dt, aes(x = Factor1, y = Factor2))+ geom_point()+ geom_text(aes(label = names), vjust = 1, hjust = 1) 5.5 Конфирматорный факторный анализ Как следует из названия, если ЭФА - это более эксплораторный метод анализа, то конфирматорный факторный анализ (КФА) предназначен для проверки моделей о структуре факторов. Как и в ЭФА, в КФА есть неизмеряемые латентные переменные, которые как-то объясняют измеряемые переменные. Однако если в ЭФА мы просто строим модель, где все латентные переменные объясняют все измеряемые переменные, то в КФА мы можем проверять более специфическую модель, где отдельные латентные переменные связаны только с определенными измеряемыми переменными. Кроме того, мы можем задавать (или не задавать) корреляции между латентными факторами и ошибками. Самый распространенный пакет для КФА в R - lavaan (LAtent VAriable ANalysis). Впрочем, это один из самых распространенных инструментов для КФА (и структурного моделирования, о чем будет позже) вообще! Давайте сразу его установим и загрузим данные по 9 тестам младшеклассников. install.packages(&quot;lavaan&quot;) library(lavaan) data(HolzingerSwineford1939) Модель предполагает, наличие трех коррелирующих друг с другом факторов: визуальный фактор (задания х1, х2, х3), текстовый фактор (задания х4, х5, х6) и фактор скорости (задания х7, х8, х9). Для того, чтобы задать такую модель, у lavaan есть свой синтаксис. Описание модели записывается в строковую переменную. HS_model &lt;- &quot; visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9 &quot; Затем происходит фиттинг модели с помощью функции cfa() hs_cfa &lt;- cfa(HS_model, data = HolzingerSwineford1939) summary(hs_cfa) ## lavaan 0.6-4 ended normally after 35 iterations ## ## Optimization method NLMINB ## Number of free parameters 21 ## ## Number of observations 301 ## ## Estimator ML ## Model Fit Test Statistic 85.306 ## Degrees of freedom 24 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard Errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## visual =~ ## x1 1.000 ## x2 0.554 0.100 5.554 0.000 ## x3 0.729 0.109 6.685 0.000 ## textual =~ ## x4 1.000 ## x5 1.113 0.065 17.014 0.000 ## x6 0.926 0.055 16.703 0.000 ## speed =~ ## x7 1.000 ## x8 1.180 0.165 7.152 0.000 ## x9 1.082 0.151 7.155 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## visual ~~ ## textual 0.408 0.074 5.552 0.000 ## speed 0.262 0.056 4.660 0.000 ## textual ~~ ## speed 0.173 0.049 3.518 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .x1 0.549 0.114 4.833 0.000 ## .x2 1.134 0.102 11.146 0.000 ## .x3 0.844 0.091 9.317 0.000 ## .x4 0.371 0.048 7.779 0.000 ## .x5 0.446 0.058 7.642 0.000 ## .x6 0.356 0.043 8.277 0.000 ## .x7 0.799 0.081 9.823 0.000 ## .x8 0.488 0.074 6.573 0.000 ## .x9 0.566 0.071 8.003 0.000 ## visual 0.809 0.145 5.564 0.000 ## textual 0.979 0.112 8.737 0.000 ## speed 0.384 0.086 4.451 0.000 Здесь мы можем увидеть, сошлась ли модель, оценки качества модели и оценки интересующих параметров модели. Модели для КФА принято рисовать в виде блок-схем с кружочками, квадратиками и стрелочками. Есть несколько вариантов, как именно отрисовывать модели, например, нужно ли отдельным кружочком рисовать ошибку для каждой измеряемой модели. Но, в целом, правила такие: круги - это латентные переменные, квадраты - измеряемые переменные, стрелочки - ассоциации между ними. Для визуализации моделей, построенных в lavaan, существует пакет semPlot: install.packages(&quot;semPlot&quot;) library(semPlot) semPaths(hs_cfa) КФА лежит в основе структурного моделирования (structural equation modelling, SEM). По сути, структурное моделирование - это КФА + анализ пути (path analysis), который можно рассматривать как расширение множественной линейной регрессии - только вместо одного линейного уравнения у нас появляется целая система уровнений. Если же в системе уравнений использовать не наблюдаемые, а на латентные переменные, то получается структурное моделирование. 5.6 Другие многомерные методы АГК, ЭФА, КФА, структурное моделирование - не единственные многомерные методы для анализа данных. За бортом осталось множество подходов, таких как кластерный анализ и многомерное шкалирование. В заключение я хочу показать еще один интересный подход к анализу данных в социальных науках - сетевой анализ, т.е. анализ графов. Этот метод активно применяется в исследованиях социальных сетей, в биологии и даже в гуманитарных науках. В основе графа лежит матрица близости между переменными. В данном случае мы можем использовать уже знакомую нам матрицу корреляций. Тогда отдельная вершина будет отдельным пунктом опросником, а связью между ними - наличие корреляции выше выбранного порога (использовать пороговое значение необязательно, но сделает визуализацию нагляднее). install.packages(&quot;igraph&quot;) opinion_cor_abs &lt;- abs(opinion_cor) diag(opinion_cor_abs) &lt;- 0 rownames(opinion_cor_abs) &lt;- colnames(opinion_cor_abs) &lt;- gsub(&quot;\\\\.&quot;, &quot;&quot;, colnames(opinion_cor_abs)) library(igraph) opinion_ig &lt;- graph.adjacency(opinion_cor_abs, weighted = TRUE, mode = &quot;lower&quot;) opinion_ig &lt;- delete.edges(opinion_ig, E(opinion_ig)[weight &lt; 0.2]) plot( opinion_ig, vertex.colour = &quot;grey&quot;, vertex.frame.color = NA, vertex.size = strength(opinion_ig) * 3 + 2, vertex.label.dist = 1, edge.curved = 0, vertex.label.color = &quot;black&quot; ) В R есть множество пакетов для интерактивных визуализаций графов. install.packages(&quot;networkD3&quot;) library(networkD3) opinion_nd3 &lt;- igraph_to_networkD3(opinion_ig) opinion_nd3$links$source &lt;- opinion_nd3$links$source - 1 opinion_nd3$links$target &lt;- opinion_nd3$links$target - 1 forceNetwork( Links = opinion_nd3$links, Nodes = opinion_nd3$nodes, Source = &#39;source&#39;, Target = &#39;target&#39;, NodeID = &#39;name&#39;, Group = 1, opacity = 1 ) Потом, правда, оказалось, что в данном случае факторный анализ ничего не доказывает, зато метод оказался очень полезным и получил большое распространение (особенно в психологии).↩ "],
["d6.html", "6 Неделя 2, День 6 6.1 Стиль кода 6.2 Вопроизводимость исследований", " 6 Неделя 2, День 6 6.1 Стиль кода После того, как мы научились делать всякие сложные вещи в R, нужно снова вернуться к тому, что иногда кажется несколько занудным и неинтересным: к стилю написания кода и workflow. После того, как путем проб и ошибок нужные результаты посчитаны, а подходящий график нарисован, очень хочется все закрыть и поскорее послать туда, куда это нужно было послать еще неделю назад. При сумбурном образе жизни современного ученого мало кому нравится идея о том, что после написания работающего кода нужно выдохнуть и все перепроверить, переписать код (если что-то можно сделать лучше), закомментировать сложные куски кода. Тем не менее, это не займет много времени, а в дальнейшем много раз окупится, когда придется возвращаться к коду или в нем окажется ошибка (а такое, как Вы уже могли догадаться, бывает часто). Есть некоторые обязательные правила, без которых у Вас ничего не будет работать. Например, нельзя использовать пробелы в названии переменных, переменные не могут начинаться с цифр и т.д. Есть менее строгие правила, которые связаны с тем, что нарушение таковых может в специфических случаях привести к ошибкам. Но даже если Вы знаете, в каких именно случаях и что может пойти не так и почему, уверены, что Вас это не коснется, то все равно игра не стоит свеч. Например, можно создать переменные с именем T и F, но делать этого не стоит, потому что это перепишет стандартные значения T и F (TRUE и FALSE). И если хотя где-то в коде Вы используете T вместо TRUE, то ничего не будет работать, придется потратить очень много времени, чтобы понять, в чем была ошибка. По этой же причине, кстати говоря, лучше не писать сокращать TRUE до T, а FALSE до F. Есть же правила, которые не связаны с тем, что они могут привести к ошибкам, а только с тем, что такой код становится проще читать. Особенно хорошо, если все придерживаются более-менее одного стиля, но, увы, такого в R нет: Вы уже могли заметить, что разные пакеты предлагают свой стиль, немного отличающийся от других. Поэтому вполне нормально, если Вы выработаете свой стиль с опытом. Главное, по возможности придерживаться его. Довольно универсальный стиль описан Хэдли Уикхемом. Вот краткий пересказ основных правил из этого гайда (и некоторых других, которые я добавил сюда сам): Для файлов. Не использовать пробелы и знак - лучше использовать нижнее подчеркивание или дефис. Использовать только маленькие буквы. В начале названия можно использовать числа, чтобы обозначить, в каком порядке нужно запускать скрипты. Для переменных. Использовать короткие, но осмысленные названия переменных. Есть несколько вариантов, как разделять слова в названии переменной: использование больших букв, точка или нижнее подчеркивание. SomeNumber &lt;- 2 + 2 #можно, но не очень удобно some.number &lt;- 2 + 2 #удобно, но не рекомендуется some_number &lt;- 2 + 2 #хорошо В целом, не стоит использовать вариант с точкой (точка обычно означает метод для generic функции), а вариант с нижнем подчеркиванием - самый популярный. Не используйте T, c и название каких-либо привычных функций для называния переменных. Не только из-за того, что это может привести к ошибкам, но и из-за того, что читающий код может не понять, что происходит. Пробелы. Всегда вокруг арифметических операторов (но не вокруг :!), после, но не до запятой, не ставить пробелов около $ и :: Кроме того, в RStudio есть удобное сочетание клавиш для форматирования кода: ctrl + shift + A (Windows) или cmd + shift + A (MacOS). Если выделить код и зажать эти клавиши, то вот такой код… for (i in 1 : 10) { if(i%%2== 0) print( paste(i , &quot;is even&quot;) ) } ## [1] &quot;2 is even&quot; ## [1] &quot;4 is even&quot; ## [1] &quot;6 is even&quot; ## [1] &quot;8 is even&quot; ## [1] &quot;10 is even&quot; …превратится в такой: for (i in 1:10) { if (i %% 2 == 0) print(paste(i , &quot;is even&quot;)) } ## [1] &quot;2 is even&quot; ## [1] &quot;4 is even&quot; ## [1] &quot;6 is even&quot; ## [1] &quot;8 is even&quot; ## [1] &quot;10 is even&quot; 6.2 Вопроизводимость исследований 6.2.1 Спорные исследовательские практики Если Вы откроете какой-нибудь более-менее классический учебник по психологии, то увидите там много результатов исследований, которые в дальнейшем не удалось воспроизвести. Эта проблема накрыла академическое психологическое сообщество относительно недавно и стала, пожалуй, самой обсуждаемой темой среди ученых-психологов. Действительно, о чем еще говорить, если половина фактов твоей науки попросту неверна? Об историческом смысле методологического кризиса, конечно же. Получается, у теорий нет никакого подтверждения, а в плане знаний о психологических фактах мы находимся примерно там же, где и психологи, которые закупали метрономы и тахистоскопы почти полтора века назад. Оказалось, что то, как устроена академическая наука, способствует публикации ложноположительных результатов. Если теоретическая гипотеза подтверждается, то это дает ученому больше плюшек, чем если гипотеза не подтверждается. Да и сами ученые как-то неосознанно хотят оказаться правыми. Ну а перепроверка предыдущих достижений в психологии оказалась не в почете: всем хочется быть первопроходцами и открывать новые неожиданные феномены, а не скрупулезно перепроверять чужие открытия. Все это привело психологию к тому, что в ней (да и не только в ней) закрепилось какое-то количество спорных исследовательских практик (questionable research practices). Спорные практики на то и спорные, что не все они такие уж плохие, многие ученые даже считают, что в них нет ничего плохого. В отличие, например, от фальсификации данных - это, очевидно, совсем плохо. Вот некоторые распространенные спорные исследовательские практики: Выбор зависимых переменных пост-фактум. По своей сути, это проблема скрытых множественных сравнений: если у нас много зависимых переменных, то можно сделать вид, что измерялось только то, на чем нашли эффект. Поэтому стоит задуматься, если возникает идея измерять все подряд на всякий случай - что конкретно предполагается обнаружить? Если конкретной гипотезы нет, то нужно так и написать, делая соответствующие поправки при анализе. Обсуждение неожиданных результатов как ожидаемых. Возможно, Вам это покажется смешным, но очень часто результаты оказываются статистически значимыми, но направлены в другую сторону, чем предполагалось в гипотезе. И в таких случаях исследователи часто пишут, как будто бы такие результаты и ожидались изначально! Приходится, правда, немного подкрутить теоретические построения. Остановка набора испытуемых при достижении уровня значимости (Optional stopping). Представьте себе, что Вы не обнаружили значимых результатов, но p-value болтается около .05. Тогда Вам захочется добрать парочку испытуемых. А потом еще. И еще немного. Проблема с таким подходом в том, что рано или поздно Вы получите статистически значимые результаты. В любом случае, даже если эффекта нет. На самом деле, эта проблема не так сильно влияет на результаты как может показаться, но это все-таки достаточно плохая практика, а ее применение увеличивает количество опубликованных ложно-положительных результатов. Количество необходимых испытуемых для исследования лучше определять заранее (см. 6.2.3). Неправильное округление до .05. Всякий раз, когда видите p = .05 будьте внимательны: вообще-то p не может быть равен именно .05. Либо автор не очень этого понимает, либо просто p больше, а не меньше .05. Например, .054, что потом округляется до .05 и преподносится как статистически значимые результаты. Использование односторонних тестов для сравнения средних. Эта практика похожа на предыдущую: исследователь получил p &gt; .05, но меньше, чем .1. Недобросовестный исследователь, который хочет опубликоваться проводит односторонний т-тест вместо двустороннего, т.е. фактически просто делит р на 2. Совсем недобросовестные даже не пишут, что они использовали односторонний т-тест, хотя по умолчанию все используют двусторонний. Данный список не претендует на полноту. Этих практик стоит избегать и других от этого отучать. Ну а если Вы думаете, что никто не заметит, если Вы так сделаете, то это не так. Например, последние две практики можно легко обнаружить с помощью сайта http://statcheck.io. Этот сайт делает магию: нужно кинуть ему статью, он распознает в ней статистические тесты и пересчитывает их. На самом деле, ничего принципиально сложного сайт не делает, он сделан с помощью R и уже знакомых нам инструментов. С помощью специальных пакетов из файлов вытаскивается текст, в тексте с помощью регулярных выражений находятся паттерны вроде “t(18) = -1.91, p = .036” - в большинстве журналов используется очень похожее форматирование статистических результатов. Зная степени свободы и т-статистику, можно пересчитать p-value. В данном случае это можно посчитать вот так: pt(-1.91, df = 18)*2 #умножаем на 2, потому что двусторонний тест ## [1] 0.07219987 Ну а дальше остается сравнить это с тем, что написали авторы. Например, бывает как в данном случае, что пересчитанный p-value в два раза больше того, что написали авторы. Это означает, скорее всего, что авторы использовали односторонний критерий. Если они об этом нигде не сказали, то это очень плохо. Самостоятельное задание: Найдите научную статью, в которой, как Вам кажется, авторы использовали спорные исследовательские практики. Если такой статьи нет на примете, то по ищите в научных базах статьи из журналов с низким импакт-фактором - средним количеством цитирований на статью. Проверьте статью через http://statcheck.io. Проинтерпретируйте все несовпадения, если такие имеются. 6.2.2 Вопроизводимые исследования в R Очевидно, что перечисленных практик стоит избегать. Однако недостаточно просто сказать “ребята, давайте вы не будете пытаться во что бы то ни стало искать неожиданные результаты и публиковать их”. Поэтому сейчас предлагаются потенциальные решения пробоемы воспроизводимости исследований, которые постепенно набирают все большое распространение. Самое распространенное решение - это использование пререгистраций. Все просто: исследователь планирует исследование, какую выборку он хочет собрать, как будет обрабатывать данные, какие результаты он будет принимать как соответствующие гипотезе, а какие - нет. Получается что-то вроде научной статьи без результатов и их обсуждения, хотя можно и в более простом виде все представить: главное, есть доказательство, что исследование Вы планировали провести именно так, а не подменяли все на ходу для красивых выводов. Другой способ добиться большей воспроизводимости результатов (и защиты от фальсификации) - это увеличение прозрачности в публикации данных и методов анализа. Существуют ученые (к счастью, такое встречается все реже), которые будут раздражаться, если их попросить дать вам данные. Мол, ну как же так, я их столько собирал, это же мои данные, а вот вы украдете у меня их и сделаете что-нибудь на них, опубликуете свою статью. Нет уж, мол, сами собирайте свои данные. Такое происходит даже для уже опубликованных статей! Мол, я терпел, и вы терпите. Очевидно, что наука - не забивание гвоздей, ценность научной работы не обязательно пропорциональна количеству задействованных испытуемых, а эти ученые что-то делают не так. Собранные данные в некоторых случаях можно использовать в других исследованиях, а если все могут посмотреть исходные данные и проверить анализ, то это вообще круто и может защитить от ошибок. Конечно, не все готовы к таким разворотам в исследовательской практике. Но лучше быть готовым, потому что в какой-то момент может оказаться, что новые практики станут обязательными. И тут R будет весьма кстати: можно выкладывать данные c RMarkdown документом, который будет сразу собирать из этого статью и графики. Данные со скриптами можно выкладывать на GitHub - это удобно для коллективной работы над проектом. Другой вариант - выкладывать данные и скрипты для анализа на сайте osf.io. Это сайт был специально создан как платформа для публикации данных исследований и скриптов для них. Самостоятельное задание: Найдите статью с кодом по на R по интересуемой Вам теме. Посмотрите код, попытайтесь его понять и запустить самостоятельно. 6.2.3 Статистическая мощность Чтобы избежать optional stopping, нужно определять размер выборки заранее. Как это сделать? Наиболее корректный способ предполагает использование анализа статистической мощности (statistical power analysis). Для этого понадобится пакет pwr. install.packages(&quot;pwr&quot;) Этот пакет предоставляет семейство функций для расчета мощности, размера эффекта, уровня \\(\\alpha\\) или нужного размера выборки для разных статистических тестов. Статистическая мощность - вероятность обнаружить статистически значимый эффект, если он действительно есть. Размер эффекта - собственно, размер эффекта в исследовании, обычно в универсальных единицах. Например, размер различия средних обычно измеряется в стандартных отклонениях. Это позволяет сравнивать эффекты в разных исследованиях и даже эффекты в разных областях науки. Если задать 3 из 4 чисел (1 - статистическая мощность - стандартно это .8; 2 - уровень \\(\\alpha\\) - стандартно .05; размер эффекта; размер выборки), то функция выдаст недостающее число. Я очень советую поиграться с этим самостоятельно. Например, если размер эффекта в единицах стандартных отклонений \\(d_{Cohen&#39;s} = 2\\), то сколько нужно испытуемых, чтобы обнаружить эффект для двухвыборочного т-теста с вероятностью .8? library(pwr) pwr.t.test(d = 2, power = 0.8, type = &quot;two.sample&quot;) ## ## Two-sample t test power calculation ## ## n = 5.089995 ## d = 2 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group Всеего 6 в каждой группе (округляем в большую сторону)! pwr.t.test(d = 2, power = 0.8, type = &quot;paired&quot;) ## ## Paired t test power calculation ## ## n = 4.220726 ## d = 2 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number of *pairs* Еще меньше, если использовать within-subject дизайн исследования и зависимый т-тест. А если эффект маленький - всего \\(d_{Cohen&#39;s} = .2\\)? pwr.t.test(d = .2, power = 0.8, type = &quot;two.sample&quot;) ## ## Two-sample t test power calculation ## ## n = 393.4057 ## d = 0.2 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group 394 испытуемых в каждой группе! Можно проверить такие расчеты самостоятельно с помощью симуляции данных. mean(replicate(1000, t.test(rnorm(394, 100, 15), rnorm(394, 103, 15))$p.value &lt; 0.05)) ## [1] 0.8 Действительно, если много раз делать случайные выборки из двух нормальных распределений с средними, отличающимися на 0.2 стандартных отклонения, то примерно в 80% случаев вы получите статистически значимые различия! Самостоятельное задание: Представьте, что эффекта нет - две выборки взяты из нормального распределения. Посчитайте, как часто t.test() будет выдавать ложноположительный результат при \\(\\alpha\\) = .05. mean(replicate(10000, t.test(rnorm(15), rnorm(15))$p.value &lt; 0.05)) ## [1] 0.0473 Будет ли это зависеть от размера выборки? mean(replicate(10000, t.test(rnorm(150), rnorm(150))$p.value &lt; 0.05)) ## [1] 0.0508 mean(replicate(10000, t.test(rnorm(1500), rnorm(1500))$p.value &lt; 0.05)) ## [1] 0.0495 Представьте, что мы работает не с данными нормального распределения, а с rlnorm(). mean(replicate(10000, t.test(rlnorm(15), rlnorm(15))$p.value &lt; 0.05)) ## [1] 0.0328 mean(replicate(10000, t.test(rlnorm(150), rlnorm(150))$p.value &lt; 0.05)) ## [1] 0.0468 "]
]
