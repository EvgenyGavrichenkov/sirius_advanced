--- 
title: "Анализ данных в социальных науках с помощью языка R. Вторая неделя"
author: "Иван Поздняков"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib]
biblio-style: apalike
link-citations: yes
description: "Конспекты для курса по анализу данных в R для Сириус"
---

Здесь содержатся материалы для курса по анализу данных в R для Сириус.


<!--chapter:end:index.Rmd-->

# Неделя 2, День 1 {#d1}

## Warmup exercise {#warm}

Итак, давайте загрузим датасет про студентов и вес их рюкзаков и сконвертируем его в data.table:

```{r, eval = FALSE, echo = TRUE}
install.packages("Stat2Data")
```


```{r, echo = TRUE}
library(Stat2Data)
library(data.table)
data("Backpack")
back <- as.data.table(Backpack)
```

__Самостоятельное задание:__

1. Исследуйте колонки BackpackWeight и BodyWeight. В чем измеряются эти переменные? Переводите их в килограммы, создав колонки BackpackWeightKG, BodyWeightKG


```{r}
back[, summary(BackpackWeight)]
back[, summary(BodyWeight)]
back[,BackpackWeightKG:= 0.45359237*BackpackWeight]
back[,BodyWeightKG:= 0.45359237*BodyWeight]
```


2. Присвойтен id каждому испытуемому

```{r}
back[,id:=1:.N]
```

3. Как различается вес рюкзака в зависимости от пола? Кто весит больше? Если допустить, что выборка репрезентативна, то можно ли сделать вывод о различии в генеральной совокупности?

```{r}
back[,mean(BackpackWeightKG), by = Sex]
back[,t.test(BackpackWeightKG ~ Sex)]
```

4. Повторите пункт 3 для веса самих студентов.

```{r}
back[,mean(BodyWeightKG), by = Sex]
back[,t.test(BodyWeightKG ~ Sex)]
```
5. Визуализируйте распределение этих двух переменных в зависимости от пола (используя ggplot2)

```{r}
library(ggplot2)
ggplot(back)+
  geom_histogram(aes(x = BodyWeightKG, fill = Sex), bins = 15, position = "identity", alpha = 0.7)
```

```{r}
ggplot(back)+
  geom_histogram(aes(x = BackpackWeightKG, fill = Sex), bins = 12, position = "identity", alpha = 0.7)
```

6. Теперь исследуем взаимосвязь переменных. Посчитайте коэффициент корреляции Пирсона и Спирмена.

```{r}
back[, cor.test(BodyWeightKG, BackpackWeightKG)]
back[, cor.test(BodyWeightKG, BackpackWeightKG, method = "spearman")]
```

7. Постройте диаграмму рассеяния с помощью ggplot2. Цветом закодирйте пол респондента

```{r}
ggplot(back, aes(x = BodyWeightKG, y = BackpackWeightKG))+
  geom_point(aes(colour = Sex), alpha = 0.5, size = 2)
```

## Простая линейная регрессия {#lm}

Вы уже умеете считать коэффициент корреляции Пирсона:

```{r}
back[,cor.test(BackpackWeightKG, BodyWeightKG)]
```

Простая линейная регрессия - это примерно то же самое. В синтаксисе линейной регрессии уже не обойтись без формул, это такой специальный тип данных в R:

```{r}
class(y ~ x)
```

### Формула {#formula}

Если видите эту волнистую линию - тильду, то значит перед Вами формула.

Давайте исследуем зависимость размера рюкзака от массы тела. В простой лингейной регрессии, в отличие от корреляции, есть направленность: одна переменная является как бы независимой переменной (предиктором), другая - как бы объясняемой переменной (outcome). Это может немного запутать: если одна переменная предиктор, а другая объясняется этим предиктором, то кажется, что они должны быть обязательно связаны причинно-следственной связью. Это не так: обозначения условны, более того, Вы можете поменять переменные местами и практически ничего не изменится! Короче говоря, "линейная регрессия" не дает никакой магической каузальной силы переменным.

### Функция lm() {#lm}

Давайте посчитаем линейную регрессию функцией lm().

```{r}
model <- lm(BackpackWeightKG ~ BodyWeightKG, data = back)
model
summary(model)
```

`print(model)` или просто `model` выводит коэфициенты линейной регрессии - это коэффициенты прямой, которая лучше всего подогнанна к данным. Как измеряется качество этой подгонки? В расстоянии точек исходных точек до прямой. По идее, расстояние до прямой нужно было бы считать просто по модулю. И так делают, хоть и очень редко. Обычно в линейной регрессии используются квадратичные расстояния точек до прямой для оценки расстояния (метод наименьших квадратов - ordinary least squares). Это дает кучу клевых математических свойств, например, возможность легко аналитически найти коэффициенты прямой линейной регрессии.  

```{r}
back[, body := BodyWeightKG]
back[, bp := BackpackWeightKG]
library(ggplot2)
ggplot(data = back,aes(x = body, y = bp))+
  geom_point(alpha = 0.3)+
  geom_abline(slope = 0.03713, intercept = 2.71125)+
  coord_cartesian(xlim = c(-1, 140))
```

Функция `predict()` позволяет скормить модели новые данные и получить предсказания для новых значений предикторов. Попробуем поиграть с этим немного. Допустим, предскажем вес рюкзака для студента весом в 100 кг:
```{r}
predict(model, newdata = data.frame(BodyWeightKG = 100))
```

Мы можем даже попробовать какие-нибудь экстремальные значения для предикторов. Например, сколько будет весить рюкзак студента весом 1000 кг?

```{r}
predict(model, newdata = data.frame(BodyWeightKG = 1000))
```

Очевидно, что в этом не очень много смысла: студент весом 1000 кг не сможет ходить на занятия, поэтому и про вес рюкзака как-то не имеет смысл спрашивать. Это проблема экстрополяции: линейная регрессия позволяет более-менее достоверно предсказывать значения внутри диапазона значений, на которых была построена модель. Еще один "странный" пример - студент весом 0 кг.

```{r}
predict(model, newdata = data.frame(BodyWeightKG = 0))
```

Здесь бессмысленность происходящего еще очевиднее. Конечно, вес студента не может быть равен нулю, иначе это не студент вовсе. Однако это позволяет понять, что такое intercept модели - это значение зависимой переменой в случае, если предиктор равен нулю. А коэффициент предиктора означает, насколько килограммов увеличивается вес рюкзака при увеличении веса студента на 1 кг: на `r model$coefficients[2]`. Не очень много!

### Интерпретация вывода линейной регрессии {#interpret}

Давайте еще раз посмотрим на summary(model):

```{r}
summary(model)
```

Теперь мы понимаем, что это за коэффициенты. Однако это всего лишь их оценка. Это значит, что мы допускаем, что в реальности есть некие настоящие коэффициенты линейной регрессии, а каждый раз собирая новые данные, они будут посчитаны как немного разные. Короче говоря, эти коэффициенты - те же статистики, со своим выборочным распределением и стандартными ошибками. На основе чего и высчитывается p-value для каждого коэффициента - вероятность получить такой и более отклоняющийся от нуля коэффициент при верности нулевой гипотезы - независимости зависимой переменной от предиктора.

Кроме p-value, у линейной регрессии есть R^2^ - доля объясненной дисперсии. Как ее посчитать? Для начала давайте сохраним как отдельные колонки ошибки (необъясненную часть модели) и предсказанные значения (они означают объясненную часть модели). Можно убедиться, что сумма предсказанных значений и ошибок будет равна зависимой переменной.


```{r}
model$residuals
back$residuals = residuals(model)
back$fitted = fitted(model)

back[, bp - (fitted + residuals)]
```

Соответственно, вся сумма объясненной дисперсии разделяется на объясненую и необъясненную. Полная дисперсия (total sum of squares = TSS) может быть посчитана как сумма квадратов разниц со средним. Необъясненная дисперсия - это сумма квадратов ошибок - residual sum of squares (RSS). 

```{r}
rss <- back[, sum(residuals^2)]
rss
tss <- back[, sum((bp - mean(bp))^2)]
tss
1- rss/tss
```

Это очень мало, мы объяснили всего `r (1- rss/tss)*100`% дисперсии. Собственно, и p-value больше, чем 0,05. При этом этот p-value тот же, что и при коэффициента корреляции Пирсона. А R^2^ - это квадрат коэффициента корреляции Пирсона, если речь идет только об одном предикторе. 

```{r}
summary(model)
```

```{r}
cor.test(back$bp, back$body)$estimate^2
```

### Допущения линейной регрессии {#lm_a}

Как и в случае с другими параметрическими методами, линейная регрессия имеет определенные допущения относительно используемых данных. Если они не соблюдаются, то все наши расчеты уровня значимости могут некорректными. 

> Очень важно ставить вопрос о том, насколько результаты будут некорректными. Как сильно нарушения допущений будет влиять на модель? Ответ на этот вопрос может быть контринтуитивен. Например, достаточно большие отклонения от нормальности нам обычно не стражны при условии того, что выборка достаточно большая.

Допущения линейной регрессии связаны с ошибками: они должны быть нормально распределены, а разброс ошибок должен не уменьшаться и не увеличиваться в зависимости от предсказанных значений. Это то, что называется гомоскедастичностью или гомогенностью (когда все хорошо) и гетероскедастичностью или гетерогенностью (когда все плохо).


Если мы применим функцию plot(), то получим 4 скаттерплота:

1. _Зависимость ошибок от предсказанных значений._ На что здесь смотреть? На симметричность относительно нижней и верхней части графика, на то, что разброс примерно одинаковый слева и справа.

2. _Q-Q plot._ Здесь все довольно просто: если ошибки являются выборкой из нормального распределения, то они выстраиваются в прямую линию. Если это мало похоже на прямую линию, то имеет место отклонение от нормальности.

3. _Scale-Location plot._ Этот график очень похож на график 1, только по оси у используются квадратные корни модуля ошибки. Еще один способ исследовать гетеро(гомо)скедастичность и находить выбросы.

4. _Residuals-Leverage plot._ Здесь по оси х - расстояние Кука, а по оси у - стандартизированный размер выбросов. Расстояние Кука показывает high-leverage points - точки, которые имеют экстремальные предсказанные значения, то есть очень большие или очень маленькие значения по предикторам. Для линейной регрессии такие значения имеют большее значение, чем экстремальные точки по предсказываемой переменной. Особенно сильное влияние имеют точки, которые имеют экстремальные значения и по предикторам, и по предсказываемой переменной. Одна такая точка может поменять направление регрессионной прямой! Расстояние Кука отражает уровень leverage, а стандартизированные ошибки отражают экстремальные значения по у (вернее, экстремальные отклонения от предсказанных значений). В этом графике нужно смотреть на точки с правой стороны графика, особенно если они находятся высоко или низко по оси у.

```{r}
plot(model)
```

Давайте теперь нарисуем регрессионную прямую на скаттерплоте:

```{r}
ggplot(data = back,aes(x = body, y = bp))+
  geom_point(alpha = 0.3)+
  geom_abline(slope = model$coefficients[2], intercept = model$coefficients[1])
```

__Самостоятельное задание:__

1. На практике как экстремальные точки часто выбирают среднее плюс минус два (или три стандартных отклонения). Напишите функцию `is_outlier()`, которая возвращает `TRUE`, если значение выходит за 2 стандартных отклонения от среднего.

```{r, echo=FALSE}
is_outlier <- function(x) abs(x - mean(x)) > sd(x)*2
```


2. Затем измените функцию `is_outlier()` так, чтобы можно было бы самостоятельно вводить количество стандартных отклонений с помощью параметра `n = `.  Пусть по умолчанию это будет 3. Но проверьте, ято все работает и на 2!

```{r, echo=FALSE}
is_outlier <- function(x, n = 2) abs(x - mean(x)) > sd(x)*n
```

3. Теперь измените функцию `is_outlier()` так, чтобы можно было выбирать функцию для меры центральности (`centr = `, по умолчанию - среднее) и функцию для меры разброса (`vary = `, по умолчанию - стандартное отклонение). Проверьте, что это работает на медиане и 3 median absolute deviation. Как Вы думаете, какой из вариантов подходит больше? Почему?

> О, да, в R функция тоже может быть использована в качестве аргумента функции. Впрочем, это не первый случай, когда мы с этим сталкиваемся. Другой пример - функции семейства  `*apply()`. Заметьте - там тоже в качестве аргумента выступала функция (как объект), а не просто название функции как строковая переменная. В этом задании нужно сделать так же.

```{r, echo=FALSE}
is_outlier <- function(x, n = 2, centr = mean, vary = sd) {
  abs(x - centr(x)) > vary(x)*n
}
```


```{r, echo=FALSE}
is_outlier <- function(x, n = 2, centr = mean, vary = sd) {
  (x > centr(x) + n * vary(x)) | (x < centr(x) - n * vary(x))
}
```

### Влияние выбросов на линейную модель {#outliers}

Давайте теперь попробуем посмотреть, как изменится модель, если выкинуть high leverage points (экстремальные значения по предиктору - body) и что будет, если выкинуть экстремальные значения по у. Обычная линия - регрессионная прямая для модели со всеми точками, штрихованная линия - регрессионная прямая для модели без экстремальных значений по предиктору, пунктирная линия - регрессионная прямая для модели без экстремальных значений по предсказываемой переменной.

```{r}
back[, body_outlier := is_outlier(body)]
back[, bp_outlier := is_outlier(bp)]
model1 <- lm(BackpackWeightKG ~ BodyWeightKG, data = back[!is_outlier(body),])
summary(model1)
model2 <- lm(BackpackWeightKG ~ BodyWeightKG, data = back[!is_outlier(bp),])
summary(model2)


ggplot(data = back, aes(x = body, y = bp, colour =bp_outlier, shape = body_outlier))+
  geom_point(alpha = 0.3)+
  geom_abline(intercept = model$coefficients[1], slope = model$coefficients[2])+
  geom_abline(intercept= model1$coefficients[1], 
              slope = model1$coefficients[2], linetype = "dashed")+
  geom_abline(intercept= model2$coefficients[1], 
              slope = model2$coefficients[2], linetype = "dotted")+
  theme_minimal()
```

## Множественная линейная регрессия {#lm_mult}

В множественной линейной регрессионной регрессии у нас появляется несколько предикторов. Какая модель лучше: где есть много предикторов или где мало предикторов? С одной стороны, чем больше предикторов, тем лучше: каждый новый предиктор может объяснить чуть больше необъясненной дисперсиии. С другой стороны, если эта прибавка маленькая (а она всегда будет не меньше нуля), то, возможно, новый предиктор просто объясняет "случайный шум". В действительности, если у нас будет достаточно много предикторов, то мы сможем объяснить любые данные! Парадоксальным образом такая модель будет давать очень плохие данные на новой выборке - это то, что в машинном обучении называют переобучением (overfitting). Идеальная модель будет включать минимум предикторов, которые лучше всего объясненяют исследуемую переменную. Это что-то вроде [бритвы Оккама](https://ru.wikipedia.org/wiki/Бритва_Оккама) в статистике.  

Поэтому часто используются показатели качества модели, которые "наказывают" модель за большое количество предикторов. Например, adjusted R^2^:

$$R_{adj} = 1 - (1 - R^2) \frac{n -1}{n - p - 1}$$

Здесь _n_ - это количество наблюдений, _p_ - количество параметров. 

Итак, добавим новый предиктор - _Units_. Это количество кредитов, которые студенты взяли в четверти[^1]. Можно предположить, что чем больше у студента набрано кредитов, тем более тяжелый у нее/него рюкзак. Давайте добавим это как второй предиктор. Для этого нужно просто записать второй предиктор в формуле через плюс.

[^1]: Кредиты - это что-то вроде часов во многих зарубежных системах образования. Более "тяжелые" курсы дают больше кредитов. Студентам необходимо набрать определенное количество кредитов, чтобы закончить вуз. 

```{r}
model3 <- lm(bp ~ body + Units, data = back)
summary(model3)
```

Множественная линейная регрессия имеет еще одно допущение: отстутсвие мультиколлинеарности. Это значит, что предикторы не должны коррелировать друг с другом. 

Для измерения мультколлинеарности существует variance inflation factor (VIF-фактор). Считается он просто: для предиктора $i$ считается линейная регрессия, где все остальные предикторы предсказывают предиктор $i$. 

Сам VIF-фактор считается на основе полученного R^2^ регрессии:

$$VIF_i = \frac{1}{1 - R_i^2}$$

Если R~i~^2^ большой, то и VIF~i~ выходит большим. Это означает, что предиктор сам по себе хорошо объясняется другими предикторами. Какой VIF считать большим? Здесь нет единого мнения, но если он выше 3 и особенно если он выше 10, то с этим нужно что-то делать.

```{r}
car::vif(model3)
```

В нашем случае это не так. Но если бы VIF был большим для какого-либо предиктора, то можно было бы либо попробовать его выкинуть или же использовать анализ главных компонент, о котором пойдет речь в один из следующих дней.


<!--chapter:end:01-Week2Day1_lm.Rmd-->


# Неделя 2, День 2 {#d2}

## Дисперсионный анализ (ANOVA) {#anova}

Дисперсионный анализ или ANOVA[^1] - один из самых распространенных методов статистического анализа в психологии и многих других дисциплинах. Дисперсионный анализ очень хорошо подходит для анализа данных, полученных в эксперименте - методе организации исследования, при котором исследователь напрямую управляет уровнями независимой переменной. Терминологическая связь между дисперсионным анализом и планированием эксперимента настолько тесная, что многие итермины пересекаются, поэтому нужно быть осторожными. Как и в случае с линейной регрессией, если мы что-то называем "независимой переменной" (или "фактором"), это не порождает никакой каузальной связи.

[^1]: ANOVA от ANalysis Of VAriance, по-русски часто читается как "АНОВА".

Еще одна важная вещь, которую нужно понимать про дисперсионный анализ, это то, что у этого метода очень запутывающее название: из названия кажется, что этот статистический метод для сравнения дисперсий. Нет, это не так (хотя такие статистические тесты тоже есть, и они нам сегодня пригодятся!). Нет, это просто сравнение средних в случае, если есть больше, чем 2 группы для сравнения.

У дисперсионного анализа очень много разновидностей, для которых придумали множество названий. "Обычная" ANOVA называется One-Way ANOVA, она же межгрупповая ANOVA, это аналог независимого т-теста для нескольких групп.

Давайте начнем сразу с проведения теста. Мы будем использовать  [данные с курса по статистике Университета Шеффилда про эффективность диет](https://www.sheffield.ac.uk/polopoly_fs/1.570199!/file/stcp-Rdataset-Diet.csv).

```{r}
library(data.table)
diet <- fread("data/stcp-Rdataset-Diet.csv")
```

Сделаем небольшой препроцессинг данных. Создадим дополнительные "факторные" переменные, создадим переменную, в которой будет разница массы "до" и "после", удалим `NA`.

```{r}
diet[, weight.loss := weight6weeks - pre.weight]
diet[, Dietf := factor(Diet, labels = LETTERS[1:3])]
diet[, Person := factor(Person)]
diet <- diet[complete.cases(diet),]
```

### Функция aov() {#aov}
 
Попробуем сразу провести дисперсионных анализ с помощью функции `aov()`:

```{r}
aov_model <- aov(weight.loss ~ Dietf, diet)
aov_model
summary(aov_model)
```

Мы получили что-то похожее на результат применения функции `lm()`. Правда, лаконичнее, но с новыми столбцами `Sum Sq`, `Mean Sq` и новой статистикой *F* вместо *t*. Что будет, если с теми же данными с той же формулой запустить `lm()` вместо `aov()`?

```{r}
summary(lm(weight.loss ~ Dietf, diet))
```

`lm()` превратил `Dietf` в две переменные, но *F* и p-value у двух моделей одинаковые! Кроме того, функция `aov()` является, по сути, просто "оберткой" над `lm()`:

> This provides a wrapper to lm for fitting linear models to balanced or unbalanced experimental designs.

### Тестирование значимости нулевой гипотезы в ANOVA. {#anova_nhst}

Как и в случае с другими статистическими тестами, мы можем выделить 4 этапа в тестировании значимости нулевой гипотезы в ANOVA: 

1. __Формулирование нулевой и альтернативной гипотезы.__ Нулевая гипотеза говорит, что между средними в генеральной совокупности нет различий:

$$H_0:\mu_1 = \mu_2 = ... = \mu_n$$
Можно было бы предположить, что ненулевая гипотеза звучит как "все средние не равны", но вообще-то это не так. Альтернативная гипотеза в дисперсионном анализе звучит так:

$$H_1: \text{Не все средние равны}$$

2. __Подсчет статистики.__ Как мы уже видели раньше, в дисперсионном анализе используется новая для нас статистика *F*. Впрочем, мы ее видели, когда смотрели на аутпут функции `lm()`, когда делали линейную регрессию.
Чтобы считать *F* (если вдруг мы хотим сделать это вручную), нужно построить талбицу ANOVA (ANOVA table).  

-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Таблица ANOVA                 Степени свободы    Суммы квадратов            Средние квадраты                                    F-статистика           
---------------------------- ----------------- --------------------------- ----------------------------------------- ----------------------------------
Межгрупповые                  $df_{b}$           $SS_{b}$                  $MS_{b} =\frac{SS_{b}}{df_{b}}$              $F=\frac{MS_{b}}{MS_{w}}$

Внутригрупповые               $df_{w}$           $SS_{w}$                  $MS_{w} =\frac{SS_{w}}{df_{w}}$  

Общие                         $df_{t}$          $SS_{t}= SS_{b} + SS_{w}$             
-------------------------------------------------------------------------------------------------------------------------------------------------------------------

Именно эту таблицу мы видели, когда использовали функцию `aov()`:

```{r}
summary(aov_model)
```

Вот как это все считается:



----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Таблица ANOVA           Степени свободы                                         Суммы квадратов                                                Средние квадраты                                    F-статистика           
--------------------- ---------------------- ---------------------------------------------------------------------------------------- ----------------------------------------------- -----------------------------------------------------------------------------------------
Между                 $df_{b}=J-1$            $SS_{b}= \sum\limits_{j=1}^J \sum\limits_{i=1}^{n_j} (\overline{x_j}-\overline{x})^2$        $MS_{b} =\frac{SS_{b}}{df_{b}}$                     $F=\frac{MS_{b}}{MS_{w}}$

Внутри                $df_{w}=N-J$            $SS_{w}= \sum\limits_{j=1}^J \sum\limits_{i=1}^{n_j} (x_{ij}-\overline{x_j})^2$              $MS_{w} =\frac{SS_{w}}{df_{w}}$  

Общие                 $df_{t}=N-1$            $SS_{t}= \sum\limits_{j=1}^J \sum\limits_{i=1}^{n_j} (x_{ij}-\overline{x})^2$             
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

$J$ означает количество групп, $N$ - общее количество наблюдений во всех группах, $n_j$ означает количество наблюдений в группе j, а $x_{ij}$ - наблюдение под номером $i$ в группе $j$. 

Вариабельность обозначается $SS$ и означает "сумму квадратов" (sum of squares) - это то же, что и дисперсия, только мы не делим вме в конце на количество наблюдений (или количество наблюдений минус один): $$SS = \sum\limits_{i=1}^{n_j} (x_{i}-\overline{x})^2$$ 

Здесь много формул, но суть довольно простая: мы разделяем вариабельность зависимой переменной на внутригрупповую и межгрупповую, считаем их соотношение, которое и будет *F*. В среднем, *F* будет равен 1 при верности нулевой гипотезы. Это означает, что и межгрупповая вариабельность, и внутригрупповая вариабельность - это просто шум. Но если же межгрупповая вариабельность - это не просто шум, то это соотношение будет сильно больше единицы.

3. __Подсчет p-value.__ В т-тесте мы смотрели, как статистика распределена при условии верности нулевой гипотезы. То есть что будет, если нулевая гипотеза верна, мы будем повторять эксперимент с точно таким же дизайном (и размером выборок) бесконечное количество раз и считать *F*. 


```{r, fig.cap="\\label{fig:fig1}F-распределение при верности нулевой гипотезы (см. детали в тексте)", out.width = '60%'}

betweendf <- 2
withindf <- 73
f <- summary(aov_model)[[1]]$F[1]

v <- seq(0.1,10, 0.01)
fdist <- data.frame(fvalues = v, pdf = df(v, betweendf, withindf))

library(ggplot2)

label <- paste0("F(", betweendf, ", ", withindf, ") = ", round(f, 3))

ggplot(fdist, aes(x = fvalues, y = pdf))+
  geom_line()+
  geom_vline(xintercept = f)+
  annotate("text", x = f+1, y = 0.2, label = label)+
  scale_y_continuous(expand=c(0,0)) + 
  theme_minimal()+
theme(axis.line.y = element_blank(),
      axis.ticks.y = element_blank(),
      axis.text.y = element_blank(),
      axis.title.y = element_blank())  
```

Заметьте, распределение *F* несимметричное[^2]. Это значит, что мы всегда считаем считаем площадь от *F* до плюс бесконечности (без умножения на 2, как мы это делали в т-тесте):

```{r}
1 - pf(f, betweendf, withindf)
```


[^2]: Форма *F*-распределения будет сильно зависеть от числа степеней свободы. Но оно всегда определено от 0 до плюс бесконечности: в числителе и знаменателе всегда неотрицательные числа.

Это и есть наш p-value! 

4.  __Сравнение p-value с уровнем $\alpha$.__ Самый простой этап: если наш p-value меньше, чем $\alpha$ (который обычно равен .05), то мы отвергаем нулевую гипотезу. Если нет - не отвергаем.  

В нашем случае это `r 1 - pf(f, betweendf, withindf)`, что, очевидно, меньше, чем .05. Отвергаем нулевую гипотезу (о том, что нет различий), принимаем ненулевую (о том, что различия есть). Все!

### Post-hoc тесты {#anova_posthoc}

Тем не менее, дисперсионного анализа недостаточно, чтобы решить, какие именно группы между собой различаются. Для этого нужно проводить post-hoc тесты (апостериорные тесты).

Post-hoc переводится с латыни как "после этого". Post-hoc тесты проводятся, если в результате ANOVA Вы отвергли нулевую гипотезу. Собственно, пост-хоки никак не связаны с дисперсионным анализом на уровне расчетов - это абсолютно независимые тесты, но исторически так сложилось, что они известны именно как дополнительный этап ANOVA.

Самый простой вариант пост-хок теста - это попарные т-тесты с поправками на множественные сравнения:

```{r}
pairwise.t.test(diet$weight.loss, diet$Dietf)
```

Второй подход связан с использованием специализированных тестов, таких как тест Тьюки (Tukey Honest Significant Differences = Tukey HSD). Для этого в R есть функция `TukeyHSD()`, которую нужно применять на объект `aov`:

```{r}
TukeyHSD(aov_model)
```

### ANOVA и т-тест как частные случаи линейной регрессии {#aov_as_lm}

Как мы уже видели, если применить `lm()` или `aov()` на одних и тех же данных с одной и той же формулой, то результат будет очень похожим. Но есть одно но: `lm()` создает из одного фактора две переменных-предиктора:

```{r}
summary(lm(weight.loss ~ Dietf, diet))
```

Дело в том, что мы не можем просто так загнать номинативную переменную в качестве предиктора в линейную регрессию. Мы можем это легко сделать, если у нас всего два уровня в номинативном предикторе. Тогда один из уровней можно обозначить за 0, другой - за 1. Такие переменные иногда называются "бинарными". Тогда это легко использовать в линейной регрессии:

```{r}
summary(lm(weight.loss ~ gender, diet))
```

Можно ли так делать? Вполне! Допущения линейной регрессии касаются остатков, а не переменных самих по себе. Разве что это немного избыточно: линейная регрессия с бинарным предиктором - это фактически независимый т-тест:

```{r}
t.test(weight.loss ~ gender, diet, var.equal = TRUE)
```

Как видите, p-value совпадают! А *t* статистика в квадрате - это *F* (при двух группах):

```{r}
t.test(weight.loss ~ gender, diet, var.equal = TRUE)$statistic^2
```

Более того, те же самые результаты можно получить и с помощью коэффициента корреляции Пирсона:

```{r}
cor.test(diet$gender, diet$weight.loss)
```

Теперь должно быть понятно, почему все эти функции делают вроде бы разные статистические тесты, но выдают такой похожий результат - это фактически один и тот же метод! Все эти методы (и некоторые из тех, что будем рассматривать далее) можно рассматривать как разновидности множественной 
линейной регрессии. [^3]

[^3]: Обобщением множественной линейной регрессии (вернее, одним из) можно считать общую линейную модель (general linear model). Общая линейная модель может предсказывать не одну, а сразу несколько объясняемых переменных в отличие от множественной линейной регрессии. Следующим этапом обобщения служит обобщенная линейная модель (generalized linear model). Фишка последней в том, что можно использовать не только модели с нормально распределенными остатками, но и, например, логистическую и пуассоновскую регрессию.

### Dummy coding {#dummy}

Тем не менее, вопрос остается открытым: как превратить номинативную переменную в количественную и загнать ее в регрессию? Для этого можно использовать "фиктивное кодирование" (dummy coding):

```{r}
diet[, isA := as.numeric(Dietf == "A")]
diet[, isB := as.numeric(Dietf == "B")]
diet[, isC := as.numeric(Dietf == "C")]
diet[c(1:2,15:16,35:36),c("Dietf", "isA", "isB", "isC")]
```

Заметьте, что такое кодирование избыточно. Если мы знаем, что диет 3, а данная диета - это не диета В и не диета С, то это диета А. Значит, одна из созданных нами колонок - "лишняя":

```{r}
diet[, isA := NULL]
```

Используем новую колонки для линейной регрессии и сравним результаты:

```{r}
summary(lm(weight.loss ~ isB + isC, diet))
summary(lm(weight.loss ~ Dietf, diet))
```

То же самое!

### Допущения ANOVA {#aov_a}

1. __Нормальность распределения ошибок:__

```{r}
hist(residuals(aov_model))
```

Как мы видим, распределение не сильно далеко от нормального - этого вполне достаточно. ANOVA - это метод достаточно устойчивый к отклонениям от нормальности.

2. __Гомогенность дисперсий.__ 

То есть их равенство. Можно посмотреть на распределение остатков:

```{r}
diet$residuals <- residuals(aov_model)
ggplot(diet, aes(x = Dietf, y = residuals))+ geom_jitter(width = 0.1, alpha = 0.5)
```

Все выглядит неплохо: нет какой-то одной группы, у которой разброс сильно больше или меньше. Есть и более формальные способы проверить равенство дисперсий. Например, с помощью теста Ливиня (Levene's test). Для того, чтобы его провести, мы воспользуемся новым пакетом `ez` (читать как "easy"). Этот пакет сильно упрощает проведение дисперсионного анализа, особенно для более сложных дизайнов.


```{r, eval = FALSE}
install.packages("ez")
```

Синтаксис довольно простой: нужно указать, данные, зависимую переменную, переменную с ID, факторы. Необходимо прописать фактор в `between =` или `within =`. В данном случае - в `between =`.

```{r}
library(ez)
ez_model <- ezANOVA(data = diet,
        dv= weight.loss,
        wid = Person, 
        between = Dietf,
        detailed = T, 
        return_aov = T)
ez_model
```

Если при проведении теста Ливиня мы получаем *p < .05*, то мы отбрасываем нулевую гипотезу о равенстве дисперсий. В данном случае мы не можем ее отбросить и поэтому принимаем [^4]

[^4]: Вообще-то эта логика не совсем корректна. Тест Ливиня - это такой же статистический тест, как и остальные. Поэтому считать, что допущения соблюдаются на основании того, что p-value больше допустимого уровня $\alpha$, - это неправильно. Но для проверки допущений такая не очень корректная практика считается допустимой.


Полученный объект (если поставить `return_aov = T`) содержит еще и объект `aov()` - на случай, если у Вас есть функции, которые работают с этим классом:

```{r}
TukeyHSD(ez_model$aov)
```

3. __Примерно одинаковое количество испытуемых в разных группах.__ Здесь у нас все в порядке: 

```{r}
diet[,.N, by = Dietf]
```

Небольшие различия в размерах групп - это ОК, тем более, что на практике такое очень часто случается: кого-то пришлось выкинуть из анализа, для какой-то строчки были потеряны данные и т.д. Однако больших различий в размерах групп стоит избегать. Самое плохое, когда группы различаются значительно по размеру (более чем в 2 раза) и стандартные отклонения отличаются значительно (более чем в 2 раза).

### Многофакторный дисперсионный анализ (Factorial ANOVA) {#fact_aov}

На практике можно встретить One-Way ANOVA (однофакторную ANOVA) довольно редко. Обычно в исследованиях встречается многофакторный дисперсионный анализ, в котором проверяется влияние сразу нескольких факторов. В научных статьях это обозначается примерно так: "3х2 ANOVA". Это означает, что был проведен двухфакторный дисперсионный анализ, причем в одном факторе было три уровня, во втором - два. В нашем случае это будут факторы "Диета" и "Пол". Это означает, что у нас две гипотезы: о влиянии диеты на потерю веса и о влиянии пола на потерю веса. Кроме того, появляется гипотеза о взаимодействии факторов - то есть о том, что разные диеты по разному влияют на потерю веса для разных полов.

Взаимодействие двух факторов хорошо видно на графике с линиями: если две линии параллельны, то взаимодействия нет. Если они не параллельны (пересекаются, сходятся, расходятся), то взаимодействие есть.

```{r}
diet[, genderf:=factor(gender, labels = c("ж", "м"))]
sem <- function(x) sd(x)/sqrt(length(x))
pivot <- diet[,.(meanloss = mean(weight.loss), se = sem(weight.loss)), by = .(Dietf, genderf)]

library(ggplot2)
pd = position_dodge(0.05)
ggplot(pivot, aes(x = Dietf, y = meanloss, colour = genderf))+
geom_line(aes(group = genderf), position = pd)+
geom_pointrange(aes(ymin = meanloss - se, ymax = meanloss +se), position = pd)
```


Как видно по картинке, разница в эффективности диеты С по сравнению с другими видна только для женщин. 

```{r}
ezANOVA(data = diet,
        dv= weight.loss,
        wid = Person, 
        between = .(Dietf, gender),
        detailed = T, 
        return_aov = T)
```

Итак, теперь мы проверяем три гипотезы вместо одной. Действительно, взаимодействие диеты и пола оказалось значимым, как и ожидалось.

### Дисперсионный анализ с повторными измерениями (Repeated-measures ANOVA) {#rm_aov}

Если обычный дисперсионный анализ - это аналог независимого т-теста для нескольких групп, то дисперсионный анализ с повторными измерениями - это аналог зависимого т-теста. В функции `ezANOVA()` для проведения дисперсионного анализа с повторными измерениями нужно просто поставить нужным параметром внутригрупповую переменную. Это означает, что в данном случае мы должны иметь данные в длинном формате, для чего мы воспользуемся функцией `melt()`:

```{r}
dietlong <- melt(diet,
measure = c("pre.weight", "weight6weeks"),
variable = "time",
value = "weight")
dietlongC <- droplevels(dietlong[Dietf == "C",])
```


```{r}
ezANOVA(dietlongC,
        dv = weight, 
        wid = Person,
        within = time)
```

### Смешанный дисперсионный анализ (Mixed between-within subjects ANOVA) {#mixed_aov}

Нам никто не мешает совмещать и внутригруппоые, и межгрупповые факторы вместе. 

```{r}
ezANOVA(dietlong,
        dv = weight, 
        wid = Person,
        within = time,
        between = Dietf)
```

Здесь нас интересует взаимодействие между факторами. Результаты, полученные для этой гипотезы, идентичны результатам по обычному дисперсионному анализу на разницу до и после - по сути это одно и то же.

### Заключение {#aov_sum}

Мы разобрали много разных вариантов дисперсионного анализа. И это неудивительно - дисперсионный анализ является одним из самых распространенных статистических методов, в особенности в экспериментальных науках.
При этом дисперсионный анализ можно представить как частный случай множественной линейной регрессии!

<!--chapter:end:02-Week2Day2_aov.Rmd-->

# Неделя 2, День 2, продолжение {#d2_rmd}

## RMarkdown {#rmd}

``` 
![Если картинка не загрузилась, будет этот текст](https://d33wubrfki0l68.cloudfront.net/61d189fd9cdf955058415d3e1b28dd60e1bd7c9b/b739c/lesson-images/rmarkdownflow.png)
```

![Если картинка не загрузилась, будет этот текст](https://d33wubrfki0l68.cloudfront.net/61d189fd9cdf955058415d3e1b28dd60e1bd7c9b/b739c/lesson-images/rmarkdownflow.png)


Вот так делать заголовки:

```
# R Markdown

## Что такое RMarkdown

## Чанки с кодом 
```

Это чанк с кодом. Он отделяется ``` с обоих сторон и {r}. Это означает, что внутри находится код на R, который должен быть выполнен:


\`\`\` {r}  
2+2  
\`\`\`

```{r}
2+2
```


### Настройки чанка {#chunk}

У чанка с кодом есть набор настроек. Самый важные из них такие:

- _echo_: будет ли показан сам код

- _message_ и _warning_: будут ли показаны сообщения и предупреждения, всплывающие во время исполнения кода

- _eval_:  будет ли испольняться код внутри чанка

### Настройка нескольких чанков {#chunks}
 
Все эти настройки можно настроить как для отдельных чанков, так и для все чанков сразу:


```{r setup2}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


### Чанки с Python! {#python}

Вместо {r} нужно написать {python}

```{python}
x = 'hello, python !'
print (x.split(" "))
```

### Код вне чанков (inline code) {#inline}

Число пи равно \` r pi \`:

```Число пи равно `r pi` ```

### Синтаксис Markdown (без R) {#md}

#### Выделение текста {#md_high}

```
*Курсив* 
_Тоже курсив_
**Полужирный**
__Тоже полужирный__
```

*Курсив* 
_Тоже курсив_
**Полужирный**
__Тоже полужирный__

#### Заголовки разных уровней {#headers}

```
## Заголовки разных уровней

### Мне заголовок

#### И моему сыну тоже

##### И моему!

###### OK, boomer

```

#### Списки {#md_lists}

* Первый вариант списка выглядит так:  

  + Можно и с подсписком
  + Почему бы и нет?

1. Кому нужен порядок
2. Тот списки номерует

#### Цитаты {#md_cite}

Цитата:

> Я устал  
> Который год во мне живет нарвал

### Таблицы {#rmd_tables}

```{r data, message=FALSE, warning = FALSE}
library(data.table)
go <- fread("data/iGLAS for R course.csv")
go[1:4,1:4]
```


```{r}
library(knitr)
kable(go[1:5,1:4])
```

#### Динамические таблицы {#dynamic_tables}

```{r}
library(DT)
datatable(go[1:5, 1:5])
```

### Визуализации {#vis}

```{r}
library(ggplot2)
library(Stat2Data)
library(data.table)
data("Backpack")
back <- as.data.table(Backpack)

ggplot_scatter <- ggplot(back, aes(x = BodyWeight, y = BackpackWeight))+
  geom_point(aes(colour = Sex), alpha = 0.5, size = 2)
ggplot_scatter
```

### Динамические визуализации в plotly {#din_vis}

```{r}
library(plotly)
ggplotly(ggplot_scatter)
```


### Вставлять HTML {#html}

<iframe width="966" height="543" src="https://www.youtube.com/embed/hHW1oY26kxQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>



<!--chapter:end:021-Week2Day2_RMD.Rmd-->

# Неделя 2, День 3 {#d2}

```{r}
library(data.table)
```


## Введение в работу с текстом {#text}

Работа с текстом - это отдельная и сложная задача. И у R все необходимые инструменты для этого!

Всю работу со строками (и текстом в целом) можно условно поделить на три уровня:

1) Базовые операции (до регулярных выражений)
2) Регулярные выражения
3) Natural language processing

Мы начнем с базовых операций. В R есть много функций для работы со строками. В принципе, их достаточно для того, чтобы делать весьма сложные вещи, но как и часто это бывает с R, есть дополнительные пакеты, которые не столько расширяют функционал, сколько делают нашу жизнь удобнее. 

Основных таких пакета два: `stringi` и `stringr`.  Давайте сразу их установим:

```{r, eval = FALSE}
install.packages("stringi")
install.packages("stringr")
```

```{r}
library(stringi)
library(stringr)
```

`stringi` - это базовый пакет, который имеет очень широкий функционал. Функции из этого пакета начинаются на `stri_`. `stringr` - это пакет, который является "оберткой" пакета `stringi`. Функции пакета `stringr` начинаются на `str_`. `stringr` - более "минималистичный": в нем меньше функций, чем в `stringi`. А еще `stringr` - это часть `tidyverse`, но этот пакет вполне можно использовать и без `tidyverse`, например, работая в `data.table`.

Тем не менее, для более-менее продвинутой работы с текстом придется выучить специальный язык - “регулярные выражения” (regular expressions или просто regex). Регулярные выражения реализованы на многих языках, в том числе в R. Но мы пока обойдемся наиболее простыми функциями, которые покроют большую часть того, что нам нужно уметь делать при работе с текстом.

Ну а после освоения базовых возможностей и регулярных выражений можно кидаться в естественную обработку языка и делать всякие топик моделлинги и прочие сентимент анализы. Впрочем, можно туда запрыгивать сразу, но обладая базовым инструментарием работы со строковыми данными, все эти штуки делать будет гораздо проще и эффективнее. Итак, поехали.

## Базовые операции с текстом {#text_base}

Строковые данные (`character`) - один из базовых типов данных в R. 

Для того, чтобы создать строковую переменную нужно использовать кавычки. Можно одинарные:

```{r}
'Всем привет!'
```

Можно двойные:

```{r}
"Всем привет!"
```

Разницы никакой! Главное использовать один вид кавычек для одного значения вектора. И еще: если использовать один вид кавычек для задания значения переменной, то другой вид кавычек можно использовать "внутри":

```{r}
"Всем 'привет'!"
```

```{r}
'Всем "привет"!'
```

Пустую строку можно сделать с помощью функции `character()`:

```{r}
character(1)
```

А вот это уже похоже на пассивную агрессию:

```{r}
"С тобой все в порядке?"
character(5)
```

Конечно, можно превращать другие переменные в строки:

```{r}
as.character(1:10)
```

```{r}
as.character(TRUE)
```

### Соединение и разъединение строк {#text_c}

Как соединить строки? Очевидным способом будет попробовать функцию `c()`:

```{r}
c("Всем", "Привет")
```

И вроде бы это то, что нужно, но если посмотрите внимательно, то увидите, что оба слова выделены в кавычки. Короче говоря, мы просто соединили два значения в вектор (ну или два вектора длиной один в вектор длиной два).

Для того, чтобы соединить строковые значения, есть функция `paste()`:

```{r}
paste("Всем", "Привет")
```

Вот это то, что нужно!

У функции `paste()` есть параметр `sep = ` - разделитель, который по умолчанию является пробелом - `sep = " "`, но его можно поменять на любой другой, например:

```{r}
paste("Всем", "Привет", sep = "хэй!")
```

Обычно, правда, нам не нужно придумывать такие ухищренные разделители, а нужно, чтобы его вообще не было:

```{r}
paste("Всем", "Привет", sep = "")
```

Есть функция `paste0()`, которая является оберткой над обычным `paste()`, но уже с `sep = ""` по умолчанию:

```{r}
paste0("Всем", "Привет")
```

Теперь попробуем создать простой `data.table` с буквами. Буквы латиницы зашиты в R как константы:

```{r}
let <- data.table(small = letters[1:10], big = LETTERS[1:10])
let
```

Давайте теперь попробуем создать колонку `both`, где объединим обе колонки с помощью какого-нибудь разделителя:

```{r}
let[, both := paste(small, big, sep = "_")]
let
```

Ага! Если немного присмотреться, то можно заметить, что функция `paste()` "склеивает" несколько векторов и выдает на выходе вектор такой же длины. А вот чтобы соединить значения одного вектора в одно значение, нужно воспользоваться параметром `collapse = `:

```{r}
let[, paste(small, big, sep = "+", collapse = " -> ")]
```

Есть и более продвинутый способ соединения строк, который происходит еще из C и очень популярен в Python. Для этого есть функция `sprintf()`. Вот пример того, как это работает:

```{r}
sprintf("Добро пожаловать в %s, на дворе %i год", "СИРИУС", year(Sys.Date()))
```

Это может быть удобно, если, например, Вы хотите в большую строчку вставить какую-то информацию из переменных.

Ну и наконец, самый продвинутый способ соединять строчки - это пакет `glue`. О да, для того, чтобы делать простые вещи очень просто нужно осваивать целый пакет! Но не беспокойтесь, у этого пакета очень интуитивно понятный синтаксис, который удобнее (на мой взгляд), чем `sprintf()`:

```{r, eval = FALSE}
install.packages("glue")
```

```{r}
library(glue)
let[, glue("{1:.N} буква латинского алфавита: {let$big}(заглавная) - {let$small}(строчная)")]
```

Теперь осталось научиться разъединять строковые значения. Для этого в пакете `data.table` есть функция `tstrsplit()` 

```{r}
let[, tstrsplit(both, "_")]
```

### Подсчет длины строк {#text_len}

Чтобы посчитать количество знаков, можно воспользоваться функцией `nchar()`:

```{r}
nchar(c("Всем", "привет"))
```

В качестве самостоятельного задания загрузите датасет со всеми текстами песен Дэвида Боуи, взятых с сайта [Genius.com]:

```{r}
b <- fread("data/bowie2.csv")
b
```


Текст песен находится в колонке `lyrics`.

```{r}
b[, n_letters := nchar(lyrics)]
b[which.max(n_letters),]
```

Это какой-то речетатив Боуи, который еще и очень непросто найти.

Какова средняя длина песен Боуи?

```{r}
b[, mean(n_letters)]
```

### Выделение подстрок и обрезание строк {#text_cut}

Еще одна полезная функция - `substr()`, она позволяет "вырезать" из `character` кусок от "сих" (`start = `) и до "сих" (`stop = `)

Выглядит это так:

```{r}
substr("Не режь меня!", 4, 7) #вырезаем от 4 знака до 7
```

Более продвинутый способ "обрезать" значения есть в пакете `stringr`:

```{r}
b[, song_name_trunc := str_trunc(song_name, width = 15)]
head(b[, song_name_trunc], 10)
```

В аргументе `width = ` вы ставите максимально допустимую длину значения в векторе. Если значение длиннее, то конец отрезается, а вместо него присобачивается то, что задано в параметре `ellipsis = ` (по умолчанию там стоит многоточие). Отрезается ровно столько, сколько нужно, чтобы начало и `ellipsis = ` вместе были не больше чем `width = `. Эта функция очень удобна при создании графиков: очень неприятно, когда все не помещается из-за одного очень длинного названия.


Функция `str_pad()` позволяет добавить нужное количество пробелов (или других знаков):

```{r}
b[, song_name_pad := str_pad(song_name, 20)]
head(b[, song_name_pad], 10)
```

Функция `str_trim()` делает обратную (и на практике более полезную вещь) - удаляет лишние пробелы слева и/или справа:

```{r}
b[, song_name_trimmed := str_trim(song_name_pad)]
```

Функция `str_squish()` делает еще круче: она еще и удаляет повторяющиеся пробелы внутри.

```{r}
str_squish(" Привет,   всем")
```


### Изменение регистра {#text_to}

Чтобы перевести маленькие буквы в большие, нужно воспользоваться функцией `toupper()`:

```{r}
toupper("В чащах юга жил бы цитрус? Да, но фальшивый экземпляръ!")
```

```{r}
tolower("Съешь ещё этих мягких французских булок, да выпей же чаю")
```

Ну а чтобы сделать первую букву каждого слова заглавной - `str_to_title()`:

```{r}
str_to_title("Съешь ещё этих мягких французских булок, да выпей же чаю")
```

### Случайные последовательности {#text_rand}

Можно сделать случайные последовательности символов с помощью `stri_rand_strings()`
```{r}
set.seed(42)
stri_rand_strings(n = 5, length = 5:9)
```

Можно задавать символы, допустимые для генерации таких "псевдослов" с помощью параметра `pattern = `. 
Например, если хотим использовать только строчные буквы латиницы, нужно использовать паттерн `"[a-z]"`. Так паттерны задаются в регулярных выражениях, к которым мы вернемся позже.

```{r}
stri_rand_strings(n = 5, length = 5:9, pattern = "[a-z]")
```

> Строго говоря, это не то, что принято называть "псевдословами". Под псевдословами подразумеваются такие последовательности, которые звучат как настоящие слова, но не имеют смысла. Например, "ошмаска" или "утурник"

Ну а функция `stri_rand_shuffle()` принимает на вход строчку и возвращает ее же, но уже с перемешанными знаками.

```{r}
stri_rand_shuffle("съешь ещё этих мягких французских булок, да выпей же чаю")
```


### Сортировка {#text_sort}

Чтобы сортировать слова в алфавитном порядке, можно воспользоваться generic функцией `sort()`:

```{r}
sort(unlist(tstrsplit("съешь ещё этих мягких французских булок, да выпей же чаю", split = " ")))
```

> generic функция в R - это функция, которая по-разному работает для разных объектов (использует соответствующий данному классу метод). Когда функция получает объект, она первым делом смотрит, что это за класс, а потом действует исходя из класса объекта. Примеры это функции `print()`, `summary()` и `plot()` - они работают почти на любых объектах, но на всех выдают что-то свое. 

__Самостоятельное задание:__

1. Создайте `data.table` `mon` следующего вида:

```{r, echo=FALSE}
mon <- data.table(n = 1:12, Month = month.name)
mon
```

> Названия месяцов - это еще одна константа, зашитая в R! Она называется `month.name`.

2. Создайте колонку `info` следующего вида: "January is the 1 month" ... "December is the 12 month"


```{r, echo = FALSE}
mon[, paste(Month, "is the", n, "month")]
```


2. Сократите длину каждого месяца так, чтобы она была не больше 6 символов:

```{r, echo = FALSE}
mon[, str_trunc(Month, 6)]
```


3. Создайте в `mon` колонку `month`, где все эти месяца будут записаны с маленькой буквы.

```{r, echo = FALSE}
mon[, month := tolower(Month)]
mon
```

4. Создайте колонку `month_anagram`, в котором будут анаграммы названия каждого месяца (т.е. в данном случае - перемешанные) из колонки `month`


```{r}
set.seed(42) #запустите, чтобы у нас одинаковые рандомизации получились
```

```{r, echo = FALSE}
mon[, month_anagram := stri_rand_shuffle(month)]
mon
```

*5. Ну а теперь сложное задание - создайте функцию `is_anagram()`, которая проверяет, что два слова являются анаграммами.

>Подсказка: если в `tstrsplit()` использовать `split = ""`, то это разделит строку пол отдельным знакам.

```{r, echo = FALSE}
is_anagram <- function(a, b) paste(sort(unlist(tstrsplit(a, ""))), collapse = "") == paste(sort(unlist(tstrsplit(b, ""))), collapse = "")
```

```{r}
is_anagram("спаниель", "апельсин")
is_anagram("скол", "клок")
```

Затем эту функцию нужно применить на колонки `month` и `month_anagram`. Скорее всего, придется либо векторизовать функцию, либо применить `mapply()`.

```{r}
mon[, mapply(is_anagram,month, month_anagram)]
```

## Поиск паттернов и регулярные выражения {#regexp}

Очень большая часть работы с текстом - это поиск паттернов. В самом элементарном виде - простых последовательностей.

### grep(), gsub() {#grep}

Для этого в R есть очень много функций. Пожалуй, самая распространенная из них -  функция `grep()`. Она выдает индексы значений вектора, в которых находится подходящая подстрока:

```{r}
mon[, grep("ber", month)]
mon[ grep("ber", month),] #Следите за запятой!
```

Можно возвращать и сами значения с помощью параметра  `value = TRUE`:

```{r}
mon[, grep("ber", month, value = TRUE)]
```

```{r}
mon[, grep("ber", month, value = TRUE, invert = TRUE)] #все остальные
```

Ну а функция `gsub()` заменяет найденный паттерн на новый. Давайте сделаем год более веселым:

```{r}
mon[, month_fun := gsub("ber", "berfest", month)]
```


Заметьте, первым параметром функции `grep()` идет `pattern = `, а не данные (вектор). Это довольно нетипичное поведение для R. Это "заимствованное" слово для R - изначально  `grep()` появилась в UNIX очень давно и означала _«search **g**lobally for lines matching the **r**egular **e**xpression, and **p**rint them»_. Вот и настала пора поразбираться с регулярными выражениями.

> Функции `grep()` и `gsub()` могут использовать "избегая" регулярных выражений, для этого нужно задать параметр `fixed = TRUE`. Я очень рекомендую это делать, если Вы еще не освоились с регулярными выражениями, иначе результат этих функций будет казаться непредсказуемым. 

### Регулярные выражения {#reg}

Регулярные выражения - это целый язык, который позволяет найти любой сложный паттерн в тексте. В основе всей сложной работы с текстом обычно лежат "регулярки". Поэтому стоит ознакомиться хотя бы с их основами.

> Регулярные выражения реализованы в очень многих языках программирования, не только в R, так что это довольно универсальный навык.

![reg.png](images/reg.png)

Учить их лучше всего интерактивно: очень удобно смотреть, что в тексте находится по введенному паттерну. В качестве примера я могу посоветовать ресурс [regexone](https://regexone.com). Потренироваться можно на [кроссворде] (https://regexcrossword.com/challenges/beginner/puzzles/1). Пакет `stringr` тоже предоставляет удобный инструмент: функции `str_view()` (показывает первый найденный паттерн) и  `str_view_all()` (показывает все найденные паттерны). Первый аргумент в них - вектор с данными, второй - паттерн регулярных выражений. Если Все введено верно, то во вкладке Viewer окна RStudio появится данный вектор с выделенными паттернами.

Если нужно найти простую последовательность, то здесь все так же, как и обычно. Очень похоже на то, что Вы делаете, если нажимаете `Ctrl + F` и пытаетесь найти в тексте какое-то ключевое слово:

```{r}
names <- c("Саня", "Ваня", "Даня", "Женя", "Аня", "Андрей", "Леша", "Лера", "Витя", "Валера")
str_view_all(names, "аня")
```

Заметьте, "Аня" осталась за бортом, потому что регулярные выражения case-sensitive. Скажем, мы хотим найти написанные как с большой, так и маленькой буквы паттерны "аня". Здесь придут на помощь наборы - возможные варианты букв, которые мы ожидаем увидеть на нужном месте: 

```{r}
str_view_all(names, "[аА]ня")
```

Ура, мы поймали Аню!

Наборы можно задавать в целом диапазоне. Например, чтобы задать все буквы кириллицы, нужно задать такой диапазон: `[а-яА-ЯёЁ]`.

```{r}
str_view_all(names, "[а-яА-ЯёЁ]ня")
```

Теперь мы поймали еще и Женю.

Символ `^` (внутри набора) означает, что мы ожидаем увидеть любые символы, кроме тех, что в наборе. Попробуйте догадаться, какие имена мы поймаем следующими паттернами: `"[^а-яА-ЯёЁ]ня"`, `"[^а]ня"`

```{r}
str_view_all(names, "[^а-яА-ЯёЁ]ня")
```

```{r}
str_view_all(names, "[^а]ня")
```

Иногда нужно найти один из двух паттернов - то есть реализовать что-то вроде логического ИЛИ. В регулярных выражениях тоже такое есть. Более того, для этого нужен тот же оператор, что и в R - `|`. Например, мы хотим найти все имена, заканчивающиеся на "ня" и начинающиеся на "Ле":
```{r}
str_view_all(names, "Ле|ня")
```

Если бы паттерн включал бы в себя еще и варианты с маленькой буквы "л", то мы бы поймали еще и Валеру:

```{r}
str_view_all(names, "[лЛ]е|ня")
```

Чтобы этого избежать, мы можем задать, что `"[лЛ]е"` должно быть в начале строки (с помощью знака `"^"`), а `"ня"` - в конце.

```{r}
str_view_all(names, "^[лЛ]е|ня$")
```

Что если мы хотим найти точку? Давайте попробуем использовать ее в регулярном выражении:

```{r}
hello <- c("При 534вет.", "всем")
str_view_all(hello, ".")
```

Что-то не то. Дело в том, что точка (как и уже знакомые нам некоторые другие символы - `[]|^$`) - это спецсимволы, которые имеют специальную функцию в регулярных выражениях. Конкретно точка означает "любой знак". Но если нам нужно найти в тексте именно точку или другой спецсимвол, то его нужно _экранировать_ с помощью специального паттерна "\\" (вне R это "\"):

```{r}
str_view_all(hello, "\\.")
```

Для наиболее распространенных последовательностей есть специальные символы. С ними все наоборот, чтобы их использовать, нужно их экранировать. Например, `\\w` выдаст все цифробуквенные (alphanumeric) знаки:

```{r}
str_view_all(hello, "\\w")
```

А `\\W` - все кроме цифробуквенных знаков

```{r}
str_view_all(hello, "\\W")
```

`\\d` -  только цифры:

```{r}
str_view_all(hello, "\\d")
```

`\\D` -   кроме цифр:
```{r}
str_view_all(hello, "\\s")
```

`\\s` -  только  пробелы:

```{r}
str_view_all(hello, "\\S")
```

`\\S` -  все кроме пробелов:
```{r}
str_view_all(hello, "\\D")
```

Следующий этап наращивания нашей мощи инструментария регулярных выражений - количество паттернов.

В общем виде оно задается с помощью фигурных скобочек:

* {n}: ровно n
* {n,}: n или больше
* {,m}: не больше m
* {n,m}: между n и m

```{r}
long <- "1888 - самый длинный год, записанный в римских цифрах: MDCCCLXXXVIII"
str_view_all(long, "C{3}")
```

```{r}
str_view_all(long, "X{1,}")
```

```{r}
str_view_all(long, "н{1,2}")
```

Для самых распространенных вариантов количества искомых паттернов есть сокращения:

* ? = 0 или 1
* + = 1 или больше
* * = 0 или больше

Например, чтобы вытащить все последовательности, начинающиеся на "Л" и заканчивающиеся на "а" (включая Ла), нужно записать так:

```{r}
str_view_all(names, "^Л.*а$")
```

Ну а что бы вытащить все последовательности, где стоит или не стоит какой-то символ, то записать нужно так:

```{r}
str_view_all(names, "^Ле.?а$")
```

__Самостоятельное задание:__

1. Найдите время в формате "04:30", "06:59" 

Проверьте на векторе `times`:

```{r}
times <- c("04:30", "06:59","fg:55","3345")
```

```{r, echo = FALSE}
str_view_all(times, "(2[0-3]|[0-1]\\d):[0-5]\\d")
```

2. Найдите время в формате "04:30", "06:59", игнорируя "невозможное" время, например, "19:84"

Проверьте на векторе `times`:

```{r}
times <- c("04:30", "06:59","fg:55","3345", "19:84")
```


```{r, echo = FALSE}
str_view_all(times, "2[0-3]|[0-1]\\d:[0-5]\\d")
```

3. Найдите время, которое может быть в других форматах: "4:20", "20-10", но не "20г21" или "2019"

Проверьте на векторе `times`:

```{r}
times <- c("04:30", "06:59","fg:55","3345", "19:84", "20-10", "4:20", "20r21", "2019")
```


```{r, echo = FALSE}
str_view_all(times, "(2[0-3]|[0-1]\\d)\\W[0-5]\\d")
```


## А что дальше? {#text_next}

Регулярные выражения - это супермощный инструмент. Однако он довольно непростой, да и выглядит совершенно монструозно. Хэдли Уикхэм приводит следующий пример реально используемого кода для поиска в тексте электронных почт:

```{r, eval = FALSE}
"(?:(?:\r\n)?[ \t])*(?:(?:(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t]
)+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:
\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(
?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ 
\t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\0
31]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\
](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+
(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:
(?:\r\n)?[ \t])*))*|(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z
|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)
?[ \t])*)*\<(?:(?:\r\n)?[ \t])*(?:@(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\
r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[
 \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)
?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t]
)*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[
 \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*
)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t]
)+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)
*:(?:(?:\r\n)?[ \t])*)?(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+
|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r
\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:
\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t
]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031
]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](
?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?
:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?
:\r\n)?[ \t])*))*\>(?:(?:\r\n)?[ \t])*)|(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?
:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?
[ \t]))*"(?:(?:\r\n)?[ \t])*)*:(?:(?:\r\n)?[ \t])*(?:(?:(?:[^()<>@,;:\\".\[\] 
\000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|
\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>
@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"
(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t]
)*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\
".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?
:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[
\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:[^()<>@,;:\\".\[\] \000-
\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(
?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\<(?:(?:\r\n)?[ \t])*(?:@(?:[^()<>@,;
:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([
^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\"
.\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\
]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\
[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\
r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] 
\000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]
|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?(?:[^()<>@,;:\\".\[\] \0
00-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\
.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,
;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|"(?
:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*
(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".
\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[
^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\]
]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\>(?:(?:\r\n)?[ \t])*)(?:,\s*(
?:(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\
".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(
?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[
\["()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t
])*))*@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t
])+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?
:\.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|
\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*|(?:
[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".\[\
]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)*\<(?:(?:\r\n)
?[ \t])*(?:@(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["
()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)
?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>
@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*(?:,@(?:(?:\r\n)?[
 \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,
;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t]
)*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\
".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*)*:(?:(?:\r\n)?[ \t])*)?
(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\["()<>@,;:\\".
\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])*)(?:\.(?:(?:
\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\[
"()<>@,;:\\".\[\]]))|"(?:[^\"\r\\]|\\.|(?:(?:\r\n)?[ \t]))*"(?:(?:\r\n)?[ \t])
*))*@(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])
+|\Z|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*)(?:\
.(?:(?:\r\n)?[ \t])*(?:[^()<>@,;:\\".\[\] \000-\031]+(?:(?:(?:\r\n)?[ \t])+|\Z
|(?=[\["()<>@,;:\\".\[\]]))|\[([^\[\]\r\\]|\\.)*\](?:(?:\r\n)?[ \t])*))*\>(?:(
?:\r\n)?[ \t])*))*)?;\s*)"
```

Обычно работа с текстовыми данными сводится к нескольким отдельным операциям. Например, нужно вытащить из HTML-файла все теги или наоборот избавиться от них. Для этого, конечно, есть специальные пакеты, например, `rvest`.
Сделать _токенизацию_, то есть разбить текст на токены - значимые единицы текста (обычно слова) - пакет `tidytext`. Обычно затем нужно перевести эти слова в неопределенную форму (лемматизация) или отбросить окончания (стеммизация)  - см. пакет `SnowballC`. 

Есть удобные инструменты, которые все это делают сразу. Например, пакет `udpipe`:

```{r, eval = FALSE}
install.packages("udpipe")
```

```{r}
library(udpipe)
```

Для примера возьмем одну песню Дэвида Боуи:

```{r}
heroes <- b[grep("Heroes",song_name)[1],lyrics]
tok_heroes <- udpipe(heroes, "english")
head(tok_heroes)
```

В результате мы имеем очень длинный датафрейм, каждая строчка которого - это слово или знак препинания. Выглядит не очень симпатично (на первый взгляд), но с этим очень удобно работать. Например, с помощью функции `merge()` можно присоединить к словам сентимент-словарь и сделать сентимент анализ песен Дэвида Боуи. Можно посчитать частотность слов в его песнях. Или же исследовать их сочетание - и сделать топик моделлинг. Освоив базовый инструментарий работы с текстом, перед Вами открывается увлекательный мир анализа естественного языка!

## Fuzzy matching {#fuzzy}

Часто бывает, что два `character` значения совпадают не полностью. Например, в одном случае слово написано в единственном числе, а в другом - во множественном. Или того хуже - в одном случае слово написано с ошибкой. Такие ситуации особенно часто возникают при соединении несколько баз вместе или же когда данные вводятся вручную. Например, представим, что мы опросили людей, кто их любимый актер. Но вот незадача: некоторые написали имя любимого актера с ошибкой!

```{r}
actors <- c("Benedict Cumberbatch", "Bandersnatch Cummerbund", "Bendenswitch Cumbersquash", "Bennendim Cumbendatch", "Bendandsnap Cucumbersnatch", "Kevin Smith", "Emma Stone", "Martin Freeman")
```

В этой ситуации нам поможет fuzzy matching - примерное (approximate) сопоставление с паттерном. Как же измеряется эта "примерная похожесть"? О, как и всегда в таких случаях, за этим стоит целая наука. Самая распространенный способ посчитать похожесть, точнее, непохожесть - это расстояние Левенштайна. Это количество операций, которые нужно сделать, чтобы получить из одной строки другую. Вот список допустимых операций:

* вставка ab → aNb
* удаление aOb → ab
* замена символа aOb → aNb
* перестановка символов ab → ba

Для работы с расстояниями есть встроенная в R функция `agrepl()`. Однако она дает довольно ограниченные возможности, поэтому мы воспользуемся пакетом `stringdist`:

```{r, eval = FALSE}
install.packages("stringdist")
```

```{r}
library(stringdist)
```

Можно посчитать расстояние Левенштайна с помощью функции `stringdist()`, можно преобразовать расстояния в меры близости с помощью функции `stringsim()`:

```{r}
stringdist("Benedict Cumberbatch", actors)
stringsim("Benedict Cumberbatch", actors)
actors[stringsim("Benedict Cumberbatch", actors)>0.4]
```

__Самостоятельное задание:__

1. Найдите в скольки процентах песен Боуи использует слово "star" (в том числе и как часть слова).

```{r, echo = FALSE}
b[grep("star", lyrics), .N]/b[,.N]*100
```

Ого, как много!
2. А теперь найдите все упоминания "star", в том числе и как часть слова, в песнях Боуи. Посчитайте частоту встречаемости каждого из них.

```{r, echo = FALSE}
table(tolower(unlist(str_extract_all(b$lyrics, "\\w*star\\w*"))))
```


3. Найдите все варианты песни `"Space Oddity"` по данной версии с помощью fuzzy matching:

```{r}
space <- b[grep("Space Oddity", song_name)[1], lyrics]
```


```{r, echo = FALSE}
b[, sim := stringsim(space, lyrics)]
plot(b$sim)
b[sim>0.35,]
```


<!--chapter:end:03-Week2Day3_text.Rmd-->

# Неделя 2, День 4 {#d4}

## Многомерные методы анализа данных {#multi}

Сегодняшнее занятие будет посвящено многомерным методам анализа данных - методам работы с данными, в которых много переменных, т.е. колонок. Мы уже сталкивались с некоторыми многомерными методами, такими как множественная линейная регрессия. Поэтому вы знаете, что многомерность создает новые проблемы. Например, при множественных корреляциях или попарных сравнениях возникает проблема множественных сравнений, а при использовании множественной регрессии лишние предикторы могут ловить только шум и приводить к переобучению (если говорить в терминах машинного обучения). Короче говоря, больше - не значит лучше. Нужно четко понимать, зачем мы используем данные и что пытаемся измерить.

Однако в некоторых случаях мы в принципе не можем ничего интересного сделать с маленьким набором переменных. Много ли мы можем измерить личностным тестом с одним единственным вопросом? Можем ли мы точно оценить уровень интеллекта по успешности выполнения одного единственного задания? Очевидно, что нет. Более того, даже концепция интеллекта в современном его представлении появилась во многом благодаря разработке многомерных методов анализа! Ну или наоборот: исследования интеллекта подстегнули развитие многомерных методов.

Наши данные как раз очень многомерные. Это данные опроса с использованием очень большого количества вопросов. Мы возьмем только колонки, содержащие в названии "Op" - это ответы на всякие мировоззренческие вопросы, а это значит, что на них нет правильного ответа, а разные люди будут отвечать по-разному. Эти вопросы связаны с отношением к науке и технологиям, к "научному мировоззрению". Например, "Scientific development is essential for improving people's lives" и "Consuming genetically modified (GMO) food is perfectly safe". Очевидно, что ответы на все эти вопросы будут как-то скоррелированы (положительно или отрицательно). С помощью таких методов как анализ главных компонент и факторный анализ можно уменьшить количество переменных и попытаться понять, что стоит на паттерном различий в ответах респондентов.


__Самостоятельное задание:__

1. Сделайте data.table opinion, в котором будут только колонки с `"OP"` или `"Op"` в названии.

```{r}
library(data.table)
data <- fread("data/iGLAS for R course.csv")
```

Для этого можно воспользоваться регулярными выражениями!

```{r}
op <- grep("iO[pP]", names(data), value = TRUE)
opinion <- data[, ..op]
```

2. Удалите все колонки, где слишком много NA, потом удалите строчки с NA:

Разобьем задачу на части: сначала создадим функцию, которая считает частоту NA в колонке: 

```{r}
frac_na <- function(x) mean(is.na(x))
```

Потом применим эту фунцкию, чтобы выделить названия колонок, в которых NA меньше половины: 

```{r}
not_na_op <- sapply(opinion, frac_na) < 0.5 
opinion <- opinion[, ..not_na_op]
```

Осталось удалить относительно небольшое количество строчек с NA:

```{r}
opinion <- opinion[complete.cases(opinion),]
```

3. Посчитайте матрицу корреляций opinion:

Напоминаю, что это можно сделать несколькими способами. Конечно, можно просто коррелировать каждую пару переменных с каждой:

```{r}
opinion[, cor.test(iOp01, iOp03)]
```

Из этих данных можно доставать значений корреляций - и так повторить для каждой пары переменных. Это не очень удобно. Можно сделать проще - с помощью функции `cor()` посчитать всю матрицу корреляций.

```{r}
opinion_cor <- cor(opinion)
opinion_cor
```

Ну а можно воспользоваться пакетом `"psych"`, который создаст специальный объект классов `"psych"` и `"corr.test"`. Внутри этого объекта будут и матрица корреляций и матрица p-values. 

```{r}
library(psych)
opinion_corr <- corr.test(opinion)
class(opinion_corr)
class(opinion_corr$r)
```

Если Вы решите проверить, совпадают ли результаты этих двух способов, то увидите, что, внезапно, нет:

```{r}
opinion_corr$r == opinion_cor
```

Причина в том, что дробные числа хранятся в компьютере как степени двойки с разной степенью точности - поэтому могут возникать такие вот небольшие различия. А вот если округлить, то все значения окажутся одинаковыми:

```{r}
round(opinion_corr$r,2) == round(opinion_cor,2)
```

Давайте отдельно посмотрим на матрицу p-values (в нижнем углу - сами p-values, в верхнем - с коррекцией на множество сравнений):

```{r}
round(opinion_corr$p, 3)
```

## Хитмап корреляций {#heatmap}

Как видите, почти все коррелирует друг с другом, даже с учетом поправок. Такие множественные корреляции лучше всего смотреть с помощью хитмап-визуализации:

```{r, eval = FALSE}
install.packages("corrplot")
```


```{r}
library(corrplot)
corrplot(opinion_cor, method = "color", order = "hclust")
```

Каждый квадратик - корреляция двух колонок. Синяя - положительная корреляция, красная - отрицательная. Чем насыщеннее цвет, тем сильнее корреляция (то есть выше значение по модулю). Колонки автоматически переставляются в нужном порядке для "группировки".

На картинке можно увидеть, что есть две основных группы вопросов, которые положительно коррелируют внутри группы друг с другом и отрицательно - с вопросами из другой группы.

Различиные многомерные методы анализа позволяют нам выйти за пределы анализа отдельных пар корреляций и попытаться понять структуру, которая стоит за этой корреляционной матрицей. Поэтому во многих многомерных методах анализа данных именно матрица корреляций выступает в роли входных данных.

## Анализ главных компонент (Principal component analysis) {#pca}

Анализ главных компонент (АГК) известен как метод "уменьшения размерности". Представьте, что вам дан набор данных с большим количеством количеством похожих переменных... Ох, надо же, наш датасет как раз именно такой! 

Действительно, подобная ситуация часто возникает в опросниковых данных. Для начала представим ответы на вопросы как точки в многомерном пространстве. Ну, многомерные пространтсва представлять сложно, но вот два измерения - вполне, получится стандартная диаграмма рассеяния.

Суть АГК в том, чтобы повернуть оси этого пространства так, чтобы первые оси объясняли как можно больший разброс данных, а последние - как можно меньший. Тогда мы могли бы отбросить последние оси и не очень-то много и потерять в данных.

Для двух осей это выглядит вот так:

![](images/q7hip.gif)

Первая ось должна минимизировать красные расстояния. Вторая ось будет просто перпендикулярна первой оси.

> Математически, АГК - это нахождение собственных векторов и собственных значений матрицы корреляций или ковариаций. Собственные вектора - это такие особенные вектора матрицы, умножив которые на данную матрицу, можно получить тот же самый вектор (т.е. того же направления), но другой длины. А вот коэффициент множителя длины нового вектора - это собственное значение. В контексте АГК, собственные вектора - это новые оси (т.е. те самые новые компоненты), а собственные значения - это размер объясняемой дисперсии с помощью новых осей.

Итак, для начала нам нужно центрировать и нормировать данные, т.е. вычесть среднее и поделить на стандартное отклонение, т.е. посчитать z-оценки. Это нужно для того, чтобы сделать все шкалы равноценными. Это особенно важно делать когда разные шкалы используют несопоставимые единицы измерения. Скажем, одна колонка - это масса человека в килограммах, а другая - рост в метрах. Если применять АГК на этих данных, то ничего хорошего не выйдет: вклад роста будет слишком маленьким. А вот если мы сделаем z-преобразование, то приведем и вес, и рост к "общему знаменателю".

В нашем случае это не так критично, поскольку во всех случаях испытуемые отвечают по одинаковой 7-балльной шкале.

```{r}
opinion_scaled <- as.data.table(scale(opinion))
```

В базовом R уже есть инструменты для АГК `princomp()` и `prcomp()`, считают они немного по-разному. Возьмем более рекомендуемый вариант, `prcomp()`.

```{r}
prcomp(opinion_scaled)
```

В принципе, z-преобразование было делать необязательно, `prcomp()` умеет делать это сам:

```{r}
pr <- prcomp(opinion, scale. = TRUE)
pr
```

`summary()` выдаст полезную информацию, в частности, долю объясненной дисперсии и кумулятивную долю объясненной дисперсии

```{r}
summary(pr)
```

### Количество извлекаемых компонент {#pca_n}

Визуализация объясненных дисперсий с помощью каждого фактора может использоваться для того, чтобы решить, какие оси оставить, а какие отбросить.

```{r}
plot(pr)
```

Если после какой-то шкалы график резко "падает" и становится ровным, то можно предположить, что эти последние шкалы представляют некоторый "шум" в данных - их и можно отбросить.

В пакете `psych` есть функция `fa.parallel()`, которая позволяет не только визуализировать, но и дает пользователю количество рекомендуемых компонент. Важно понимать, что эти границы условны, поэтому не стоит безусловно доверять этой функции.

```{r}
psych::fa.parallel(opinion)
```

Можно визуализировать данные в новых осях (например, в первых двух):
```{r}
plot(pr$x[,1:2])
```

Наконец, выбрать желаемое количество компонент можно через параметр `rank. = `

```{r}
prcomp(opinion, scale. = TRUE, rank. = 4)
```

## Эксплораторный факторный анализ {#fa}

Чарльз Спирмен был один из пионеров использования коэффициентов корреляции в научных исследованиях. Он уже в самом начале XX века использовал корреляционные матрицы для исследования связи разных тестов, которые проходили школьники. Он обнаружил, что оценки по самым разным тестам коррелируют друг с другом. Короче говоря, если учащийся хорошо справляется с математикой, то и с музыкой у него, скорее всего, будет хорошо.

Чарльз Спирмен предположил, что за этими шкалами стоит некоторый единый фактор, который он назвал фактором g - общий интеллект. С точки зрения Спирмена, общий интеллект - это латентный фактор, который объясняет существующие корреляции между баллами за разные тесты, а то, что общий интеллект не объясняет - это отдельные способности.

Для того, чтобы доказать свою теорию, Чарльз Спирмен придумал эксплораторный факторный анализ (ЭФА; exploratory factor analysis). Потом, правда, оказалось, что в данном случае факторный анализ ничего не доказывает, зато метод оказался очень полезным и получил большое распространение (особенно в психологии).

Суть ЭФА несколько сложнее АГК. Если в АГК мы просто вертим оси исходного пространства, чтобы максимизировать дисперсию первых осей и минимизировать дисперсию последних, то в ЭФА мы строим модель с заданным количеством латентных (т.е. "скрытых") переменных, которые должны объяснять общую дисперсию наблюдаемых переменных и быть ортогональными (перпендикулярными) друг другу. Ну а все необъясненное остается влиянием независимых индивидуальных факторов. Кроме того, полученные латентные факторы можно еще "повращать" для большей интерпретируемости латентных факторов. Причем "вращение" может быть как ортогональным (самое распространенное - варимакс) или косоугольным (например, облимин). В первом случае факторы останутся нескоррелированными, во втором случае - нет.

Для ЭФА есть много пакетов, более того, уже знакомый нам `psych` умеет делать ЭФА.

```{r}
fa_none_fit <- factanal(opinion_scaled, factors = 6, rotation = "varimax")
fa_none_fit
```

Получаемый результат имеет примерно тот же вид, что и при АГК. Нужно смотреть на кумулятивную объясненную дисперсию, а также смотреть на факторные нагрузки - связь латентных факторов с наблюдаемыми шкалами. Особенно нас интересуют факторные нагрузки близкие к 1 и -1 (значения близкие к нулю скрыты). Анализируя эти шкалы, можно сделать вывод о том, что из себя представляет латентный фактор содержательно. Возьмем для примера второй фактор: в нем есть как выраженные положительные, так и отрицательные нагрузки:

- __"Consuming genetically modified (GMO) food is perfectly safe"__ - положительная нагрузка _0.569_

- __"When you are ill, how likely are you to turn to alternative medicine (such as homeopathy) rather than seeking treatment from conventional medicine?"__ - отрицательная нагрузка _-0.575_

Можно предположить, что этот фактор означает доверие к академической науке: те, у кого большое значение по этому фактору не боятся ГМО, а те, у кого низкое значение, - верят в эффективность гомеопатии.

Давайте визуализируем с помощью ggplot2 факторные нагрузки по первым двум факторам: 

```{r}
load <- fa_none_fit$loadings[,1:2]
library(ggplot2)
load_dt <- as.data.table(load)
load_dt$names <- rownames(load)
load
ggplot(load_dt, aes(x = Factor1, y = Factor2))+
  geom_point()+
  geom_text(aes(label = names), vjust = 1, hjust = 1)
```

## Конфирматорный факторный анализ {#cfa}

Как следует из названия, если ЭФА - это более эксплораторный метод анализа, то конфирматорный факторный анализ (КФА) предназначен для проверки моделей о структуре факторов. 

Как и в ЭФА, в КФА есть неизмеряемые латентные переменные, которые как-то объясняют измеряемые переменные. Однако если в ЭФА мы просто строим модель, где все латентные переменные объясняют все измеряемые переменные, то в КФА мы можем проверять более специфическую модель, где отдельные латентные переменные связаны только с определенными измеряемыми переменными. Кроме того, мы можем задавать (или не задавать) корреляции между латентными факторами и ошибками. 

Самый распространенный пакет для КФА в R - lavaan (LAtent VAriable ANalysis). Впрочем, это один из самых распространенных инструментов для КФА (и структурного моделирования, о чем будет позже) вообще!

Давайте сразу его установим и загрузим данные по 9 тестам младшеклассников. 

```{r, eval = FALSE}
install.packages("lavaan")
```


```{r}
library(lavaan)
data(HolzingerSwineford1939)
```

Модель предполагает, наличие трех коррелирующих друг с другом факторов: визуальный фактор (задания х1, х2, х3), текстовый фактор (задания х4, х5, х6) и фактор скорости (задания х7, х8, х9). Для того, чтобы задать такую модель, у `lavaan` есть свой синтаксис. Описание модели записывается в строковую переменную.

```{r}
HS_model <- "
visual =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed =~ x7 + x8 + x9
"
```

Затем происходит фиттинг модели с помощью функции `cfa()`

```{r}
hs_cfa <- cfa(HS_model, data = HolzingerSwineford1939)
summary(hs_cfa)
```

Здесь мы можем увидеть, сошлась ли модель, оценки качества модели и оценки интересующих параметров модели.

Модели для КФА принято рисовать в виде блок-схем с кружочками, квадратиками и стрелочками. Есть несколько вариантов, как именно отрисовывать модели, например, нужно ли отдельным кружочком рисовать ошибку для каждой измеряемой модели. Но, в целом, правила такие: _круги_ - это латентные переменные, _квадраты_ - измеряемые переменные, _стрелочки_ - ассоциации между ними.

```{r, eval = FALSE}
install.packages("semPlot")
```

```{r}
library(semPlot)
semPaths(hs_cfa)
```

КФА лежит в основе структурного моделирования (structural equation modelling, SEM). По сути, структурное моделирование - это КФА + анализ пути (path analysis), который можно рассматривать как расширение множественной линейной регрессии - только вместо одного линейного уравнения у нас появляется целая система уровнений. Если же в системе уравнений использовать не наблюдаемые, а на латентные переменные, то получается структурное моделирование.

## Другие многомерные методы {#other_multi}

АГК, ЭФА, КФА, структурное моделирование - не единственные многомерные методы для анализа данных. За бортом осталось множество подходов, таких как кластерный анализ и многомерное шкалирование. В заключение я хочу показать еще один интересный подход к анализу данных в социальных науках - сетевой анализ, т.е. анализ графов. Этот метод активно применяется в исследованиях социальных сетей, в биологии и даже в гуманитарных науках.

В основе графа лежит матрица близости между переменными. В данном случае мы можем использовать уже знакомую нам матрицу корреляций. Тогда отдельная вершина будет отдельным пунктом опросником, а связью между ними - наличие корреляции выше выбранного порога (использовать пороговое значение необязательно, но сделает визуализацию нагляднее).

```{r, eval = FALSE}
install.packages("igraph")
```

```{r}
opinion_cor_abs <- abs(opinion_cor)
diag(opinion_cor_abs) <- 0
rownames(opinion_cor_abs) <- colnames(opinion_cor_abs) <- gsub("\\.", "", colnames(opinion_cor_abs))


library(igraph)
opinion_ig <- graph.adjacency(opinion_cor_abs, weighted = TRUE, mode = "lower")
opinion_ig <- delete.edges(opinion_ig, E(opinion_ig)[weight < 0.2])
plot(opinion_ig, vertex.colour = "grey", vertex.frame.color = NA,
     vertex.size = strength(opinion_ig)*3+2, vertex.label.dist = 1,
     edge.curved = 0, vertex.label.color = "black")
```

В R есть множество пакетов для интерактивных визуализаций графов.

```{r, eval = FALSE}
install.packages("edgebundleR")
```

```{r}
library(edgebundleR)
edgebundle(opinion_ig)
```

```{r, eval = FALSE}
install.packages("networkD3")
```

```{r}
library(networkD3)
opinion_nd3 <- igraph_to_networkD3(opinion_ig)
forceNetwork(Links = opinion_nd3$links, Nodes = opinion_nd3$nodes,
             Source = 'source', Target = 'target', 
             NodeID = 'name', Group = 1, opacity = 1)
```


<!--chapter:end:04-Week2Day4_fa.Rmd-->

# Неделя 2, День 6 {#d6}

## Стиль кода {#style}

После того, как мы научились делать всякие сложные вещи в R, нужно снова вернуться к тому, что иногда кажется несколько занудным и неинтересным: к стилю написания кода и "воркфлоу". После того, как путем проб и ошибок нужные результаты посчитаны, а подходящий график нарисован, очень хочется все закрыть и поскорее послать туда, куда это нужно было послать еще неделю назад. 
При сумбурном образе жизни современного ученого мало кому нравится идея о том, что после написания работающего кода нужно выдохнуть и все перепроверить, переписать код (если что-то можно сделать лучше), закомментировать сложные куски кода. Тем не менее, это не займет много времени, а в дальнейшем много раз окупится, когда придется возвращаться к коду или в нем окажется ошибка (а такое, как Вы уже могли догадаться, бывает часто).

Есть некоторые обязательные правила, без которых у Вас ничего не будет работать. Например, нельзя использовать пробелы в названии переменных, переменные не могут начинаться с цифр и т.д. Есть менее строгие правила, которые связаны с тем, что нарушение таковых может в специфических случаях привести к ошибкам. Но даже если Вы знаете, в каких именно случаях и что может пойти не так и почему, уверены, что Вас это не коснется, то все равно игра не стоит свеч. Например, можно создать переменные с именем `T` и `F`, но делать этого не стоит, потому что это перепишет стандартные значения `T` и `F` (`TRUE` и `FALSE`). И если хотя где-то в коде Вы используете `T` вместо `TRUE`, то ничего не будет работать, придется потратить очень много времени, чтобы понять, в чем была ошибка. По этой же причине, кстати говоря, лучше не писать сокращать `TRUE` до `T`, а `FALSE` до `F`. 

Есть же правила, которые не связаны с тем, что они могут привести к ошибкам, а только с тем, что такой код становится проще читать. Особенно хорошо, если все придерживаются более-менее одного стиля, но, увы, такого в R нет: Вы уже могли заметить, что разные пакеты предлагают свой стиль, немного отличающийся от других.

Поэтому вполне нормально, если Вы выработаете свой стиль с опытом. Главное, по возможности придерживаться его. Довольно универсальный стиль [описан](http://adv-r.had.co.nz/Style.html) Хэдли Уикхемом. 

Вот краткий пересказ основных правил из этого гайда (и некоторых других, которые я добавил сюда сам):

- _Для файлов._ Не использовать пробелы и знак  \ - лучше использовать нижнее подчеркивание или дефис. Использовать только маленькие буквы. В начале названия можно использовать числа, чтобы обозначить, в каком порядке нужно запускать скрипты.
- _Для переменных._ Использовать короткие, но осмысленные названия переменных. Есть несколько вариантов, как разделять слова в названии переменной: использование больших букв, точка или нижнее подчеркивание. 

```{r}
SomeNumber <- 2 + 2 #можно, но не очень удобно
some.number <- 2 + 2 #удобно, но не рекомендуется
some_number <- 2 + 2 #хорошо
```

В целом, не стоит использовать вариант с точкой (точка обычно означает метод для generic функции), а вариант с нижнем подчеркиванием - самый популярный. Не используйте `T`, `c` и какие-либо привычные функции для называния переменных. Не только из-за того, что это может привести к ошибкам, но и из-за того, что читающий код может не понять, что происходит.

- _Пробелы._ Всегда вокруг арифметических операторов (но не вокруг `:`!), после, но не до запятой, не ставить пробелов около `$` и `::`

Кроме того, в RStudio есть удобное сочетание клавиш для форматирования кода: `ctrl + shift + A` (Windows) или `cmd + shift + A` (MacOS). Если выделить код и зажать эти клавиши, то вот такой код...

```{r}
for  (i in 1 : 10)  { if(i%%2== 0)
print(
paste(i , "is even") )
                    }
```

...превратится в такой:

```{r}
for  (i in 1:10)  {
  if (i %% 2 == 0)
  print(paste(i , "is even"))
}
```

## Вопроизводимость исследований {#repr}

### Спорные исследовательские практики {#quest}

Если Вы откроете какой-нибудь более-менее классический учебник по психологии, то увидите там много результатов исследований, которые в дальнейшем не удалось воспроизвести. Эта проблема накрыла академическое психологическое сообщество относительно недавно и стала, пожалуй, самой обсуждаемой темой среди ученых-психологов. Действительно, о чем еще говорить, если половина фактов твоей науки попросту неверна? ~~Об историческом смысле методологического кризиса, конечно же.~~ Получается, у теорий нет никакого подтверждения, а в плане знаний о психологических фактах мы находимся примерно там же, где и психологи, которые закупали метрономы и тахистоскопы почти полтора века назад.

Оказалось, что то, как устроена академическая наука, способствует публикации ложноположительных результатов. Если теоретическая гипотеза подтверждается, то это дает ученому больше плюшек, чем если гипотеза не подтверждается. Да и сами ученые как-то неосознанно хотят оказаться правыми. Ну а перепроверка предыдущих достижений в психологии оказалась не в почете: всем хочется быть первопроходцами и открывать новые неожиданные феномены, а не скрупулезно перепроверять чужие открытия. Все это привело психологию к тому, что в ней (да и не только в ней) закрепилось какое-то количество спорных исследовательских практик (questionable research practices). Спорные практики на то и спорные, что не все они такие уж плохие, многие ученые даже считают, что в них нет ничего плохого. В отличие, например, от фальсификации данных - это, очевидно, совсем плохо.

Вот некоторые распространенные спорные исследовательские практики:

- _Выбор зависимых переменных пост-фактум._ По своей сути, это проблема скрытых множественных сравнений: если у нас много зависимых переменных, то можно сделать вид, что измерялось только то, на чем нашли эффект. Поэтому стоит задуматься, если возникает идея измерять все подряд на всякий случай - что конкретно предполагается обнаружить? Если конкретной гипотезы нет, то нужно так и написать, делая соответствующие поправки при анализе.

- _Обсуждение неожиданных результатов как ожидаемых._ Возможно, Вам это покажется смешным, но очень часто результаты оказываются статистически значимыми, но направлены в другую сторону, чем предполагалось в гипотезе. И в таких случаях исследователи часто пишут, как будто бы такие результаты и ожидались изначально! Приходится, правда, немного подкрутить теоретические построения.

- _Остановка набора испытуемых при достижении уровня значимости (Optional stopping)._ Представьте себе, что Вы не обнаружили значимых результатов, но p-value болтается около 0.05. Тогда Вам захочется добрать парочку испытуемых. А потом еще. И еще немного. Проблема с таким подходом в том, что рано или поздно Вы получите статистически значимые результаты. В любом случае, даже если эффекта нет. На самом деле, эта проблема не так сильно влияет на результаты как может показаться, но это все-таки достаточно плохая практика, а ее применение увеличивает количество опубликованных ложно-положительных результатов.

- _Неправильное округление до .05._ Всякий раз, когда видите *p = .05* будьте внимательны: вообще-то он не может быть равен именно *.05*. Либо автор не очень этого понимает, либо просто *p* больше, а не меньше *.05*. Например, *.054*, что потом округляется до *.05* и преподносится как статистически значимые результаты. 

- _Использование односторонних тестов для сравнения средних._ Эта практика похожа на предыдущую: исследователь получил *p > .05*, но меньше, чем *.1*. Недобросовестный исследователь, который хочет опубликоваться проводит односторонний т-тест вместо двустороннего, т.е. фактически просто делит р на 2. Совсем недобросовестные даже не пишут, что они использовали односторонний т-тест, хотя по умолчанию все используют двусторонний.

Данный список не претендует на полноту. Этих практик стоит избегать и других от этого отучать. Ну а если Вы думаете, что никто не заметит, если Вы так сделаете, то это не так. 

Например, последние две практики можно легко обнаружить с помощью сайта  http://statcheck.io. Этот сайт делает магию: нужно кинуть ему статью, он распознает в ней статистические тесты и пересчитывает их. На самом деле, ничего сложного, а это сайт сделан с помощью R и уже знакомых нам инструментов: с помощью специальных пакетов из файлов вытаскивается текст, в тексте с помощью регулярных выражений находятся паттерны вроде "*t*(18) = -1.91, *p* = .036" - в большинстве журналов используется очень похожее форматирование статистических результатов. Зная степени свободы и т-статистику, можно пересчитать p-value. В данном случае это можно посчитать вот так:

```{r}
pt(-1.91, df = 18)*2 #умножаем на 2, потому что двусторонний тест
```

Ну а дальше остается сравнить это с тем, что написали авторы. Например, бывает как в данном случае, что пересчитанный p-value в два раза больше того, что написали авторы. Это означает, скорее всего, что авторы использовали односторонний критерий. Если они об этом нигде не сказали, то это очень плохо.

__Самостоятельное задание__:

1. Найдите научную статью, в которой, как Вам кажется, авторы использовали спорные исследовательские практики. Если такой статьи нет на примете, то по ищите в научных базах статьи из журналов с низким импакт-фактором - средним количеством цитирований на статью.

2. Проверьте статью через http://statcheck.io. Проинтерпретируйте все несовпадения, если такие имеются.

### Вопроизводимые исследования в R {#repr_r}

Очевидно, что перечисленных практик стоит избегать. Однако недостаточно просто сказать "ребята, давайте вы не будете пытаться во что бы то ни стало искать неожиданные результаты и публиковать их". Поэтому сейчас предлагаются потенциальные решения пробоемы воспроизводимости исследований, которые постепенно набирают все большую популярность. Самое распространенное решение - это использование пререгистраций. Все просто: исследователь планирует исследование, какую выборку он хочет собрать, как будет обрабатывать данные, какие результаты он будет принимать как соответствующие гипотезе, а какие - нет. Получается что-то вроде научной статьи без результатов и их обсуждения, хотя можно и в более простом виде все представить: главное, есть доказательство, что исследование вы планировали провести именно так, а не подменяли все на ходу для красивых выводов. Другой способ добиться большей воспроизводимости результатов (и защиты от фальсификации) - это увеличение прозрачности в публикации данных и методов анализа. Существуют ученые (к счастью, такое встречается все реже), которые будут раздражаться, если их попросить дать вам данные. Мол, ну как же так, я их столько собирал, это же мои данные, а вот вы украдете у меня их и сделаете что-нибудь на них, опубликуете свою статью. Нет уж, мол, сами собирайте свои данные. Я терпел, и вы терпите. Очевидно, что наука - не забивание гвоздей, а ценность научной работы не обязательно пропорциональна количеству задействованных испытуемых. Собранные данные в некоторых случаях можно использовать в других исследованиях, а если все могут посмотреть исходные данные и проверить анализ, то это вообще круто и может защитить от ошибок. 

Конечно, не все готовы к таким разворотам в исследовательской практике. Но лучше быть готовым, потому что в какой-то момент может оказаться, что новые практики станут обязательными. И тут R будет весьма кстати: можно выкладывать данные c RMarkdown документом, который будет сразу собирать из этого статью и графики. Данные со скриптами можно выкладывать на [GitHub](https://github.com) - это удобно для коллективной работы над проектом. Другой вариант - выкладывать данные и скрипты для анализа на сайте [osf.io](osf.io). Это сайт специально сделанный как платформа для публикации данных исследований и скриптов для них.

__Самостоятельное задание__:

1. Найдите статью с кодом по на R по интересуемой Вам теме. Посмотрите код, попытайтесь его понять и запустить самостоятельно.

### Статистическая мощность {#power}

Чтобы избежать _optional stopping_, нужно определять размер выборки заранее. Как это сделать? Наиболее корректный способ предполагает использование анализа статистической мощности (statistical power analysis). Для этого понадобится пакет `pwr`.

```{r, eval = FALSE}
install.packages("pwr")
```

Этот пакет предоставляет семейство функций для расчета мощности, размера эффекта, уровня значимости или нужного размера выборки для разных статистических тестов.

Статистическая мощность - вероятность обнаружить статистически значимый эффект, если он действительно есть. Размер эффекта - собственно, размер эффекта в исследовании, обычно в универсальных единицах. Например, размер различия средних обычно измеряется в стандартных отклонениях. Это позволяет сравнивать эффекты в разных исследованиях и даже эффекты в разных областях науки.

Если задать 3 из 4 чисел (статистическая мощность - стандартно это .8, уровень значимости - стандартно .05, размер эффекта, размер выборки), то функция выдаст недостающее число. 

Я очень советую поиграться с этим самостоятельно. Например, если размер эффекта в единицах стандартных отклонений Cohen's d = 2, то сколько нужно испытуемых, чтобы обнаружить эффект для двухвыборочного т-теста с вероятностью .8?

```{r}
library(pwr)
pwr.t.test(d = 2, power = 0.8, type = "two.sample")
```

Всеего 6 в каждой группе (округляем в большую сторону)!

```{r}
pwr.t.test(d = 2, power = 0.8, type = "paired")
```

Еще меньше, если использовать within-subject дизайн исследования и зависимый т-тест.

А если эффект маленький - всего d = .2?

```{r}
pwr.t.test(d = .2, power = 0.8, type = "two.sample")
```

394 испытуемых в каждой группе!

Можно проверить такие расчеты самостоятельно с помощью симуляции данных.

```{r}
mean(replicate(1000, t.test(rnorm(394, 100, 15), rnorm(394, 103, 15))$p.value < 0.05))
```

Действительно, если много раз делать случайные выборки из двух нормальных распределений с средними, отличающимися на 0.2 стандартных отклонения, то примерно в 80% случаев вы получите статистически значимые различия!

__Самостоятельное задание__:

1. Представьте, что эффекта нет - две выборки взяты из нормального распределения. Посчитайте, как часто `t.test()` будет выдавать ложноположительный результат при альфа = .05.

```{r}
mean(replicate(10000, t.test(rnorm(15), rnorm(15))$p.value < 0.05))
```

2. Будет ли это зависеть от размера выборки?

```{r}
mean(replicate(10000, t.test(rnorm(150), rnorm(150))$p.value < 0.05))
mean(replicate(10000, t.test(rnorm(1500), rnorm(1500))$p.value < 0.05))
```


3. Представьте, что мы работает не с данными нормального распределения, а с `rlnorm()`. 

```{r}
mean(replicate(10000, t.test(rlnorm(15), rlnorm(15))$p.value < 0.05))
mean(replicate(10000, t.test(rlnorm(150), rlnorm(150))$p.value < 0.05))
```


<!--chapter:end:05-Week2Day5_style.Rmd-->

